id,Title,Journal,Year,Abstract,Keywords,Recommend
1,ISP: An Optimal Out-of-Core Image-Set Processing Streaming Architecture for Parallel Heterogeneous Systems,"Visualization and Computer Graphics, IEEE Transactions on",2012,"Image population analysis is the class of statistical methods that plays a central role in understanding the development, evolution, and disease of a population. However, these techniques often require excessive computational power and memory that are compounded with a large number of volumetric inputs. Restricted access to supercomputing power limits its influence in general research and practical applications. In this paper we introduce ISP, an Image-Set Processing streaming framework that harnesses the processing power of commodity heterogeneous CPU/GPU systems and attempts to solve this computational problem. In ISP, we introduce specially designed streaming algorithms and data structures that provide an optimal solution for out-of-core multiimage processing problems both in terms of memory usage and computational efficiency. ISP makes use of the asynchronous execution mechanism supported by parallel heterogeneous systems to efficiently hide the inherent latency of the processing pipeline of out-of-core approaches. Consequently, with computationally intensive problems, the ISP out-of-core solution can achieve the same performance as the in-core solution. We demonstrate the efficiency of the ISP framework on synthetic and real datasets.","graphics processing units;image processing;parallel processing;pipeline processing;statistical analysis;CPU-GPU systems;ISP framework;asynchronous execution mechanism;computational efficiency;data structures;image population analysis;memory usage;optimal out-of-core image-set processing streaming architecture;out-of-core approach pipeline processing;parallel heterogeneous systems;statistical methods;streaming algorithms;supercomputing power;Computational modeling;Data models;Graphics processing unit;Hardware;MIMO;Parallel processing;Streaming media;GPUs;atlas construction;diffeomorphism;multiimage processing framework.;out-of-core processing;Algorithms;Brain;Computer Graphics;Databases, Factual;Diagnostic Imaging;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Regression Analysis",1
2,High performance computing in biomedical imaging research ,Parallel Computing ,1998,"The Mayo Biomedical Imaging Resource (BIR) conducts research into and development of image analysis, visualization, and measurement capabilities and software tools for biomedical imaging applications. The design goal for these tools includes full interactivity, yet some tools are both compute bound and time sensitive. Therefore, effective use of these capabilities requires that they be executed on high performance computers. This paper provides an overview of the high performance computing activities in the BIR, including resources, algorithms and applications.",Biomedical imaging,1
3,\CUDA\ optimization strategies for compute- and memory-bound neuroimaging algorithms ,Computer Methods and Programs in Biomedicine ,2012,"As neuroimaging algorithms and technology continue to grow faster than CPU performance in complexity and image resolution, data-parallel computing methods will be increasingly important. The high performance, data-parallel architecture of modern graphical processing units (GPUs) can reduce computational times by orders of magnitude. However, its mas- sively threaded architecture introduces challenges when GPU resources are exceeded. This paper presents optimization strategies for compute- and memory-bound algorithms for the CUDA architecture. For compute-bound algorithms, the registers are reduced through variable reuse via shared memory and the data throughput is increased through heavier thread workloads and maximizing the thread conﬁguration for a single thread block per multiprocessor. For memory-bound algorithms, ﬁtting the data into the fast but limited GPU resources is achieved through reorganizing the data into self-contained structures and employing a multi-pass approach. Memory latencies are reduced by selecting mem- ory resources whose cache performance are optimized for the algorithm’s access patterns. We demonstrate the strategies on two computationally expensive algorithms and achieve optimized GPU implementations that perform up to 6x faster than unoptimized ones. Compared to CPU implementations, we achieve peak GPU speedups of 129x for the 3D unbiased nonlinear image registration technique and 93× for the non-local means surface denoising algorithm.",Graphics Processing Unit (GPU),1
4,Power-Efficient Medical Image Processing using PUMA,,2009,"Graphics processing units (GPUs) are becoming an increasingly popular platform to run applications that require a high computation throughput. They are limited, however, by memory bandwidth and power and, as such, cannot always achieve their full potential. This paper presents the PUMA architecture - a domain-speciﬁc accelerator designed speciﬁcally for medical imaging applications, but with sufﬁcient generality to make it programmable. The goal is to closely match the performance achieved by GPUs in this domain but at a fraction of the power consumption. The results are quite promising - PUMA achieves upto 2X the performance of a modern GPU architecture and has upto a 54X improved efﬁciency on a ﬂoating-point and memory-intensive MRI reconstruction algorithm.",coprocessors;magnetic resonance imaging;medical image processing;parallel architectures;MRI reconstruction algorithm;domain specific accelerator;graphics processing units;magnetic resonance imaging;medical image processing;parallel micro architecture for medical applications;Acceleration;Bandwidth;Biomedical image processing;Computer architecture;Energy consumption;Graphics;Hardware;Image analysis;Parallel processing;Registers,1
5,Accelerating 3D Medical Image Segmentation with High Performance Computing,,2008,Digital processing of medical images has helped physicians and patients during past years by allowing examination and diagnosis on a very precise level. Nowadays possibly the biggest deal of support it can offer for modern healthcare is the use of High Performance Computing architectures to treat the huge amounts of data that can be collected by modern acquisition devices. This paper presents a parallel processing implementation of an image segmentation algorithm that operates on a computer cluster equipped with 10 processing units. Thanks to well-organized distribution of the workload we manage to significantly shorten the execution time of the developed algorithm and reach a performance gain very close to linear.,,1
6,High Performance Computing (HPC) in Medical Image Analysis (MIA) at the Surgical Planning Laboratory (SPL),,1998,"This paper outlines some of the usage and applications for HPC in the medical image analysis field. As opposed to traditional HPC work that focuses on developing new optimization strategies and improving the implementation of existing environments, the work reported here is focussing strongly on the utilization of HPC technology (both commercial and public domain) in an application driven clinical and research environment. Work performed at the Surgical Planning Laboratory (SPL) of Brigham and Women's Hospital and Harvard Medical School in Boston is used as an example of this type of activity.",,1
7,Image Segmentation on GPGPUs: A Cellular Automata-based Approach,,2013,"Image segmentation is one of the most difficult tasks in image processing and plays a critical role in the analysis of medical images used for diagnosis and treatment. With the decreased hardware costs and improvements in computing power of many-core architectures, there is an opportunity to both improve upon image segmentation algorithms and to make this technology more accessible. This paper describes our on-going research efforts to implement efficient image segmentation algorithms on graphical processing units (GPUs). A focused case study was performed with a suitable algorithm based on Cellular Automata, a parallel computational technique. Preliminary segmentation results are shown to validate our approach. Plans to improve the algorithm by making it more robust to noise and more efficient on GPU architectures are discussed. Our use of graph theoretic techniques and their implementation on GPUs will have broad application to other areas requiring computationally intensive calculations, as found in many problems involving modeling and simulation.","GPUs, OpenCL, cellular automata, fuzzy connectedness, image segmentation",1
8,High-performance medical image registration using new optimization techniques,"Information Technology in Biomedicine, IEEE Transactions on",2006,"Optimization of a similarity metric is an essential component in intensity-based medical image registration. The increasing availability of parallel computers makes parallelizing some registration tasks an attractive option to increase speed. In this paper, two new deterministic, derivative-free, and intrinsically parallel optimization methods are adapted for image registration. DIviding RECTangles (DIRECT) is a global technique for linearly bounded problems, and multidirectional search (MDS) is a recent local method. The performance of DIRECT, MDS, and hybrid methods using a parallel implementation of Powell's method for local refinement, are compared. Experimental results demonstrate that DIRECT and MDS are robust, accurate, and substantially reduce computation time in parallel implementations",image registration;medical image processing;optimisation;DIRECT;MDS;Powell's direction set method;dividing rectangle technique;high-performance medical image registration;linearly bounded problem;multidirectional search;optimization;parallel computing;Biomedical imaging;Concurrent computing;Convergence;Image registration;Interpolation;Memory architecture;Optimization methods;Parallel processing;Robustness;Technological innovation;DIviding RECTangles (DIRECT);medical image registration;multidirectional search (MDS);optimization;parallel computing,1
9,Large-Scale Biomedical Image Analysis in Grid Environments,"Information Technology in Biomedicine, IEEE Transactions on",2008,"This paper presents the application of a component-based Grid middleware system for processing extremely large images obtained from digital microscopy devices. We have developed parallel, out-of-core techniques for different classes of data processing operations employed on images from confocal microscopy scanners. These techniques are combined into a data preprocessing and analysis pipeline using the component-based middleware system. The experimental results show that: 1) our implementation achieves good performance and can handle very large datasets on high-performance Grid nodes, consisting of computation and/or storage clusters and 2) it can take advantage of Grid nodes connected over high-bandwidth wide-area networks by combining task and data parallelism.","biomedical optical imaging;grid computing;medical image processing;middleware;optical microscopy;Grid-node;biomedical image analysis;component-based Grid middleware system;confocal microscopy scanner;data preprocessing;digital microscopy;image processing;parallel computation;wide-area network;Digital microscopy;digital microscopy;image processing;out of core;out-of- core;parallel computation;Database Management Systems;Image Interpretation, Computer-Assisted;Information Dissemination;Information Storage and Retrieval;Internet;Microscopy;Radiology Information Systems;Signal Processing, Computer-Assisted",1
10,GPU implementation of map-MRF for microscopy imagery segmentation,,2009,"Recent developments in 3D low-light level CCD (L3CCD) image capture have enabled the study of the dynamics of biomedical bodies within cells. This paper firstly presents an improved algorithm for automatic segmentation of such imagery. It allows for the specific nature of noise in L3CCD data. Secondly, the massive volume of data produced by continuous real time 3D scans requires a high performance computation facility for automatic segmentation and tracking. The paper presents details and results of a GPU implementation of a version of the segmentation algorithm, and shows that on an NVIDIA GeForce 8800GTX, coded in CUDA C, the algorithm runs around 550 times faster than the Matlab version of the algorithm running on a PC.",CCD image sensors;biomedical optical imaging;cellular biophysics;computer graphics;image segmentation;medical image processing;optical microscopy;3D low-light level CCD;CUD C;NVIDIA GeForce 8800GTX;automatic segmentation;automatic tracking;biomedical bodies;cells;image capture;microscopy;Biomedical computing;Biomedical image processing;Charge coupled devices;Electron microscopy;Gaussian distribution;High performance computing;Image segmentation;Information technology;Layout;Statistical distributions;Image segmentation;accelerators,1
11,PIPS - A general purpose Parallel Image Processing System,,1994,In this paper we discuss general requirements for parallel implementations of image processing algorithms. Of special importance are the communication demands in the parallel system which are determined by the algorithm under consideration. These communication demands are decisive for determining the best way to distribute the image data to the network processors. Analysing these factors for some typical image processing algorithms reveals that it is difficult to find a fixed network structure which meets all the demands equally well. A possibility to cope with these problems is to emb ed different topologies into a fixed network structure. The mentioned factors were guidelines for the design of the general purpose Parallel Image Pro cessing System Pips. We describe the structure of this software package and elucidate for some selected topics the implementation details. Experimental results from an implementation on a multiprocessor system are presented.,,1
12,Grid-enabling medical image analysis,,2005,"Digital medical image processing is a promising application area for grids. Given the volume of data, the sensitivity of medical information, and the joint complexity of medical datasets and computations expected in clinical practice, the challenge is to fill the gap between the grid middleware and the requirements of clinical applications. The research project AGIR (Grid Analysis of Radiologi- cal Data) presented in this paper addresses this challenge through a combined approach: on one hand, leveraging the grid middleware through core grid medical services which target the requirements of medical data processing applica- tions; on the other hand, grid-enabling a panel of applica- tions ranging from algorithmic research to clinical applica- tions.",,1
13,Automated detection of pelvic fractures from volumetric CT images,,2012,"Pelvic fractures are a major cause of trauma patient mortality. Detection and management of pelvic injuries is challenging due to myriad injury patterns and associated complications such as hemorrhage and infection. In this paper, we propose an automated method of pelvic fracture detection from volumetric CT images. A coarse-to-fine strategy is adopted where a potential region containing the fracture is identified first using intensity and curvature information. The above region is modeled as a weighted graph and a fracture is modeled as a minimum cut in this graph. A second localizing algorithm models the same fracture as a valley, based on the signs of the mean and Gaussian curvature. The minimum cuts as well as the spatial consistent valleys, in isolation, generate a small number of false positives in addition to the true fracture. A joint decision process based on the volumetric graph cuts and the spatially consistent valleys eliminates the false positives. Experimental results indicate the effectiveness of the proposed scheme.",biomechanics;bone;computerised tomography;feature extraction;graph theory;injuries;medical image processing;Gaussian curvature;automated detection;automated method;coarse-to-fine strategy;curvature information;hemorrhage;infection;injury patterns;intensity information;joint decision process;localizing algorithm;minimum graph cut;pelvic fractures;pelvic injuries;spatially consistent valleys;trauma patient mortality;volumetric CT images;volumetric graph cuts;weighted graph;Bones;Computed tomography;Injuries;Joints;Solid modeling;Surface cracks;Graph Cut;Mean and Gaussian curvatures;Pelvic fracture,1
14,Toward Real-time Image Guided Neurosurgery Using Distributed and Grid Computing,,2006,"Neurosurgical resection is a therapeutic intervention in the treatment of brain tumors. Precision of the resection can be improved by utilizing Magnetic Resonance Imaging (MRI) as an aid in decision making during Image Guided Neurosurgery (IGNS). Image registration adjusts pre-operative data according to intra-operative tissue deformation. Some of the approaches increase the registration accuracy by tracking image landmarks through the whole brain volume. High computational cost used to render these techniques inappropriate for clinical applications.In this paper we present a parallel implementation of a state of the art registration method, and a number of needed incremental improvements. Overall, we reduced the response time for registration of an average dataset from about an hour and for some cases more than an hour to less than seven minutes, which is within the time constraints imposed by neurosurgeons. For the first time in clinical practice we demonstrated, that with the help of distributed computing non-rigid MRI registration based on volume tracking can be computed intra-operatively.","distributed computing, grid computing, image registration, medical imaging",1
15,Image based measurements for evaluation of pelvic organ prolapse,Journal of Biomedical Science and Engineering,2013,"Magnetic resonance imaging (MRI) measurements are essential for the diagnosis of pelvic organ prolapse given the inaccuracy of clinical examination. However, MRI pelvic floor measurements are currently performed manually and can be inconsistent and timeconsuming. In this paper, we present a scheme for semi-automatic measurement modeling on MRI based on image segmentation and intersecting point identification methods. The segmentation algorithm is a multistage mechanism based on block grouping, support vector machine classification, morphological operation and prior shape information. Block grouping is achieved by classifying blocks as bone or background based on image texture features. The classified blocks are then used to find the initial segmentation by the first phase morphological opening. Prior shape information is incorporated into the initial segmentation to obtain the final segmentation using registration with the similarity type transformation. After segmentation, points of reference that are used for pelvic floor measurements are identified using morphological skeleton operation. The experiments on the MRI images show that the presented scheme can detect the points of reference on the pelvic floor structure to determine the reference lines needed for the assessment of pelvic organ prolapse. This will lead towards more consistent and faster pelvic organ prolapse diagnosis on dynamic MRI studies, and possible screening procedures for predicting predisposition to pelvic organ prolapse by radiologic evaluation of pelvic floor measurements.",,1
16,Accelerating Advanced MRI Reconstructions on GPUs,,2008,,"cuda, gpgpu, gpu computing, mri, reconstruction",1
17,Histology image analysis for carcinoma detection and grading ,Computer Methods and Programs in Biomedicine,2012,"This paper presents an overview of the image analysis techniques in the domain of histopathology, speciﬁcally, for the objective of automated carcinoma detection and classi- ﬁcation. As in other biomedical imaging areas such as radiology, many computer assisted diagnosis (CAD) systems have been implemented to aid histopathologists and clinicians in cancer diagnosis and research, which have been attempted to signiﬁcantly reduce the labor and subjectivity of traditional manual intervention with histology images. The task of automated histology image analysis is usually not simple due to the unique charac- teristics of histology imaging, including the variability in image preparation techniques, clinical interpretation protocols, and the complex structures and very large size of the images themselves. In this paper we discuss those characteristics, provide relevant back- ground information about slide preparation and interpretation, and review the application of digital image processing techniques to the ﬁeld of histology image analysis. In particular, emphasis is given to state-of-the-art image segmentation methods for feature extraction and disease classiﬁcation. Four major carcinomas of cervix, prostate, breast, and lung are selected to illustrate the functions and capabilities of existing CAD systems.",Histopathology,1
18,The Agile Library for Biomedical Image Reconstruction Using GPU Acceleration,Computing in Science Engineering,2013,"A cheap way to speed up image-reconstruction software is to use modern graphics hardware that can execute algorithms in a massively parallel manner. Here, the authors discuss Agile, an open source library designed for image reconstruction in biomedical sciences. Its modular, object-oriented, and templated design eases the integration of the library into user code.",image reconstruction;medical image processing;parallel processing;public domain software;software libraries;GPU acceleration;agile library;biomedical image reconstruction software;biomedical sciences;massively parallel manner;modular object-oriented templated design;open source library;Biomedical image processing;Graphics processing unit;Image reconstruction;Libraries;Magnetic resonance imaging;GPUs;MRI;biomedical science;finite-element methods;medical resonance imaging;parallel algorithms;scientific computing,1
19,DIPE: A Distributed Environment for Medical Image Processing,,1997,"DIPE is a distributed environment that provides image processing services over integrated teleradiology services networks. DIPE integrates existing and new image processing software and employs sophisticated execution scheduling mechanisms for the efficient management of computational resources within a distributed environment. It can also be extended to provide various added- value services, such as management and retrieval of image processing software modules, as well as advanced charging procedures based on quality of service. DIPE can be viewed as the natural evolution of the legacy field of medical image processing towards a service over the emergent health care telematics networks.",,1
20,A Parallel Implementation of 4-Dimensional Haralick Texture Analysis for Disk-Resident Image Datasets,,2004,"Texture analysis is one possible method to detect features in biomedical images. During texture analysis, texture related information is found by examining local variations in image brightness. 4-dimensional (4D) Haralick texture analysis is a method that extracts local variations along space and time dimensions and represents them as a collection of fourteen statistical parameters. However, the application of the 4D Haralick method on large time-dependent 2D and 3D image datasets is hindered by computation and memory requirements. This paper presents a parallel implementation of 4D Haralick texture analysis on PC clusters. We present a performance evaluation of our implementation on a cluster of PCs. Our results show that good performance can be achieved for this application via combined use of task- and data-parallelism.",,1
21,Processing of Medical Images in Virtual Distributed Environment,,2009,"The processing of medical images within a PACS system depends on high capacity of communication channels and high performance of computational resources. We introduce pilot project utilizing grid technology to distribute functionality of PACS system to several machines located in distant places which allows economizing utilization of network channels. We also discuss benefits and disadvantages of virtualization techniques allowing to separate physical machine capabilities from the operating system. We compare this pilot project utilizing high speed CESNET 2 network with similar mature projects based mainly on P2P secure connection, centralized system and proprietary protocols.","PACS, grid, virtualization",1
22,A novel medical image data-based multi-physics simulation platform for computational life sciences.,Interface Focus,2013,"Simulating and modelling complex biological systems in computational life sciences requires specialized software tools that can perform medical image data-based modelling, jointly visualize the data and computational results, and handle large, complex, realistic and often noisy anatomical models. The required novel solvers must provide the power to model the physics, biology and physiology of living tissue within the full complexity of the human anatomy (e.g. neuronal activity, perfusion and ultrasound propagation). A multi-physics simulation platform satisfying these requirements has been developed for applications including device development and optimization, safety assessment, basic research, and treatment planning. This simulation platform consists of detailed, parametrized anatomical models, a segmentation and meshing tool, a wide range of solvers and optimizers, a framework for the rapid development of specialized and parallelized finite element method solvers, a visualization toolkit-based visualization engine, a Python scripting interface for customized applications, a coupling framework, and more. Core components are cross-platform compatible and use open formats. Several examples of applications are presented: hyperthermia cancer treatment planning, tumour growth modelling, evaluating the magneto-haemodynamic effect as a biomarker and physics-based morphing of anatomical models.",,1
23,"Scalable, high-performance 3D imaging software platform: system architecture and application to virtual colonoscopy.",Conf Proc IEEE Eng Med Biol Soc,2012,"One of the key challenges in three-dimensional (3D) medical imaging is to enable the fast turn-around time, which is often required for interactive or real-time response. This inevitably requires not only high computational power but also high memory bandwidth due to the massive amount of data that need to be processed. In this work, we have developed a software platform that is designed to support high-performance 3D medical image processing for a wide range of applications using increasingly available and affordable commodity computing systems: multi-core, clusters, and cloud computing systems. To achieve scalable, high-performance computing, our platform (1) employs size-adaptive, distributable block volumes as a core data structure for efficient parallelization of a wide range of 3D image processing algorithms; (2) supports task scheduling for efficient load distribution and balancing; and (3) consists of a layered parallel software libraries that allow a wide range of medical applications to share the same functionalities. We evaluated the performance of our platform by applying it to an electronic cleansing system in virtual colonoscopy, with initial experimental results showing a 10 times performance improvement on an 8-core workstation over the original sequential implementation of the system.","Colon, pathology; Colonography, Computed Tomographic, methods/standards; Humans; Imaging, Three-Dimensional, methods; Software, standards; Time Factors",1
24,Run-time Optimizations for Replicated Dataflows on Heterogeneous Environments,,2010,"The increases in multi-core processor parallelism and in the ﬂexibility of many-core accelerator processors, such as GPUs, have turned traditional SMP systems into hierarchical, heterogeneous computing environments. Fully exploiting these improvements in parallel system design remains an open problem. Moreover, most of the current tools for the development of parallel applications for hierarchical systems concentrate on the use of only a single processor type (e.g., accelerators) and do not coordinate several heterogeneous processors. Here, we show that making use of all of the heterogeneous computing resources can signiﬁcantly improve application performance. Our approach, which consists of optimizing applications at run-time by efﬁciently coordinating application task execution on all available processing units is evaluated in the context of replicated dataﬂow applications. The proposed techniques were developed and implemented in an integrated run-time system targeting both intra- and inter-node parallelism. The experimental results with a real-world complex biomedical application show that our approach nearly doubles the performance of the GPU-only implementation on a distributed heterogeneous accelerator cluster.","GPGPU, filter-stream, run-time optimizations",1
25,Fast multi-core based multimodal registration of 2D cross-sections and 3D datasets.,BMC Bioinformatics,2010,"Solving bioinformatics tasks often requires extensive computational power. Recent trends in processor architecture combine multiple cores into a single chip to improve overall performance. The Cell Broadband Engine (CBE), a heterogeneous multi-core processor, provides power-efficient and cost-effective high-performance computing. One application area is image analysis and visualisation, in particular registration of 2D cross-sections into 3D image datasets. Such techniques can be used to put different image modalities into spatial correspondence, for example, 2D images of histological cuts into morphological 3D frameworks.We evaluate the CBE-driven PlayStation 3 as a high performance, cost-effective computing platform by adapting a multimodal alignment procedure to several characteristic hardware properties. The optimisations are based on partitioning, vectorisation, branch reducing and loop unrolling techniques with special attention to 32-bit multiplies and limited local storage on the computing units. We show how a typical image analysis and visualisation problem, the multimodal registration of 2D cross-sections and 3D datasets, benefits from the multi-core based implementation of the alignment algorithm. We discuss several CBE-based optimisation methods and compare our results to standard solutions. More information and the source code are available from http://cbe.ipk-gatersleben.de.The results demonstrate that the CBE processor in a PlayStation 3 accelerates computational intensive multimodal registration, which is of great importance in biological/medical image processing. The PlayStation 3 as a low cost CBE-based platform offers an efficient option to conventional hardware to solve computational problems in image processing and bioinformatics.","Algorithms; Image Interpretation, Computer-Assisted, methods; Image Processing, Computer-Assisted, methods; Imaging, Three-Dimensional, methods",1
26,A survey of GPU-based medical image computing techniques.,Quant Imaging Med Surg,2012,"Medical imaging currently plays a crucial role throughout the entire clinical applications from medical scientific research to diagnostics and treatment planning. However, medical imaging procedures are often computationally demanding due to the large three-dimensional (3D) medical datasets to process in practical clinical applications. With the rapidly enhancing performances of graphics processors, improved programming support, and excellent price-to-performance ratio, the graphics processing unit (GPU) has emerged as a competitive parallel computing platform for computationally expensive and demanding tasks in a wide range of medical image applications. The major purpose of this survey is to provide a comprehensive reference source for the starters or researchers involved in GPU-based medical image processing. Within this survey, the continuous advancement of GPU computing is reviewed and the existing traditional applications in three areas of medical image processing, namely, segmentation, registration and visualization, are surveyed. The potential advantages and associated challenges of current GPU-based medical imaging are also discussed to inspire future applications in medicine.",,1
27,A review of algorithms for medical image segmentation and their applications to the female pelvic cavity,Computer Methods in Biomechanics and Biomedical Engineering,2010,,,0
28,Segmentation of female pelvic organs in axial magnetic resonance images using coupled geometric deformable models ,Computers in Biology and Medicine,2013,,Image segmentation,0
29,Thinning methodologies-a comprehensive survey,"Pattern Analysis and Machine Intelligence, IEEE Transactions on",1992,"A comprehensive survey of thinning methodologies is presented. A wide range of thinning algorithms, including iterative deletion of pixels and nonpixel-based methods, is covered. Skeletonization algorithms based on medial axis and other distance transforms are not considered. An overview of the iterative thinning process and the pixel-deletion criteria needed to preserve the connectivity of the image pattern is given first. Thinning algorithms are then considered in terms of these criteria and their modes of operation. Nonpixel-based methods that usually produce a center line of the pattern directly in one pass without examining all the individual pixels are discussed. The algorithms are considered in great detail and scope, and the relationships among them are explored",iterative methods picture processing connectivity image pattern iterative deletion nonpixel based method picture processing pixel based method thinning Algorithm design and analysis Character recognition Data compression Image analysis Iterative algorithms Pattern analysis Pattern recognition Pixel Skeleton White blood cells,0
30,Parallel adaptive wavelet analysis,Future Generation Computer Systems,2001,"Adapted wavelet analysis in the sense of wavelet packet algorithms is a highly relevant procedure in different types of applications, like, e.g. data compression, feature extraction, classification problems, data analysis, numerical mathematics, etc. Given a large or high-dimensional data set the computational demand is too high for interactive or “nearly-interactive” processing. Therefore, parallel processing is one of the possibilities to accelerate the processing speed. In this case, special attention has to be paid towards handling of the large amount of data in addition to the proper organization of the computations. We investigate different data decomposition approaches, border handling techniques and programming paradigms. The memory consuming decomposition into a given arbitrary basis after adaptive basis choice is resolved by a localized decomposition strategy.",Wavelet packets MIMD algorithms Best basis algorithm,1
31,An XML-based System for Synthesis of Data from Disparate Databases,Journal of the American Medical Informatics Association,2006,"Diverse data sets have become key building blocks of translational biomedical research. Data types captured and referenced by sophisticated research studies include high throughput genomic and proteomic data, laboratory data, data from imagery, and outcome data. In this paper, the authors present the application of an XML-based data management system to support integration of data from disparate data sources and large data sets. This system facilitates management of XML schemas and on-demand creation and management of XML databases that conform to these schemas. They illustrate the use of this system in an application for genotype–phenotype correlation analyses. This application implements a method of phenotype–genotype correlation based on phylogenetic optimization of large data sets of mouse SNPs and phenotypic data. The application workflow requires the management and integration of genomic information and phenotypic data from external data repositories and from the results of phenotype–genotype correlation analyses. Our implementation supports the process of carrying out a complex workflow that includes large-scale phylogenetic tree optimizations and application of Maddison's concentrated changes test to large phylogenetic tree data sets. The data management system also allows collaborators to share data in a uniform way and supports complex queries that target data sets.",,0
32,Large-Scale Biomedical Image Analysis in Grid Environments,"Information Technology in Biomedicine, IEEE Transactions on",2008,"This paper presents the application of a component-based Grid middleware system for processing extremely large images obtained from digital microscopy devices. We have developed parallel, out-of-core techniques for different classes of data processing operations employed on images from confocal microscopy scanners. These techniques are combined into a data preprocessing and analysis pipeline using the component-based middleware system. The experimental results show that: 1) our implementation achieves good performance and can handle very large datasets on high-performance Grid nodes, consisting of computation and/or storage clusters and 2) it can take advantage of Grid nodes connected over high-bandwidth wide-area networks by combining task and data parallelism.",biomedical optical imaging grid computing medical image processing middleware optical microscopy Grid-node biomedical image analysis component-based Grid middleware system confocal microscopy scanner data preprocessing digital microscopy image processing parallel computation wide-area network Digital microscopy digital microscopy image processing out of core out-of- core parallel computation Database Management Systems Image Interpretation Computer-Assisted Information Dissemination Information Storage and Retrieval Internet Microscopy Radiology Information Systems Signal Processing Computer-Assisted,0
33,Simplified implementation of medical image processing algorithms into a grid using a workflow management system,Future Generation Computer Systems,2010,"In this paper, we describe the grid integration of medical image processing applications as grid workflows. The workflow management system is able to execute all tasks related to grid communication, such as authorization, scheduling and monitoring. It remains to the developer to make the code accessible for the workflow manager, and to define, what to do with it. Coarse-grained parallelization of processing steps for runtime reduction can easily be realized. We describe the procedure how to port the code to the grid and show exemplarily the integration of segmentation and registration algorithms for transrectal ultrasound guided prostate biopsies.",Workflow management Healthgrids Image processing software,0
34,3D medical volume reconstruction using web services,Computers in Biology and Medicine,2008,"We address the problem of 3D medical volume reconstruction using web services. The use of proposed web services is motivated by the fact that the problem of 3D medical volume reconstruction requires significant computer resources and human expertise in medical and computer science areas. Web services are implemented as an additional layer to a dataflow framework called data to knowledge. In the collaboration between UIC and NCSA, pre-processed input images at NCSA are made accessible to medical collaborators for registration. Every time UIC medical collaborators inspected images and selected corresponding features for registration, the web service at NCSA is contacted and the registration processing query is executed using the image to knowledge library of registration methods. Co-registered frames are returned for verification by medical collaborators in a new window. In this paper, we present 3D volume reconstruction problem requirements and the architecture of the developed prototype system at http://isda.ncsa.uiuc.edu/MedVolume. We also explain the tradeoffs of our system design and provide experimental data to support our system implementation. The prototype system has been used for multiple 3D volume reconstructions of blood vessels and vasculogenic mimicry patterns in histological sections of uveal melanoma studied by fluorescent confocal laser scanning microscope.",Web services architecture Workflow Medical image analysis Test case,0
35,HEp-2 cell pattern classification with discriminative dictionary learning,Pattern Recognition,2014," The paper presents a supervised discriminative dictionary learning algorithm specially designed for classifying HEp-2 cell patterns. The proposed algorithm is an extension of the popular K-SVD algorithm: at the training phase, it takes into account the discriminative power of the dictionary atoms and reduces their intra-class reconstruction error during each update. Meanwhile, their inter-class reconstruction effect is also considered. Compared to the existing extension of K-SVD, the proposed algorithm is more robust to parameters and has better discriminative power for classifying HEp-2 cell patterns. Quantitative evaluation shows that the proposed algorithm outperforms general object classification algorithms significantly on standard HEp-2 cell patterns classifying benchmark11 An early version of this algorithm ranks 2nd out of 28 algorithms in the 1st international contest on HEp-2 Cells classification [1]. and also achieves competitive performance on standard natural image classification benchmark.",HEp-2 cell classification Sparse representation Image coding Dictionary learning Image classification Singular value decomposition,1
36,An introduction to neural computing,Neural Networks,1988,"This article contains a brief survey of the motivations, fundamentals, and applications of artificial neural networks, as well as some detailed analytical expressions for their theory.",Neural computing Neural networks Adaptive systems Learning machines Pattern recognition,0
37,Patient-specific geometry modeling and mesh generation for simulating Obstructive Sleep Apnea Syndrome cases by Maxillomandibular Advancement,Mathematics and Computers in Simulation,2011,"The objective of this paper is the reconstruction of upper airway geometric models as hybrid meshes from clinically used Computed Tomography (CT) data sets in order to understand the dynamics and behaviors of the pre- and postoperative upper airway systems of Obstructive Sleep Apnea Syndrome (OSAS) patients by viscous Computational Fluid Dynamics (CFD) simulations. The selection criteria for OSAS cases studied are discussed because two reasonable pre- and postoperative upper airway models for CFD simulations may not be created for every case without a special protocol for CT scanning. The geometry extraction and manipulation methods are presented with technical barriers that must be overcome so that they can be used along with computational simulation software as a daily clinical evaluation tool. Eight cases are presented in this paper, and each case consists of pre- and postoperative configurations. The results of computational simulations of two cases are included in this paper as demonstration.",Image processing Mesh generation Computational Fluid Dynamics (CFD) Computed Tomography (CT) Obstructive Sleep Apnea Syndrome (OSAS) Maxillomandibular Advancement (MMA),0
38,Automatic spinal canal detection in lumbar MR images in the sagittal view using dynamic programming,Computerized Medical Imaging and Graphics,2014," As there is an increasing need for the computer-aided effective management of pathology in lumbar spine, we have developed a computer-aided diagnosis and characterization framework using lumbar spine MRI that provides radiologists a second opinion. In this paper, we propose a left spinal canal boundary extraction method, based on dynamic programming in lumbar spine MRI. Our method fuses the absolute intensity difference of T1-weighted and T2-weighted sagittal images and the inverted gradient of the difference image into a dynamic programming scheme and works in a fully automatic fashion. The boundaries generated by our method are compared against reference boundaries in terms of the Euclidean distance and the Chebyshev distance. The experimental results from 85 clinical data show that our methods find the boundary with a mean Euclidean distance of 3 mm, achieving a speedup factor of 167 compared with manual landmark extraction. The proposed method successfully  landmarks automatically and fits well with our framework for computer-aided diagnosis in lumbar spine.",Boundary extraction Lumbar spine Magnetic resonance imaging Computer-aided diagnosis,0
39,Adjoint-based inverse analysis of windkessel parameters for patient-specific vascular models,Journal of Computational Physics,2013,"A human circulatory system is composed of more than 50,000 miles of blood vessels. Such a huge network of vessels is responsible for the elevated pressure values within large arteries. As such, modeling of large blood arteries requires a full modeling of circulatory system. This in turn is computationally not affordable. Thus, a multiscale modeling of the arterial network is a necessity. The multiscale approach is achieved through, first, modeling the arterial regions of interest with 3D models and the rest of the circulatory network with reduced-dimensional (reduced-D) models, then coupling the multiscale domains together. Though reduced-D models can well reproduce physiology, calibrating them to fit 3D patient-specific Fluid Structure Interaction (FSI) geometries has received little attention. For this reason, this work develops calibration methods for reduced-D models using adjoint based methods. We also propose a reduced modeling complexity (RMC) approach that reduces the calibration cost of expensive FSI models using pure fluid modeling. Finally, all of the developed calibration techniques are tested on patient-specific arterial geometries, showing the power and stability of the proposed calibration methods.",Adjoint Windkessel Fluid–Structure Interaction Computational fluid dynamics Coupled 3D-0D modeling Multiscale hemodynamics,0
40,A data distributed parallel algorithm for nonrigid image registration,Parallel Computing,2005,"Image registration is a technique for defining a geometric relationship between each point in images. This paper presents a data distributed parallel algorithm that is capable of aligning large-scale three-dimensional (3-D) images of deformable objects. The novelty of our algorithm is to overcome the limitations on the memory space as well as the execution time. In order to enable this, our algorithm incorporates data distribution, data-parallel processing, and load balancing techniques into Schnabel’s registration algorithm that realizes robust and efficient alignment based on information theory and adaptive mesh refinement. We also present some experimental results obtained on a 128-CPU cluster of PCs interconnected by Myrinet and Fast Ethernet switches. The results show that our algorithm requires less amount of memory resources, so that aligns datasets up to 1024 × 1024 × 590 voxel images with reducing the execution time from hours to minutes, a clinically compatible time.",Nonrigid image registration Adaptive mesh refinement Free-form deformation Data distribution Load balancing,1
41,O3-DPACS Open-Source Image-Data Manager/Archiver and HDW2 Image-Data Display: An IHE-compliant project pushing the e-health integration in the world,Computerized Medical Imaging and Graphics,2006,"After many years of study, development and experimentation of open PACS and Image workstation solutions including management of medical data and signals (DPACS project), the research and development at the University of Trieste have recently been directed towards Java-based, IHE compliant and multi-purpose servers and clients. In this paper an original Image-Data Manager/Archiver (O3-DPACS) and a universal Image-Data Display (HDW2) are described. O3-DPACS is also part of a new project called Open Three (O3) Consortium, promoting Open Source adoption in e-health at European and world-wide levels. This project aims to give a contribution to the development of e-health through the study of Healthcare Information Systems and the contemporary proposal of new concepts, designs and solutions for the management of health data in an integrated environment: hospitals, Regional Health Information Organizations and citizens (home-care, mobile-care and ambient assisted living).",,0
42,"Electrical defibrillation optimization: an automated, iterative parallel finite-element approach","Biomedical Engineering, IEEE Transactions on",1997,"To date, optimization of electrode systems for electrical defibrillation has been limited to hand-selected electrode configurations. Here, the authors present an automated approach which combines detailed, three-dimensional (3-D) finite-element torso models with optimization techniques to provide a flexible analysis and design tool for electrical defibrillation optimization. Specifically, a parallel direct search (PDS) optimization technique is used with a representative objective function to find an electrode configuration which corresponds to the satisfaction of a postulated defibrillation criterion with a minimum amount of power and a low possibility of myocardium damage. For adequate representation of the thoracic inhomogeneities, 3-D finite-element torso models are used in the objective function computations. The CPU-intensive finite-element calculations required for the objective function evaluation have been implemented on a message-passing parallel computer in order to complete the optimization calculations in a timely manner. To illustrate the optimization procedure, it has been applied to a representative electrode configuration for transmyocardial defibrillation, namely the subcutaneous patch-right ventricular catheter (SP-RVC) system. Sensitivity of the optimal solutions to various tissue conductivities has been studied. Results for the optimization of defibrillation systems are presented which demonstrate the feasibility of the approach.",defibrillators finite element analysis iterative methods medical computing optimisation physiological models CPU-intensive finite-element calculations automated iterative parallel finite-element approach electrical defibrillation optimization electrode configuration hand-selected electrode configurations message-passing parallel computer myocardium damage objective function computations postulated defibrillation criterion subcutaneous patch-right ventricular catheter system thoracic inhomogeneities three-dimensional finite-element torso models transmyocardial defibrillation Catheters Concurrent computing Conductivity Defibrillation Design optimization Electrodes Finite element methods Iterative methods Myocardium Torso Algorithms Animals Anisotropy Dogs Electric Conductivity Electric Countershock Electric Impedance Electrodes Heart Image Processing Computer-Assisted Models Cardiovascular Muscle Skeletal Radiography Thoracic Sensitivity and Specificity,0
43,Distributed and collaborative visualization of large data sets using high-speed networks,Future Generation Computer Systems,2006,"We describe an architecture for distributed collaborative visualization that integrates video conferencing, distributed data management and grid technologies as well as tangible interaction devices for visualization. High-speed, low-latency optical networks support high-quality collaborative interaction and remote visualization of large data.",Collaborative visualization Distributed applications Co-scheduling Interaction devices Video conferencing,
44,Gradient flows and variational principles for cardiac electrophysiology: Toward efficient and robust numerical simulations of the electrical activity of the heart,Computer Methods in Applied Mechanics and Engineering,2014," The computer simulation of the electrical activity of the heart has experienced tremendous advances in the last decade. However, the acceptance of computational methods in the medical community will largely depend on their reliability, efficiency and robustness. In this work, we present a gradient-flow reformulation of the cardiac electrophysiology equations, and propose a minimax variational principle for the time-discretized electrophysiology problem. Based on results from variational analysis, we derive bounds on the time-step size that guarantee the existence and uniqueness of the saddle point, and in turn of the weak solution of the electrophysiology incremental problem. We also show conditions under which the minimax problem is equivalent to an effective minimization principle, which is amenable to a Rayleigh–Ritz finite-element analysis. The derived time-step bounds guarantee the strict convexity of the objective function resulting from spatial discretization, thus ensuring the convergence of gradient-descent methods. The proposed theory is applied to the widely employed FitzHugh–Nagumo model, which is shown to conform to the variational framework proposed in this work. The applicability of the method and its implications on the robustness of time integration are demonstrated by way of numerical simulations of the electrical behavior in a single-cell and 3D wedge and biventricular geometries. We envision that the proposed framework will open the door to the development of robust and efficient electrophysiology models and simulations.",Cardiac electrophysiology Variational principles Gradient flow FitzHugh–Nagumo Saddle-point variational problems,
45,Optimizing grid computing configuration and scheduling for geospatial analysis: An example with interpolating DEM,Computers & Geosciences,2011,"Many geographic analyses are very time-consuming and do not scale well when large datasets are involved. For example, the interpolation of DEMs (digital evaluation model) for large geographic areas could become a problem in practical application, especially for web applications such as terrain visualization, where a fast response is required and computational demands exceed the capacity of a traditional single processing unit conducting serial processing. Therefore, high performance and parallel computing approaches, such as grid computing, were investigated to speed up the geographic analysis algorithms, such as DEM interpolation. The key for grid computing is to configure an optimized grid computing platform for the geospatial analysis and optimally schedule the geospatial tasks within a grid platform. However, there is no research focused on this. Using DEM interoperation as an example, we report our systematic research on configuring and scheduling a high performance grid computing platform to improve the performance of geographic analyses through a systematic study on how the number of cores, processors, grid nodes, different network connections and concurrent request impact the speedup of geospatial analyses. Condor, a grid middleware, is used to schedule the DEM interpolation tasks for different grid configurations. A Kansas raster-based DEM is used for a case study and an inverse distance weighting (IDW) algorithm is used in interpolation experiments.",Geospatial cyberinfrastructure DEM interpolation HPC Grid computing Scheduling System performance,
46,Binary image algebra and optical cellular logic processor design,"Computer Vision, Graphics, and Image Processing",1989,"Techniques for digital optical cellular image processing are presented. A binary image algebra (BIA), built from five elementary images and three fundamental operations, serves as its software and leads to a formal parallel language approach to the design of parallel binary image processing algorithms. Its applications and relationships with other computing theories demonstrate that BIA is a powerful systematic tool for formalizing and analyzing parallel algorithms. Digital optical cellular image processors (DOCIPs), based on cellular automata and cellular logic architectures, serve as its hardware and implement parallel binary image processing tasks efficiently. An algebraic structure provides a link between the algorithms of BIA and architectures of DOCIP. Optical computing suggests an efficient and high-speed implementation of the DOCIP architectures because of its inherent parallelism and 3D global free interconnection capabilities. Finally, the instruction set and the programming of the DOCIPs are illustrated.",,
47,Time-efficient sparse analysis of histopathological whole slide images,Computerized Medical Imaging and Graphics,2011,"Histopathological examination is a powerful standard for the prognosis of critical diseases. But, despite significant advances in high-speed and high-resolution scanning devices or in virtual exploration capabilities, the clinical analysis of whole slide images (WSI) largely remains the work of human experts. We propose an innovative platform in which multi-scale computer vision algorithms perform fast analysis of a histopathological WSI. It relies on application-driven for high-resolution and generic for low-resolution image analysis algorithms embedded in a multi-scale framework to rapidly identify the high power fields of interest used by the pathologist to assess a global grading. GPU technologies as well speed up the global time-efficiency of the system. Sparse coding and dynamic sampling constitute the keystone of our approach. These methods are implemented within a computer-aided breast biopsy analysis application based on histopathology images and designed in collaboration with a pathology department. The current ground truth slides correspond to about 36,000 high magnification (40×) high power fields. The processing time to achieve automatic WSI analysis is on a par with the pathologist's performance (about ten minutes a WSI), which constitutes by itself a major contribution of the proposed methodology.",Digitized histopathology Breast cancer grading Whole slide image Multi-scale analysis Dynamic sampling Virtual microscopy Graphics processing unit,
48,Interactive Workflow-based Infrastructure for Urgent Computing,Procedia Computer Science,2013," Workflow became a mainstream formalism for complex scientific problems’ representation and is applied to different domains. In this paper we propose and analyze the interactive workflow model as the base for urgent computing (UC) infrastructures. Majority of research works in the area of urgent computing is focused on the deadline-driven scheduling issues for the existing high performance computing. This work tries to look at the problem of building a workflow-driven UC infrastructure from another side and to take into account the interactive nature of decision support. Workflow formalism is taken as the base for investigation. After analyzing the peculiarities of workflow-based UC infrastructure we propose an extended interactive workflow model. This model unifies the interactive capabilities of workflows and allows interactive systems to be described by consolidation of heterogeneous resources: high performance computing, users, software, external devices and data sources. This approach is represented on the crowd management application and is partially shown in the implemented prototype.",urgent computing workflow management system interactive workflow computational steering,
49,Validation of a parallel genetic algorithm for image reconstruction from projections,Journal of Parallel and Distributed Computing,2003,"The problem of accurate image reconstruction from projections has repeatedly arisen over the last decades in a large number of scientific, medical and technical fields. Reconstruction algorithms use data from electron microscopes to reconstruct molecular structures or X-ray projection data to compute medical images. Usually, the applied projection data are noisy and therefore iterative algorithms are used to solve numerically a number of equations. Theory and empirical results demonstrate that genetic algorithms (GA) can accurately solve a broad class of problems, especially if noisy input data are used. GA are based on the evolution of random tries by individuals, and therefore the time to find an appropriate solution is rather long. In this work, we use a parallel approach using JavaSpaces to speed up a genetic reconstruction algorithm.",Parallel genetic algorithm Reconstruction Image processing JavaSpaces,
50,A Problem Solving Environment Portal for Multidisciplinary Design Optimization,Advances in Engineering Software,2009,"Multidisciplinary Design Optimization (MDO) is a design methodology that derives optimal design solutions by concurrently considering various mutually dependent design elements from an assortment of disciplines. As such, it is applicable to the designing of ships and automobiles, as well as to aero vehicles. However, applying MDO methodologies in the real world would require a designer to spend an enormous amount of time arranging and integrating resources used in the process. This paper proposes a Problem Solving Environment (PSE) Portal for MDO methodologies, providing an environment that enables designers to utilize design resources conveniently even without working knowledge of the systems. Furthermore, the PSE portal yields an optimal MDO environment by allowing for global collaborative sites, which securely share design resources, and by offering users an efficient interface.",Problem Solving Environment Portal Multidisciplinary Design Optimization Globus toolkit Web services,
51,Real-Time Volume Rendering Visualization of Dual-Modality PET/CT Images With Interactive Fuzzy Thresholding Segmentation,"Information Technology in Biomedicine, IEEE Transactions on",2007,"Three-dimensional (3-D) visualization has become an essential part for imaging applications, including image-guided surgery, radiotherapy planning, and computer-aided diagnosis. In the visualization of dual-modality positron emission tomography and computed tomography (PET/CT), 3-D volume rendering is often limited to rendering of a single image volume and by high computational demand. Furthermore, incorporation of segmentation in volume rendering is usually restricted to visualizing the pre-segmented volumes of interest. In this paper, we investigated the integration of interactive segmentation into real-time volume rendering of dual-modality PET/CT images. We present and validate a fuzzy thresholding segmentation technique based on fuzzy cluster analysis, which allows interactive and real-time optimization of the segmentation results. This technique is then incorporated into a real-time multi-volume rendering of PET/CT images. Our method allows a real-time fusion and interchangeability of segmentation volume with PET or CT volumes, as well as the usual fusion of PET/CT volumes. Volume manipulations such as window level adjustments and lookup table can be applied to individual volumes, which are then fused together in real time as adjustments are made. We demonstrate the benefit of our method in integrating segmentation with volume rendering in its application to PET/CT images. Responsive frame rates are achieved by utilizing a texture-based volume rendering algorithm and the rapid transfer capability of the high-memory bandwidth available in low-cost graphic hardware",computerised tomography data visualisation fuzzy set theory image fusion image segmentation image texture interactive systems medical image processing pattern clustering positron emission tomography rendering (computer graphics) computed tomography computer-aided diagnosis dual-modality PET-CT images fuzzy C-means cluster analysis graphic hardware image-guided surgery interactive fuzzy thresholding segmentation technique interchangeability positron emission tomography radiotherapy planning rapid transfer capability real-time 3-D volume rendering real-time fusion real-time optimization texture-based volume rendering algorithm three-dimensional data visualization Application software Bandwidth Computed tomography Computer aided diagnosis Image segmentation Positron emission tomography Rendering (computer graphics) Surgery Table lookup Visualization Dual-modality positron emission tomography and computed tomography (PET/CT) fuzzy $C$-means cluster analysis interactive three-dimensional (3-D) segmentation multi-volume rendering real-time volume rendering Algorithms Artificial Intelligence Computer Systems Fuzzy Logic Image Enhancement Image Interpretation Computer-Assisted Imaging Three-Dimensional Information Storage and Retrieval Pattern Recognition Automated Positron-Emission Tomography Reproducibility of Results Sensitivity and Specificity Subtraction Technique Tomography X-Ray Computed User-Computer Interface,
52,Automatic navigation path generation based on two-phase adaptive region-growing algorithm for virtual angioscopy,Medical Engineering & Physics,2006,"In this paper, we propose a fast and automated navigation path generation algorithm to visualize inside of carotid artery using MR angiography images. The carotid artery is one of the body regions not accessible by real optical probe but can be visualized with virtual endoscopy. By applying two-phase adaptive region-growing algorithm, the carotid artery segmentation is started at the initial seed, which is located on the initially thresholded binary image. This segmentation algorithm automatically detects the branch position with stack feature. Combining with a priori knowledge of anatomic structure of carotid artery, the detected branch position is used to separate the carotid artery into internal carotid artery and external carotid artery. A fly-through path is determined to automatically move the virtual camera based on the intersecting coordinates of two bisectors on the circumscribed quadrangle of segmented carotid artery. In consideration of the interactive rendering speed and the usability of standard graphic hardware, endoscopic view of carotid artery is generated by using surface rendering algorithm with perspective projection method. In addition, the endoscopic view is provided with ray casting algorithm for off-line navigation of carotid artery. Experiments have been conducted on both mathematical phantom and clinical data sets. This algorithm is more effective than key-framing and topological thinning method in terms of automated features and computing time. This algorithm is also applicable to generate the centerline of renal artery, coronary artery, and airway tree which has tree-like cylinder shape of organ structures in the medical imagery.",Carotid artery Virtual angioscopy Adaptive region growing Navigation path Medical image segmentation,
53,"Impact of HIPAA provisions on the stock market value of healthcare institutions, and information security and other information technology firms",Computers & Security,2012,"Title 1 of the Health Insurance Portability and Accountability Act (HIPAA) was enacted to improve the portability of healthcare insurance coverage and Title II was intended to alleviate fraud and abuse. The development of a health information system was suggested in Title II of HIPAA as a means of promoting standardization to improve the efficiency of the healthcare system and ensure that electronic healthcare information is transferred securely and kept private. Since the legislation places the onus of providing the described improvements on healthcare institutions and part of these requirements relate to information technology (IT) and information security (IS), the process of complying with the legislation will necessitate acquiring products and services from IT/IS firms. From the viewpoint of stock market analysts, this increase in demand for IT/IS products and services has the potential to boost the profitability of public IT/IS firms, in turn positively enhancing their stock market valuation. Following the same logic, the legislation's compliance burdens shared by healthcare firms are expected to require hefty costs, thus potentially reducing the profitability of healthcare firms and reflecting negatively on their stock price. The intent of this paper is to evaluate the stock market reaction to the introduction of HIPAA legislation by evaluating the abnormal movement in the price of the stock of public healthcare institutions, IT, and IS firms. We conduct event-study analyses around the announcement dates of the various provisions of HIPAA. An event study is a standard statistical methodology used to determine whether the occurrence of a specific event or events results in a statistically significant reaction in financial markets. The advantage of the event study methodology for policy analysis is that it provides an anchor for determining value, which eliminates reliance on ad hoc judgments about the impact of specific events or policies on stock prices. While event studies have been conducted that examine the market effect of security and privacy breaches on firms, none has attempted to determine the impact, in terms of resulting market reaction, of the HIPAA legislation itself. The results of the study confirm the logic above, while also providing insight into specific stages of the legislative path of HIPAA.",HIPAA Healthcare Information technology Information security Event study,
54,Holographic image archive,Computerized Medical Imaging and Graphics,1996,"This paper presents and associative technique for content-based retrieval into image archive, based on a computing paradigm called Multidimensional Holographic Associative Computing (MHAC). Unlike any prior Artificial Associative Memory (AAM), MHAC has the unique ability to focus on any subset of pixels in the sample image and retrieve learned images based on the similarity of the visual objects. In addition, MHAC is adaptive, graciously accommodative of imprecision, efficient, parallelizable, scalable and optically realizable. Together, these excellent properties of MHAC offer a promising novel approach to a content-based search into massive image archives. The paper presents the necessary transformational steps to incorporate this new mechanism into a complete image archival and retrieval system. This is the first associative search approach for content-based retrieval in image repository. The results show that this search system is capable of retrievals by using pattern objects as small as 10–15% of the query image frame at better than 90% accuracy. This demonstrates the potential of MHAC for handing content-based image applications far beyond the capability of current associative memories. The design, methodology and performance of this system have been illustrated in this paper through its application in managing a Medical Image Archive (MEDIA).",Associative memory Content-based retrieval Attention,
55,Computer-based imaging and interventional MRI: applications for neurosurgery,Computerized Medical Imaging and Graphics,1999,"Advances in computer technology and the development of open MRI systems definitely enhanced intraoperative image-guidance in neurosurgery. Based upon the integration of previously acquired and processed 3D information and the corresponding anatomy of the patient, this requires computerized image-processing methods (segmentation, registration, and display) and fast image integration techniques. Open MR systems equipped with instrument tracking systems, provide an interactive environment in which biopsies and minimally invasive interventions or open surgeries can be performed. Enhanced by the integration of multimodal imaging these techniques significantly improve the available treatment options and can change the prognosis for patients with surgically treatable diseases.",Interventional MRI Image-guidance Minimally invasive therapy Neurosurgery Surgical planning,
56,Parallel reflective symmetry transformation for volume data,Computers & Graphics,2008,"Many volume data possess symmetric features that can be clearly observed, for example, those existing in diffusion tensor images. The exploitations of symmetries for volume datasets, however, are relatively limited due to the prohibitive computational cost of detecting the symmetries. In this paper we present an efficient parallel algorithm for symmetry computation in volume data represented by regular grids. Optimization is achieved by converting the raw data into a hierarchical tree-like structure. We use a novel algorithm to partition the tree and distribute the data among processors to minimize the data dependency at run time. The computed symmetries are useful for several volume visualization applications, including optimal view selection and slice position exploration.",Symmetry Volume rendering Parallel computing,
57,Medical image processing utilizing neural networks trained on a massively parallel computer,Computers in Biology and Medicine,1995,"While finding many applications in science, engineering, and medicine, artificial neural networks (ANNs) have typically been limited to small architectures. In this paper, we demonstrate how very large architecture neural networks can be trained for medical image processing utilizing a massively parallel, single-instruction multiple data (SIMD) computer. The two- to three-orders of magnitude improvement in processing time attainable using a parallel computer makes it practical to train very large architecture ANNs. As an example we have trained several ANNs to demonstrate the tomographic reconstruction of 64 × 64 single photon emission computed tomography (SPECT) images from 64 planar views of the images. The potential for these large architecture ANNs lies in the fact that once the neural network is properly trained on the parallel computer the corresponding interconnection weight file can be loaded on a serial computer. Subsequently, relatively fast processing of all novel images can be performed on a PC or workstation.",Massively parallel computing Image processing Neural networks Tomography Single photon emission computed tomography (SPECT),1
58,"3D visualization, analysis, and treatment of the prostate using trans-urethral ultrasound",Computerized Medical Imaging and Graphics,2003,"In the year 2000, it is estimated that over 20,000 men underwent transperineal interstitial permanent prostate brachytherapy (TIPPB) for treatment of prostate cancer. Trans-urethral ultrasound (TUUS) is a new interactive, real-time 3D imaging method that may be effective in therapy-guidance during and after TIPPB. TUUS provides higher resolution than trans-rectal ultrasound (TRUS). TUUS can be used to accurately localize radioactive seeds and therefore contribute to more accurate determination of radiation dose distribution throughout the tissue after the completion of the procedure, similar to information currently provided by expensive and offline CT scans. A TUUS catheter can be used to acquire 2D section images or 3D volume images for detailed analyses of the prostate and associated tissue. Initial development of TUUS imaging was carried out on an ultrasound-equivalent prostate phantom with cylindrical dummy radiation sources. This was followed by preliminary studies in animals and then in patients. Both CT and TRUS data were acquired in these studies for comparative purposes. Segmentation of the prostate capsule and radioactive seeds was carried out using several semi-automated 3D algorithms and image processing techniques. Presentation of the data to the clinician is provided by a variety of complementary 2D and 3D display methods. In comparison with the CT data, TUUS data provided both greater spatial resolution and better soft tissue differentiation. In comparison to the TRUS data, TUUS data provided greater resolution and better seed localization. Combining these advantages suggests the possibility of TUUS becoming the exclusive imaging method in prostate cancer brachytherapy.",Trans-urethral ultrasound Prostate imaging Prostate neoplasms/(radiotherapy) Brachytherapy,
59,Wave dispersion in the hybrid-Vlasov model: Verification of Vlasiator,Physics of Plasmas,2013,"Vlasiator is a new hybrid-Vlasov plasma simulation code aimed at simulating the entire magnetosphere of the Earth. The code treats ions (protons) kinetically through Vlasov's equation in the six-dimensional phase space while electrons are a massless charge-neutralizing fluid [M. Palmroth, J. Atmos. Sol.-Terr. Phys. 99, 41 (2013); A. Sandroos, Parallel Comput. 39, 306 (2013)]. For first global simulations of the magnetosphere, it is critical to verify and validate the model by established methods. Here, as part of the verification of Vlasiator, we characterize the low- plasma wave modes described by this model and compare with the solution computed by the Waves in Homogeneous, Anisotropic Multicomponent Plasmas (WHAMP) code [K. Ronnmark, Kiruna Geophysical Institute Reports No. 179, 1982], using dispersion curves and surfaces produced with both programs. The match between the two fundamentally different approaches is excellent in the low-frequency, long wavelength range which is of interest in global magnetospheric simulations. The left-hand and right-hand polarized wave modes as well as the Bernstein modes in the Vlasiator simulations agree well with the WHAMP solutions. Vlasiator allows a direct investigation of the importance of the Hall term by including it in or excluding it from Ohm's law in simulations. This is illustrated showing examples of waves obtained using the ideal Ohm's law and Ohm's law including the Hall term. Our analysis emphasizes the role of the Hall term in Ohm's law in obtaining wave modes departing from ideal magnetohydrodynamics in the hybrid-Vlasov model. 2013 AIP Publishing LLC.",Computer simulation Dispersion (waves) Magnetohydrodynamics Magnetosphere Phase space methods Sols Vlasov equation,
60,Distributed system for processing 3D medical images,Computers in Biology and Medicine,1997,"Three-dimensional (3D) image data generated by radiological imaging modalities such as CT and MRI can provide detailed structural insight. Automating the analysis of these images can improve the consistency of the results and reduce user interaction time, but introduces a tremendous computational burden. To address this problem, we have designed a distributed processing environment for the rapid processing of 3D medical images. Our system allows a user to perform automatic 3D filtering, segmentation, and measurement on a 3D image using a heterogeneous network of processors and the PVM protocol.",3D medical image analysis PVM Massively parallel processing High performance computing Visualization,
61,Distributed system for processing 3D medical images,Computers in Biology and Medicine,1997,"Three-dimensional (3D) image data generated by radiological imaging modalities such as CT and MRI can provide detailed structural insight. Automating the analysis of these images can improve the consistency of the results and reduce user interaction time, but introduces a tremendous computational burden. To address this problem, we have designed a distributed processing environment for the rapid processing of 3D medical images. Our system allows a user to perform automatic 3D filtering, segmentation, and measurement on a 3D image using a heterogeneous network of processors and the PVM protocol. (C) 1997 Elsevier Science Ltd.",,
62,Distributed system for processing multidimensional radiological images,,1996,,,
63,The UK e-Science Core Programme and the Grid,Future Generation Computer Systems,2002,"This paper describes the £120M UK ‘e-Science’ (http://www.research-councils.ac.uk/ and http://www.escience-grid.org.uk) initiative and begins by defining what is meant by the term e-Science. The majority of the £120M, some £75M, is funding large-scale e-Science pilot projects in many areas of science and engineering. The infrastructure needed to support such projects must permit routine sharing of distributed and heterogeneous computational and data resources as well as supporting effective collaboration between groups of scientists. Such an infrastructure is commonly referred to as the Grid. Apart from £10M towards a Teraflop computer, the remaining funds, some £35M, constitute the e-Science ‘Core Programme’. The goal of this Core Programme is to advance the development of robust and generic Grid middleware in collaboration with industry. The key elements of the Core Programme will be outlined including details of a UK e-Science Grid testbed. The pilot e-Science projects that have so far been announced are then briefly described. These projects span a range of disciplines from particle physics and astronomy to engineering and healthcare, and illustrate the breadth of the UK e-Science Programme. In addition to these major e-Science projects, the Core Programme is funding a series of short-term e-Science demonstrators across a number of disciplines as well as projects in network traffic engineering and some international collaborative activities. We conclude with some remarks about the need to develop a data architecture for the Grid that will allow federated access to relational databases as well as flat files.",e-Science Core Programme Grid,
64,Deployment of one-sided communication technique for parallel computing in Katsevich CT image reconstruction,,2006,"This paper focuses' on the implementation of parallel! approach of Katsevich CT image reconstruction algorithm using one-sided communication technique. One-sided communication is a new feature provided by MPI-2 standard. It natively supports new network techniques such as InfiniBand and Myrinet. In this project, we implement three kinds of one-sided approaches for Katsevich algorithm. The results are compared the other P2P models on the performance metrics - -speedup, efficiency, and cost.",,
65,Fast connected-component labeling,Pattern Recognition,2009,"Labeling of connected components in a binary image is one of the most fundamental operations in pattern recognition: labeling is required whenever a computer needs to recognize objects (connected components) in a binary image. This paper presents a fast two-scan algorithm for labeling of connected components in binary images. We propose an efficient procedure for assigning provisional labels to object pixels and checking label equivalence. Our algorithm is very simple in principle, easy to implement, and suitable for hardware and parallel implementation. We show the correctness of our algorithm, analyze its complexity, and compare it with other labeling algorithms. Experimental results demonstrated that our algorithm is superior to conventional labeling algorithms.",Labeling algorithm Label equivalence Connected component Linear-time algorithm Pattern recognition,
66,Histology image analysis for carcinoma detection and grading,Computer Methods and Programs in Biomedicine,2012,"This paper presents an overview of the image analysis techniques in the domain of histopathology, specifically, for the objective of automated carcinoma detection and classification. As in other biomedical imaging areas such as radiology, many computer assisted diagnosis (CAD) systems have been implemented to aid histopathologists and clinicians in cancer diagnosis and research, which have been attempted to significantly reduce the labor and subjectivity of traditional manual intervention with histology images. The task of automated histology image analysis is usually not simple due to the unique characteristics of histology imaging, including the variability in image preparation techniques, clinical interpretation protocols, and the complex structures and very large size of the images themselves. In this paper we discuss those characteristics, provide relevant background information about slide preparation and interpretation, and review the application of digital image processing techniques to the field of histology image analysis. In particular, emphasis is given to state-of-the-art image segmentation methods for feature extraction and disease classification. Four major carcinomas of cervix, prostate, breast, and lung are selected to illustrate the functions and capabilities of existing CAD systems.",Histopathology Carcinoma Histology image analysis Image segmentation Feature extraction Computed assisted diagnosis,
67,Interfacing to distributed active data archives,Future Generation Computer Systems,1999,"The general problem of managing large data archives or libraries of digital data is particularly challenging when the system must cope with active data which is processed on-demand. Conventional data archives are designed so that data is ingested into the system, and retrieved by users to satisfy particular requests. An active data archive can be defined as one where much of the data is generated on-demand, as value-added data products or services derived from existing data holdings. Interfacing users and applications to such an archive system requires a more complex infrastructure than conventional or passive archive systems. We describe two archive systems we have built for providing web-based access to satellite and geospatial imagery as well as to medical imagery such as that from the Visible Human datasets. We contrast the requirements and features of these archives and discuss the Java and CORBA software infrastructure that we have developed to interface to them. We also describe our integration of commercial products, such as StudioCentral and Informix with our software and some of the geospatial standard interface definitions, such as GIAS and OpenGIS.",Digital library Active data On-line archive Data services Geospatial imagery Visible human data,
68,Wave dispersion in the hybrid-Vlasov model: verification of Vlasiator,Physics of Plasmas,2013,"Vlasiator is a new hybrid-Vlasov plasma simulation code aimed at simulating the entire magnetosphere of the Earth. The code treats ions (protons) kinetically through Vlasov's equation in the six-dimensional phase space while electrons are a massless charge-neutralizing fluid [M. Palmroth et al., J. Atmos. Sol.-Terr. Phys. 99, 41 (2013); A. Sandroos et al., Parallel Comput. 39, 306 (2013)]. For first global simulations of the magnetosphere, it is critical to verify and validate the model by established methods. Here, as part of the verification of Vlasiator, we characterize the low- plasma wave modes described by this model and compare with the solution computed by the Waves in Homogeneous, Anisotropic Multicomponent Plasmas (WHAMP) code [K. Ronnmark, Kiruna Geophysical Institute Reports No. 179, 1982], using dispersion curves and surfaces produced with both programs. The match between the two fundamentally different approaches is excellent in the low-frequency, long wavelength range which is of interest in global magnetospheric simulations. The left-hand and right-hand polarized wave modes as well as the Bernstein modes in the Vlasiator simulations agree well with the WHAMP solutions. Vlasiator allows a direct investigation of the importance of the Hall term by including it in or excluding it from Ohm's law in simulations. This is illustrated showing examples of waves obtained using the ideal Ohm's law and Ohm's law including the Hall term. Our analysis emphasizes the role of the Hall term in Ohm's law in obtaining wave modes departing from ideal magnetohydrodynamics in the hybrid-Vlasov model.",dispersion (wave) magnetosphere plasma Bernstein waves plasma magnetohydrodynamics plasma simulation plasma transport processes polarisation Vlasov equation,
69,Enhanced bisecting -means clustering using intermediate cooperation,Pattern Recognition,2009,"Bisecting k-means (BKM) is very attractive in many applications as document-retrieval/indexing and gene expression analysis problems. However, in some scenarios when a fraction of the dataset is left behind with no other way to re-cluster it again at each level of the binary tree, a “refinement” is needed to re-cluster the resulting solutions. Current approaches to refine the clustering solutions produced by the BKM employ end-result enhancement using k-means (KM) clustering. In this hybrid model, KM waits for the former BKM to finish its clustering and then it takes the final set of centroids as initial seeds for a better refinement. In this paper, a cooperative bisecting k-means (CBKM) clustering algorithm is presented. The CBKM concurrently combines the results of the BKM and KM at each level of the binary hierarchical tree using cooperative and merging matrices. Undertaken experimental results show that the CBKM achieves better clustering quality than that of KM, BKM, and single linkage (SL) algorithms with comparable time performance over a number of artificial, text documents, and gene expression datasets.",Bisecting clustering Cooperative clustering Quality measures,
70,Volume CAD—CW-complexes based approach,Computer-Aided Design,2005,"In this paper, we propose a novel shape and physical attribute handling system Volume-CAD designated for practical and robust use in the manufacturing process of various industries. The internal data structure ‘Kitta Cube’ is formalized by CW-complexes. Complex is a structure consisting of different dimensional items with consistency (without gap or overlaps), therefore allowing each volume comprising of a material separated by boundaries to be represented completely. Volume-CAD (VCAD11Registered trade mark. ) could be called ‘solid and fluid model’, as it goes beyond the solid model. As applications of VCAD, this paper discusses surface simplification of Kitta Cubes, cell labeling (three-dimensional coloring), and multiple-materials situation. These models can be used not only for manufacturing but also throughout the product life cycle from shipment tracking until the end of the product life cycle.",CW-complex Cell Volume CAD CAE CAM,
71,Distributed and hardware accelerated computing for clinical medical imaging using proton computed tomography (pCT),Journal of Parallel and Distributed Computing,2013," Proton computed tomography (pCT) is an imaging modality that has been in development to support targeted dose delivery in proton therapy. It aims to accurately map the distribution of relative stopping power. Because protons traverse material media in non-linear paths, pCT requires individual proton processing. Image reconstruction then becomes a time-consuming process. Clinical-use scenarios that require images from billions of protons in less than ten or fifteen minutes have motivated us to use distributed and hardware-accelerated computing methods to achieve fast image reconstruction. Combined use of MPI and GPUs demonstrates that clinically viable image reconstruction is possible. On a 60-node CPU/GPU computer cluster, we achieved efficient strong and weak scaling when reconstructing images from two billion histories in under seven minutes. This represents a significant improvement over the previous state-of-the-art in pCT, which took almost seventy minutes to reconstruct an image from 131 million histories on a single-CPU, single-GPU computer.",Computed tomography Proton computed tomography Image reconstruction Iterative reconstruction Relative stopping power RSP MLP GPU PCT CUDA MPI Scaling,
72,A simulated annealing-based optimal threshold determining method in edge-based segmentation of grayscale images,Applied Soft Computing,2011,"Image segmentation is a significant low-level method of the image processing area. As the matter of the fact that there is no selected certainty in interpreting the computer vision problems, there are many likely solutions. Some morphological methods used in image segmentation cause over-segmentation problems. Region merging, the usage of markers and the usage of multi-scale are the solutions for the over-segmentation problems found in the literature. However, these approaches give rise to under-segmentation problem. Simulated annealing (SA) is an optimization technique for soft computing. In our study, the problem of image segmentation is treated as a p-median (i.e., combinatorial optimization) problem. Therefore, the SA is used to solve p-median problem as a probabilistic metaheuristic. In the optimization method that is introduced in this paper, optimal threshold has been obtained for bi-level segmentation of grayscale images using our entropy-based simulated annealing (ESA) method. In addition, this threshold is used in determining optimal contour for edge-based image segmentation of grayscale images. Compared to the available methods (i.e., Otsu, only-entropy and Snake method) in the literature, our ESA method is more feasible in terms of performance measurements, threshold values and coverage area ratio of the region of interest (ROI).",Simulated annealing Optimization Optimal threshold Bi-level segmentation Edge-based segmentation Grayscale image segmentation,
73,Ultrasound confidence maps using random walks,Medical Image Analysis,2012,"Advances in ultrasound system development have led to a substantial improvement of image quality and to an increased use of ultrasound in clinical practice. Nevertheless, ultrasound attenuation and shadowing artifacts cannot be entirely avoided and continue to challenge medical image computing algorithms. We introduce a method for estimating a per-pixel confidence in the information depicted by ultrasound images, referred to as an ultrasound confidence map, which emphasizes uncertainty in attenuated and/or shadow regions. Our main novelty is the modeling of the confidence estimation problem within a random walks framework by taking into account ultrasound specific constraints. The solution to the random walks equilibrium problem is global and takes the entire image content into account. As a result, our method is applicable to a variety of ultrasound image acquisition setups. We demonstrate the applicability of our confidence maps for ultrasound shadow detection, 3D freehand ultrasound reconstruction, and multi-modal image registration.",Ultrasound Confidence Random walks Transmission,
74,Three-Dimensional Ultrasound: From Acquisition to Visualization and From Algorithms to Systems,"Biomedical Engineering, IEEE Reviews in",2009,"One of the key additions to clinical ultrasound (US) systems during the last decade was the incorporation of three-dimensional (3-D) imaging as a native mode. Compared to previous-generation 3-D US imaging systems, today's systems offer easier volume acquisition and deliver superior image quality with various visualization options. This has come as a result of many technological advances and innovations in transducer design, electronics, computer architecture, and algorithms. While freehand 3-D US techniques continue to be used, mechanically scanned and/or two-dimensional (2-D) matrix-array transducers are increasingly adopted, enabling higher volume rates and easier acquisition. More powerful computing engines with instruction-level and data-level parallelism and high-speed memory access support new and improved 3-D visualization capabilities. Many clinical US systems today have a 3-D option that offers interactive acquisition and display. In this paper, we cover the innovations of the last decade that have enabled the current 3-D US systems from acquisition to visualization, with emphasis on transducers, algorithms, and computation.",3-D ultrasound 4-D ultrasound algorithms systems transducers volume rendering,
75,Performance of ILU preconditioning techniques in simulating anisotropic diffusion in the human brain,Future Generation Computer Systems,2004,"We conduct simulations for the unsteady state anisotropic diffusion process in the human brain by discretizing the governing diffusion equation on a face-centered cubic grid and adopting a high performance differential-algebraic equation solver, IDA, to deal with the resulting large-scale system of DAEs. Incomplete LU preconditioning techniques are used with the GMRES method to accelerate the convergence rate of the iterative solution. We then investigate and compare the efficiency and effectiveness of a number of ILU preconditioners, and find out that the ILUT with a dual dropping strategy gives the best overall performance when it is provided with the optimum choices of the fill-in parameter and the threshold dropping tolerance.",Anisotropic diffusion DT-MRI FCC grid Preconditioning,
76,Low cost scaleable parallel image processing system,Microprocessors and Microsystems,2001,The design and application of a DSP-based Parallel Image Processing system is presented. A scaleable system based around a TMS320C40 DSP Master–Slave architecture is shown to be a suitable vehicle for industrial inspection problems. Custom vision bus and parallel communication channels are used to pass data between local and shared memory in the image processing system. The target application of SMT solder bond inspection required the fast processing of 2-D FFTs. Analysis of the system's suitability to this task is presented followed by actual results from a three-processor system. The target inspection rate of 100 SMT solder joints per second can be met.,Image processing Parallel architectures PCB inspection Industrial inspection 2D FFT TMS320C40 DSP,
77,Parallel simulation of anisotropic diffusion with human brain DT-MRI Data,Computers & Structures,2004,"We conduct simulations for the 3D unsteady state anisotropic diffusion process with DT-MRI data in the human brain by discretizing the governing diffusion equation on Cartesian grid and adopting a high performance differential–algebraic equation (DAE) solver, the parallel version of implicit differential–algebraic (IDA) solver, to tackle the resulting large scale system of DAEs. Parallel preconditioning techniques including sparse approximate inverse and banded-block-diagonal preconditioners are used with the GMRES method to accelerate the convergence rate of the iterative solution. We then investigate and compare the efficiency and effectiveness of the two parallel preconditioners. The experimental results of the diffusion simulations on a parallel supercomputer show that the sparse approximate inverse preconditioning strategy, which is robust and efficient with good scalability, gives a much better overall performance than the banded-block-diagonal preconditioner.",Anisotropic diffusion simulation Diffusion tensor magnetic resonance imaging (DT-MRI) Implicit differential–algebraic solver Parallel preconditioning techniques Sparse approximate inverse preconditioner Banded-block-diagonal preconditioner,
78,Tumor-Cut: Segmentation of Brain Tumors on Contrast Enhanced MR Images for Radiosurgery Applications,"Medical Imaging, IEEE Transactions on",2012,"In this paper, we present a fast and robust practical tool for segmentation of solid tumors with minimal user interaction to assist clinicians and researchers in radiosurgery planning and assessment of the response to the therapy. Particularly, a cellular automata (CA) based seeded tumor segmentation method on contrast enhanced T1 weighted magnetic resonance (MR) images, which standardizes the volume of interest (VOI) and seed selection, is proposed. First, we establish the connection of the CA-based segmentation to the graph-theoretic methods to show that the iterative CA framework solves the shortest path problem. In that regard, we modify the state transition function of the CA to calculate the exact shortest path solution. Furthermore, a sensitivity parameter is introduced to adapt to the heterogeneous tumor segmentation problem, and an implicit level set surface is evolved on a tumor probability map constructed from CA states to impose spatial smoothness. Sufficient information to initialize the algorithm is gathered from the user simply by a line drawn on the maximum diameter of the tumor, in line with the clinical practice. Furthermore, an algorithm based on CA is presented to differentiate necrotic and enhancing tumor tissue content, which gains importance for a detailed assessment of radiation therapy response. Validation studies on both clinical and synthetic brain tumor datasets demonstrate 80%-90% overlap performance of the proposed algorithm with an emphasis on less sensitivity to seed initialization, robustness with respect to different and heterogeneous tumor types, and its efficiency in terms of computation time.",biomedical MRI brain cancer cellular automata graph theory image enhancement image segmentation iterative methods medical image processing radiation therapy sensitivity surgery tumours T1 weighted magnetic resonance images brain tumors cellular automata contrast enhanced MR images graph theory image segmentation iterative CA framework level set surface minimal user interaction necrotic tissue radiation therapy radiosurgery robustness seed selection sensitivity sensitivity parameter shortest path problem spatial smoothness state transition function tumor cut tumor probability map volume of interest Automata Biomedical imaging Image edge detection Image segmentation Magnetic resonance imaging Planning Tumors Brain tumor segmentation cellular automata contrast enhanced magnetic resonance imaging (MRI) necrotic tissue segmentation radiosurgery radiotherapy seeded segmentation shortest paths Algorithms Brain Brain Neoplasms Databases Factual Humans Magnetic Resonance Imaging Models Biological Radiosurgery Radiotherapy Computer-Assisted Reproducibility of Results,
79,Efficient FPGA implementation of DWT and modified SPIHT for lossless image compression,Journal of Systems Architecture,2007,"In this paper, we present an implementation of the image compression technique set partitioning in hierarchical trees (SPIHT) in programmable hardware. The lifting based Discrete Wavelet Transform (DWT) architecture has been selected for exploiting the correlation among the image pixels. In addition, we provide a study on what storage elements are required for the wavelet coefficients. A modified SPIHT (Set Partitioning in Hierarchical Trees) algorithm is presented for encoding the wavelet coefficients. The modifications include a simplification of coefficient scanning process, use of a 1-D addressing method instead of the original 2-D arrangement for wavelet coefficients and a fixed memory allocation for the data lists instead of the dynamic allocation required in the original SPIHT. The proposed algorithm has been illustrated on both the 2-D Lena image and a 3-D MRI data set and is found to achieve appreciable compression with a high peak-signal-to-noise ratio (PSNR).",Medical image compression Wavelet transform SPIHT Lifting scheme PSNR,
80,Quantitative characterization of lung disease,Computerized Medical Imaging and Graphics,2005,"The increase in prevalence, incidence and variety of pulmonary diseases has precipitated the need for more non-invasive quantitative assessment of structure/function relationships in the lung. This need requires concise description not only of lung anatomy but also of associated underlying mechanics of pulmonary function, as well as deviation from normal in specific diseases. This can be facilitated through the use of adaptive deformable surface models of the lung at end inspiratory and expiratory volumes. Lung surface deformation may be used to represent tissue excursion, which can characterize both global and regional lung mechanics. In this paper, we report a method for robust determination and visualization of pulmonary structure and function using clinical CT scans. The method provides both intuitive 3D parametric visualization and objective quantitative assessment of lung structure and associated function, in both normal and pathological cases.",Lung disease CT imaging Pulmonary mechanics 3D visualization Deformable models,
81,Visualization of perfusion abnormalities with GPU-based volume rendering,Computers & Graphics,2012,"This article presents an innovative GPU-based solution for visualization of perfusion abnormalities detected in dynamic brain perfusion computer tomography (dpCT) maps in an augmented-reality environment. This new graphic algorithm is a vital part of a complex system called DMD (detection measure and description), which was recently proposed by the authors. The benefit of this algorithm over previous versions is its ability to operate in real time to satisfy the needs of augmented reality simulation. The performance speed (in frames per second) of six volume-rendering algorithms was determined for models with and without semi-transparent pixels.",Augmented reality Dynamic brain perfusion Pattern recognition Diagnostic support system,
82,A system for detecting and describing pathological changes using dynamic perfusion computer tomography brain maps,Computers in Biology and Medicine,2011,"This paper presents a novel method of detecting and describing pathological changes that can be visualized on dynamic computer tomography brain maps (perfusion CT). The system was tested on a set of dynamic perfusion computer tomography maps. Each set consisted of two perfusion maps (CBF, CBV and TTP for testing the irregularity detection algorithm) and one CT brain scan (for the registration algorithm) from 8 different patients with suspected strokes. In 36 of the 84 brain maps, abnormal perfusion was diagnosed. The results of the algorithm were compared with the findings of a team of two radiologists. All of the CBF and CBV maps that did not show a diagnosed asymmetry were classified correctly (i.e. no asymmetry was detected). In four of the TTP maps the algorithm found asymmetries, which were not classified as irregularities in the medical diagnosis; 84.5% of the maps were diagnosed correctly (85.7% for the CBF, 85.7% for the CBV and 82.1% for the TTP); 75% of the errors in the CBF maps and 100% of the errors in the CBV and the TTP maps were caused by the excessive detection of asymmetry regions. Errors in the CBFs and the CBVs were eliminated in cases in which the symmetry axis was selected manually. Subsequently, 96.4% of the CBF maps and 100% of the CBV maps were diagnosed correctly.",Brain change detection Asymmetry detection Brain atlas Dynamic brain perfusion CT Computer-aided diagnosis Image interpretation,
83,Enhancing throughput for streaming applications running on cluster systems,Journal of Parallel and Distributed Computing,2013," The exploitation of throughput in a parallel application that processes an input data stream is a difficult challenge. For typical coarse-grain applications, where the computation time of tasks is greater than their communication time, the maximum achievable throughput is determined by the maximum task computation time. Thus, the improvement in throughput above this maximum would eventually require the modification of the source code of the tasks. In this work, we address the improvement of throughput by proposing two task replication methodologies that have the target throughput to be achieved as an input parameter. They proceed by generating a new task graph structure that permits the target throughput to be achieved. The first replication mechanism, named DPRM (Data Parallel Replication Mechanism), exploits the inner task data parallelism. The second mechanism, named TCRM (Task Copy Replication Mechanism), creates new execution paths inside the application task graph structure that allows more than one instance of data to be processed concurrently. We evaluate the effectiveness of these mechanisms with three real applications executed in a cluster system: the MPEG2 video compressor, the IVUS (Intra-Vascular Ultra-Sound) medical image application and the BASIZ (Bright and SAtured Images Zone) video processing application. In all these cases, the obtained throughput was greater after applying the proposed replication mechanism than what the application could provide with the original implementation.",Streaming applications Pipeline execution Data parallelism Task parallelism Task replication mechanisms,
84,Group-wise construction of reduced models for understanding and characterization of pulmonary blood flows from medical images,Medical Image Analysis,2014," 3D computational fluid dynamics (CFD) in patient-specific geometries provides complementary insights to clinical imaging, to better understand how heart disease, and the side effects of treating heart disease, affect and are affected by hemodynamics. This information can be useful in treatment planning for designing artificial devices that are subject to stress and pressure from blood flow. Yet, these simulations remain relatively costly within a clinical context. The aim of this work is to reduce the complexity of patient-specific simulations by combining image analysis, computational fluid dynamics and model order reduction techniques. The proposed method makes use of a reference geometry estimated as an average of the population, within an efficient statistical framework based on the currents representation of shapes. Snapshots of blood flow simulations performed in the reference geometry are used to build a POD (Proper Orthogonal Decomposition) basis, which can then be mapped on new patients to perform reduced order blood flow simulations with patient specific boundary conditions. This approach is applied to a data-set of 17 tetralogy of Fallot patients to simulate blood flow through the pulmonary artery under normal (healthy or synthetic valves with almost no backflow) and pathological (leaky or absent valve with backflow) conditions to better understand the impact of regurgitated blood on pressure and velocity at the outflow tracts. The model reduction approach is further tested by performing patient simulations under exercise and varying degrees of pathophysiological conditions based on reduction of reference solutions (rest and medium backflow conditions respectively).",Computational fluid dynamics Pulmonary artery Tetralogy of Fallot Atlas construction Proper orthogonal decomposition,
85,Robust finite impulse response beamforming applied to medical ultrasound,"Ultrasonics, Ferroelectrics, and Frequency Control, IEEE Transactions on",2009,"We previously described a beamformer architecture that replaces the single apodization weights on each receive channel with channel-unique finite impulse response (FIR) filters. The filter weights are designed to optimize the contrast resolution performance of the imaging system. Although the FIR beamformer offers significant gains in contrast resolution, the beamformer suffers from low sensitivity, and its performance rapidly degrades in the presence of noise. In this paper, a new method is presented to improve the robustness of the FIR beamformer to electronic noise as well as variation or uncertainty in the array response. A method is also described that controls the sidelobe levels of the FIR beamformer's spatial response by applying an arbitrary weighting function in the filter design algorithm. The robust FIR beamformer is analyzed using a generalized cystic resolution metric that quantifies a beamformer's clinical imaging performance as a function of cyst size and channel input SNR. Fundamental performance limits are compared between 2 robust FIR beamformer's the dynamic focus FIR (DF-FIR) beamformer and the group focus FIR (GF-FIR) beamformer-the conventional delay-and-sum (DAS) beamformer, and the spatial-matched filter (SMF) beamformer. Results from this study show that the new DF- and GF-FIR beamformers are more robust to electronic noise compared with the optimal contrast resolution FIR beamformer. Furthermore, the added robustness comes with only a slight loss in cystic resolution. Results from the generalized cystic resolution metric show that a 9-tap robust FIR beamformer outperforms the SMF and DAS beamformer until receive channel input SNR drops below -5 dB, whereas the 9-tap optimal contrast resolution beamformer's performance deteriorates around 50 dB SNR. The effects of moderate phase aberrations, characterized by an a priori root-mean-square strength of 28 ns and an a priori full-width at half-maximum correlation length of 3.6 mm, are inves- igated on the robust FIR beamformers. Full sets of robust FIR beamformer filter weights are constructed using an in silico model scanner and the L14-5/38 mm probe. Using the derived weights, a series of simulated point target and anechoic cyst B-mode images are generated to investigate further the potential increases in contrast resolution when using the robust FIR beamformers. Under the investigated conditions, the 7-tap optimal contrast resolution beamformer and the 7-tap robust beamformer with added SNR constraint increase lesion detectability by 247 and 137% compared with the conventional DAS beamformer, respectively. Finally, experimental phantom and in vivo images are produced using this novel receive architecture. The simulated and experimental images clearly show a reduction in clutter and an increase in contrast resolution compared with the conventionally beamformed images. This novel receive beamformer can be applied to any conventional ultrasound system where the system response is reasonably well characterized.",FIR filters aberrations biomedical equipment biomedical ultrasonics clutter image resolution image scanners mean square error methods medical image processing spatial filters 7-tap robust beamformer 9-tap optimal contrast resolution beamformer's performance DAS beamformer FIR beamformer filter SMF beamformer anechoic cyst B-mode image beamformer clinical imaging performance channel input signal-to-noise ratio channel-unique finite impulse response clutter reduction delay-and-sum beamformer experimental phantom generalized cystic resolution half-maximum correlation in silico model scanner medical ultrasound in vivo images moderate phase aberration optimal contrast resolution FIR beamformer robust FIR beamformer robust finite impulse response beamforming root-mean-square method simulated point target spatial-matched filter Array signal processing Biomedical imaging Design optimization Finite impulse response filter Focusing Image resolution Noise robustness Performance gain Spatial resolution Ultrasonic imaging Algorithms Image Enhancement Image Interpretation Computer-Assisted Imaging Three-Dimensional Numerical Analysis Computer-Assisted Reproducibility of Results Sensitivity and Specificity Signal Processing Computer-Assisted,
86,Classification of cancer cell death with spectral dimensionality reduction and generalized eigenvalues,Artificial Intelligence in Medicine,2011,"Objective Accurate cell death discrimination is a time consuming and expensive process that can only be performed in biological laboratories. Nevertheless, it is very useful and arises in many biological and medical applications. Methods and material Raman spectra are collected for 84 samples of A549 cell line (human lung cancer epithelia cells) that has been exposed to toxins to simulate the necrotic and apoptotic death. The proposed data mining approach for the multiclass cell death discrimination problem uses a multiclass regularized generalized eigenvalue algorithm for classification (multiReGEC), together with a dimensionality reduction algorithm based on spectral clustering. Results The proposed algorithmic scheme can classify A549 lung cancer cells from three different classes (apoptotic death, necrotic death and control cells) with 97.78% ± 0.047 accuracy versus 92.22 ± 0.095 without the proposed feature selection preprocessing. The spectrum areas depicted by the algorithm corresponds to the 〉C O bond from the lipids and the lipid bilayer. This chemical structure undergoes different change of state based on cell death type. Further evidence of the validity of the technique is obtained through the successful classification of 7 cell spectra that undergo hyperthermic treatment. Conclusions In this study we propose a fast and automated way of processing Raman spectra for cell death discrimination, using a feature selection algorithm that not only enhances the classification accuracy, but also gives more insight in the undergoing cell death process.",Spectral clustering Dimensionality reduction Generalized eigenvalue classification Raman spectroscopy Cancer treatment,
87,Efficient parallel reduction to bidiagonal form,Parallel Computing,1999,"Most methods for calculating the SVD (singular value decomposition) require to first bidiagonalize the matrix. The blocked reduction of a general, dense matrix to bidiagonal form, as implemented in ScaLAPACK, does about one half of the operations with BLAS3. By subdividing the reduction into two stages dense &rarr banded and banded &rarr bidiagonal with cubic and quadratic arithmetic costs, respectively, we are able to carry out a much higher portion of the calculations in matrix-matrix multiplications. Thus, higher performance can be expected. This paper presents and compares three parallel techniques for reducing a full matrix to banded form. (The second reduction stage is described in another paper [B. Lang, Parallel Comput. 22 (1996) 1-18]). Numerical experiments on the Intel Paragon and IBM SP/1 distributed memory parallel computers demonstrate that the two-stage reduction approach can be significantly superior if only the singular values are required.",Parallel algorithms Computational complexity Linear equations Matrix algebra Parallel processing systems,
88,Web based prediction for diabetes treatment,Future Generation Computer Systems,2011,"Diabetics need continuous support during their therapy because the clinical treatment could be improved in the medical cures;the doctors need to share the new information and to support their patients in an interactive way. In this paper, we describe a web based software tool able to perform predictions on the future glycaemia level of the patients for a specific time horizon; to perform what-if analysis, showing how the glycaemia level could vary if some parameters are changed (either in the diet of the patient or in the clinical treatment); to share information and suggestions, creating a virtual community between specialists and patients; to inform users about their health condition, by a classification algorithm and to allow the patient to maintain a daily diary and to know about the evolution of the health condition. The system merges software engineering and Operation Research methods and could become a valid interactive support for patients and specialists.",Medical information system Optimization Decision support system Web based services,
89,Multi-elitist immune clonal quantum clustering algorithm,Neurocomputing,2013,"The quantum clustering (QC) algorithm suffers from the issues of getting stuck in local extremes and computational bottleneck when handling large-size image segmentation. By embedding a potential evolution formula into affinity function calculation of multi-elitist immune clonal optimization, and updating the cluster center based on the distance matrix, the multi-elitist immune clonal quantum clustering algorithm (ME-ICQC) is proposed in this paper. In the proposed framework, elitist population is composed of the individuals with high affinity, which is considered to play dominant roles in the evolutionary process. It can help to find the global optimal solution or near-optimal solution for most tested tasks. The diversity of population can be well maintained by general subgroup evolution of ME-ICQC. These different functions are implemented by the dissimilar mutation strategies or crossover operators. The bi-group exchanges the information of excellence antibodies using the hypercube co-evolution operation. Compared with existing algorithms, the ME-ICQC achieves an improved clustering accuracy with more stable convergence, but it is not significantly better than other optimization techniques combined with QC. Also, the experimental results also show that our algorithm performs well on multi-class, parameters-sensitive and large-size datasets.",Quantum clustering Clonal selection Multi-elitist co-evolution Adaptive mutation Image segmentation,
90,A tool for analytical simulation of B-splines surface deformation,Computer-Aided Design,2013,"A non-planar surface deformation model based on B-splines as finite elements is presented here. The model includes the variational formulation, the system of ordinary differential equations derived from it and its analytical solution. The model has been checked for a variety of surfaces such as tiles, half spheres, planes, etc. Furthermore, we are able to solve the system analytically by only moving a reduced number of control points to deform the surface. This makes the method faster, since numerical methods are no longer necessary.",Computer graphics Surface deformation Finite elements B-splines,
91,Large-scale functional MRI study on a production grid,Future Generation Computer Systems,2010,"Functional magnetic resonance imaging (fMRI) analysis is usually carried out with standard software packages (e.g., FSL and SPM) implementing the General Linear Model (GLM) computation. Yet, the validity of an analysis may still largely depend on the parameterization of those tools, which has, however, received little attention from researchers. In this paper we study the influence of three of those parameters, namely (i) the size of the spatial smoothing kernel, (ii) the hemodynamic response function delay and (iii) the degrees of freedom of the fMRI-to-anatomical scan registration. In addition, two different values of acquisition parameters (echo times) are compared. The study is performed on a data set of 11 subjects, sweeping a significant range of parameters. It involves almost one CPU year and produces 1.4 Terabytes of data. Thanks to a grid deployment of the FSL FEAT application, this compute and data intensive problem can be handled and the execution time is reduced to less than a week. Results suggest that optimal parameter values for detecting activation in the amygdalae deviate from the default typically adopted in such studies. Moreover, robust results indicate no significant difference between brain activation maps obtained with the two echo times.",Large-scale experiment Production grid fMRI,
92,A Service-Oriented Architecture enabling dynamic service grouping for optimizing distributed workflow execution,Future Generation Computer Systems,2008,"In this paper, we describe a Service-Oriented Architecture allowing the optimization of the execution of service workflows. We discuss the advantages of the service-oriented approach with regard to the enactment of scientific applications on a grid infrastructure. Based on the development of a generic Web-Services wrapper, we show how the flexibility of our architecture enables dynamic service grouping for optimizing the application execution time. We demonstrate performance results on a real medical imaging application. On a production grid infrastructure, the optimization proposed introduces a significant speed-up (from 1.2 to 2.9) when compared to a traditional execution.",Grid workflows Service-Oriented Architecture Legacy code wrapper Service grouping,
93,A model of pilot-job resource provisioning on production grids,Parallel Computing,2011,"Pilot-job systems emerged as a computation paradigm to cope with the heterogeneity of large-scale production grids, greatly reducing fault ratios and middleware overheads. They are now widely adopted to sustain the computation of scientific applications on such platforms. However, a model of pilot-job systems is still lacking, making it difficult to build realistic experimental setups for their study (e.g. simulators or controlled platforms). The variability of production conditions, background loads and resource characteristics further complicate this issue. This paper presents a model of pilot-job resource provisioning. Based on a probabilistic modeling of pilot submission and registration, the number of pilots registered to the application host and the makespan of a divisible-load application are derived. The model takes into account job failures and it does not make any assumption on the characteristics of the computing resources, on the scheduling algorithm or on the background load. Only a minimally invasive monitoring of the grid is required. The model is evaluated in production conditions, using logs acquired on a pilot-job server deployed in the biomed virtual organization of the European Grid Infrastructure. Experimental results show that the model is able to accurately describe the number of registered pilots along time periods ranging from a few hours to a few days and in different pilot submission conditions.",Production grids Pilot-job systems Probabilistic modeling,
94,Least-squares smoothing of 3D digital curves,Real-Time Imaging,2005,"In this paper an efficient procedure for 3D digital curve smoothing is presented. It is described by linear operators which allow to perform the constrained, position invariant, least-squares smoothing of 3D digital curves minimizing the undersampling, digitizing and quantizing error and to calculate various curve characteristics and invariants related to the original digitized curve. They are represented by sparse symmetric circulant Toeplitz matrices with integer coefficients which can be efficiently realized in serial as well as in parallel manner.",,
95,Interactive volume reconstruction and measurement on the grid,Methods of Information in Medicine,2005,"Objectives: To prove the advantages of integrating grid computing within medical image analysis software, and to discuss the technological, sociological and health care-related issues. Methods. Presentation of an instant volume reconstruction and measurement tool (PTM3D) used in clinical practice, including percutaneous nephrolithotomy examples; description of a parallel implementation of volume reconstruction, evaluation of this implementation on lung and body reconstruction, presentation of the technical limitations for clinical use and description and discussion of a prototype grid implementation. Results: Volume reconstruction can broaden its medical scope and use by accessing high-performance computing systems; interactive exploration of medical images can co-exist with the usual batch workload of grid systems; the EGEE grid middleware offers some of the required core services; a fully adequate computing environment needs further evolution to integrate real-time constraints. Conclusions: Clinical experiments of a grid-enabled PTM3D become possible. Widespread adoption of grid technology in the medical images analysis field will benefit from this early user"" project. Convergences appear between two broadly different fields", towards the need of a smooth integration of the new resources offered by grid systems into the everyday tools of their respective end-users. It can be expected that the convergence will mature towards truly interactive grids,
96,Compactly Supported Radial Basis Functions Based Collocation Method for Level-Set Evolution in Image Segmentation,"Image Processing, IEEE Transactions on",2007,"The partial differential equation driving level-set evolution in segmentation is usually solved using finite differences schemes. In this paper, we propose an alternative scheme based on radial basis functions (RBFs) collocation. This approach provides a continuous representation of both the implicit function and its zero level set. We show that compactly supported RBFs (CSRBFs) are particularly well suited to collocation in the framework of segmentation. In addition, CSRBFs allow us to reduce the computation cost using a kd-tree-based strategy for neighborhood representation. Moreover, we show that the usual reinitialization step of the level set may be avoided by simply constraining the l1-norm of the CSRBF parameters. As a consequence, the final solution is topologically more flexible, and may develop new contours (i.e., new zero-level components), which are difficult to obtain using reinitialization. The behavior of this approach is evaluated from numerical simulations and from medical data of various kinds, such as 3-D CT bone images and echocardiographic ultrasound images.",finite difference methods image representation image segmentation partial differential equations radial basis function networks trees (mathematics) collocation method compactly supported RBF compactly supported radial basis functions finite differences schemes image segmentation kd-tree-based strategy level-set evolution neighborhood representation partial differential equation Biomedical imaging Bones Computational efficiency Computed tomography Finite difference methods Image segmentation Level set Numerical simulation Partial differential equations Ultrasonic imaging Active contours collocation deformable models level sets partial differential equations (PDEs) radial basis functions (RBFs) segmentation Algorithms Artificial Intelligence Image Enhancement Image Interpretation Computer-Assisted Imaging Three-Dimensional Pattern Recognition Automated Reproducibility of Results Sensitivity and Specificity,
97,OLS with multiple high dimensional category variables,Computational Statistics & Data Analysis,2013," A new algorithm is proposed for OLS estimation of linear models with multiple high-dimensional category variables. It is a generalization of the within transformation to arbitrary number of category variables. The approach, unlike other fast methods for solving such problems, provides a covariance matrix for the remaining coefficients. The article also sets out a method for solving the resulting sparse system, and the new scheme is shown, by some examples, to be comparable in computational efficiency to other fast methods. The method is also useful for transforming away groups of pure control dummies. A parallelized implementation of the proposed method has been made available as an R-package lfe on CRAN.",Alternating projections Fixed effect estimator Kaczmarz method Two-way fixed effects Multiple fixed effects High dimensional category variables Panel data,
98,"A case study of innovative population-based algorithms in 3D modeling: Artificial bee colony, biogeography-based optimization, harmony search",Expert Systems with Applications,1750-1762,,,
99,SLA-driven dynamic cloud resource management,Future Generation Computer Systems,2014," As the size and complexity of Cloud systems increase, the manual management of these solutions becomes a challenging issue as more personnel, resources and expertise are needed. Service Level Agreement (SLA)-aware autonomic cloud solutions enable managing large scale infrastructure management meanwhile supporting multiple dynamic requirement from users. This paper contributes to these topics by the introduction of Cloudcompaas, a SLA-aware PaaS Cloud platform that manages the complete resource lifecycle. This platform features an extension of the SLA specification WS-Agreement, tailored to the specific needs of Cloud Computing. In particular, Cloudcompaas enables Cloud providers with a generic SLA model to deal with higher-level metrics, closer to end-user perception, and with flexible composition of the requirements of multiple actors in the computational scene. Moreover, Cloudcompaas provides a framework for general Cloud computing applications that could be dynamically adapted to correct the QoS violations by using the elasticity features of Cloud infrastructures. The effectiveness of this solution is demonstrated in this paper through a simulation that considers several realistic workload profiles, where Cloudcompaas achieves minimum cost and maximum efficiency, under highly heterogeneous utilization patterns.",Service Level Agreement Cloud computing Quality of service Monitoring,
100,A cascaded genetic algorithm for efficient optimization and pattern matching,Image and Vision Computing,2002,"A modified genetic algorithm (GA) based search strategy is presented here that is computationally more efficient than the conventional GA. Here the idea is to start a GA with the chromosomes of small length. Such chromosomes represent possible solutions with coarse resolution. A finite space around the position of solution in the first stage is subject to the GA at the second stage. Since this space is smaller than the original search space, chromosomes of same length now represent finer resolution. In this way, the search progresses from coarse to fine solution in a cascaded manner. Since chromosomes of small length are used at each stage, the overall approach becomes computationally more efficient than a single stage algorithm with the same degree of final resolution. The effectiveness of the proposed GA has been demonstrated for the optimization of some synthetic functions and on pattern recognition problem namely dot pattern matching and object matching with edge map.",Genetic algorithm Search technique Chromosome Mutation Optimization Dot pattern matching Pattern recognition,
101,Efficient cellular automaton segmentation supervised by pyramid on medical volumetric data and real time implementation with graphics processing unit,Expert Systems with Applications,2011,"In surgery simulation, the extracted tissue data can be operated repeatedly in a Virtual-reality (VR) system which provides a good alternative to classical training method. Fully automated segmentation techniques cannot guarantee the efficiency and precision in general case. This paper describes a user interactive segmentation method: given a labeled 2D image plane in Multi-Planar Reformation (MPR), the rest tissues are segmented automatically by a cellular automaton in multi-scale domain. Labels image generated in higher level Gaussian pyramid can be extended to lower level ones according to selected resolution. An edge indicator function is also set to avoid over-segmentation in Laplacian pyramid. The evolution can be observed and guided with volume rendering results. The proposed method shows the merits of higher precision, real time response in GPU framework and few interactions are required.",Interactive segmentation Cellular automaton (CA) Pyramid Multi-Planar Reformation Tissues extraction,
102,Parallel computing for fringe pattern processing: A multicore CPU approach in MATLAB® environment,Optics and Lasers in Engineering,2009,"In the process of measurements such as optical interferometry and fringe projection, an important stage is fringe pattern analysis. Many advanced fringe analysis algorithms have been proposed including regularized phase tracking (RPT), partial differential equation based methods, wavelet transform, Wigner–Ville distribution, and windowed Fourier transform. However, most of those algorithms are computationally expensive. MATLAB® is a general algorithm development environment with powerful image processing and other supporting toolboxes. It is also commonly used in photomechanical data analysis. With rapid development of multicore CPU technique, using multicore computer and MATLAB® is an intuitive and simple way to speed up the algorithms for fringe pattern analysis. The paper introduces two acceleration approaches for fringe pattern processing. The first approach is task parallelism using multicore computer and MATLAB® parallel computing toolbox. Since some algorithms are embarrassing problems, our first approach makes use of this characteristic to parallelize these algorithms. For this approach, parallelized windowed Fourier filtering (WFF) algorithm serves as an example to show how parallel computing toolbox accelerates the algorithm. Second, data parallelism using multicore computer and MATLAB® parallel computing toolbox is proposed. A high level parallel wrapping structure is designed, which can be used for speeding up any local processing algorithms. WFF, windowed Fourier ridges (WFR), and median filter are used as examples to illustrate the speedup. At last, the results show that the parallel versions of former sequential algorithm with simple modifications achieve the speedup up to 6.6 times.",Parallel computing Fringe pattern analysis MATLAB® parallel computing toolbox Windowed Fourier transform,
103,Digital filter implementation over FPGA platform with LINUX OS,Procedia Engineering,2012,"The embedded processors on FPGA's are a good tool to specific propose works. In this work we present how the FPGA is used to apply a Sobel filter to a set of images, also the step needed to set-up the entire system is described. An embedded processor, with a Linux distribution implemented is used to run a special compilation of C filter program, the filter is compared with the results obtained with a PC running the same filter, in the embedded system all the process runs in the FPGA and the exit file can be accessed by ftp or http server embedded into the Linux system.",C Digital filter FPGA Image process,
104,Task-oriented asymmetric multiprocessing for interactive image-guided surgery,Parallel Computing,1998,"Interactive, Image-Guided Surgery (IIGS) is a technique for using three-dimensional medical images (tomograms) as maps for surgical guidance. The process requires real-time tracking of surgical position and mapping of that position onto the image volume or volumes. Because of the mission-critical nature of the process and the large volumes of data which must be manipulated, system performance is essential. We have evolved a technique for providing very rapid tracking and display on modest computer systems by distributing parts of the overall task to separate processors in the system. By using processors of architecture best suited for each task we enhance the total process efficiency as defined by system performance per cost.",Computer-assisted surgery Image-guided surgery,
105,Three-dimensional Fuzzy Kernel Regression framework for registration of medical volume data,Pattern Recognition,2013," In this work a general framework for non-rigid 3D medical image registration is presented. It relies on two pattern recognition techniques: kernel regression and fuzzy c-means clustering. The paper provides theoretic explanation, details the framework, and illustrates its application to implement three registration algorithms for CT/MR volumes as well as single 2D slices. The first two algorithms are landmark-based approaches, while the third one is an area-based technique. The last approach is based on iterative hierarchical volume subdivision, and maximization of mutual information. Moreover, a high performance Nvidia CUDA based implementation of the algorithm is presented. The framework and its applications were evaluated with a number of tests, which show that the proposed approaches achieve valuable results when compared with state-of-the-art techniques. Additional assessment was taken by expert radiologists, providing performance feedback from the final user perspective.",Non-rigid registration Fuzzy regression Mutual information Interpolation GPU computing,
106,Towards high performance cell segmentation in multispectral fine needle aspiration cytology of thyroid lesions,Computer Methods and Programs in Biomedicine,2010,"Thyroid nodule is a common cancer of the thyroid gland that affects up to 20% of the world population and approximately 50% of 60-year-old persons. Early detection and screening of the disease, especially analysis by fine needle aspiration cytology (FNAC), has led to improved diagnosis and management of the disease. Simultaneously, advances in imaging technology has enabled the rapid digitization of large volumes of FNAC specimen leading to increased interest in computer assisted diagnosis (CAD). This has led to development of a variety of algorithms for automated analysis of FNAC images, but due to the large scale memory and computing resource requirements, has had limited success in clinical use. In this paper, we present our experiences with two parallel versions of a code used for texture-based segmentation of thyroid FNAC images, a critical first step in realizing a fully automated CAD solution. An MPI version of the code is developed to exploit distributed memory compute resources such as PC clusters. An OpenMP version is developed for the currently emerging multi-core CPU architectures, which allow for parallel execution on every desktop system. Experiments are performed with image sizes ranging from 1024 × 1024 pixels up to 12288 × 12288 pixels with 21 spectral channels. Both versions are evaluated for performance and scalability.",Fine needle aspiration cytology Image analysis Parallel computing PC clusters Multi-core processors,
107,A cloud-integrated web platform for marine monitoring using GIS and remote sensing. Application to oil spill detection through SAR images,Future Generation Computer Systems,2014," Geographic Information Systems (GIS) have gained popularity in recent years because they provide spatial data management and access through the Web. This article gives a detailed description of a tool that offers an integrated framework for the detection and localization of marine spills using remote sensing, GIS, and cloud computing. Advanced segmentation algorithms are presented in order to isolate dark areas in SAR images, including fuzzy clustering and wavelets. In addition, cloud computing is used for scaling up the algorithms and providing communication between users.",Ocean monitoring GIS Cloud computing Remote sensing,
108,Fourier cross-sectional profile for vessel detection on retinal images,Computerized Medical Imaging and Graphics,2010,"Retinal blood vessels are important objects in ophthalmologic images. In spite of many attempts for vessel detection, it appears that existing methodologies are based on edge detection or modeling of vessel cross-sectional profiles in intensity. The application of these methodologies is hampered by the presence of a wide range of retinal vessels. In this paper we define a universal representation for upward and downward vessel cross-sectional profiles with varying boundary sharpness. This expression is used to define a new scheme of vessel detection based on symmetry and asymmetry in the Fourier domain. Phase congruency is utilized for measuring symmetry and asymmetry so that our scheme is invariant to vessel brightness variations. We have performed experiments on fluorescein images and color fundus images to show the efficiency of the proposed algorithm technique. We also have performed a width measurement study, using an optimal medial axis skeletonization scheme as a post-processing step, to compare the technique with the generalized Gaussian profile modeling. The new algorithm technique is promising for automated vessel detection where optimizing profile models is difficult and preserving vessel width information is necessary.",Asymmetry and symmetry Cross-sectional profile Feature detection Retinal images Vessel detection,
109,Automatic localization of solid organs on 3D CT images by a collaborative majority voting decision based on ensemble learning,Computerized Medical Imaging and Graphics,2012,"Purpose Organ segmentation is an essential step in the development of computer-aided diagnosis/surgery systems based on computed tomography (CT) images. A universal segmentation approach/scheme that can adapt to different organ segmentations can substantially increase the efficiency and robustness of such computer-aided systems. However, this is a very challenging problem. An initial determination of the approximate position and range of a target organ in CT images is prerequisite for precise organ segmentations. In this study, we have proposed a universal approach that enables automatic localization of the approximate position and range of different solid organs in the torso region on three-dimensional (3D) CT scans. Methods The location of a target organ in a 3D CT scan is presented as a 3D rectangle that bounds the organ region tightly and accurately. Our goal was to automatically and effectively detect such a target organ-specific 3D rectangle. In our proposed approach, multiple 2D detectors are trained using ensemble learning and their outputs are combined using a collaborative majority voting in 3D to accomplish the robust organ localizations. Results We applied this approach to localize the heart, liver, spleen, left-kidney, and right-kidney regions independently using a CT image database that includes 660 torso CT scans. In the experiment, we manually labeled the abovementioned target organs from 101 3D CT scans as training samples and used our proposed approach to localize the 5 kinds of target organs separately on the remaining 559 torso CT scans. The localization results of each organ were evaluated quantitatively by comparing with the corresponding ground truths obtained from the target organs that were manually labeled by human operators. Experimental results showed that success rates of such organ localizations were distributed from 99% to 75% of the 559 test CT scans. We compared the performance of our approach with an atlas-based approach. The errors of the detected organ-center-positions in the successful CT scans by our approach had a mean value of 5.14 voxels, and those errors were much smaller than the results (mean value about 25 voxels) from the atlas-based approach. The potential usefulness of the proposed organ localization was also shown in a preliminary investigation of left kidney segmentation in non-contrast CT images. Conclusions We proposed an approach to accomplish automatic localizations of major solid organs on torso CT scans. The accuracy of localizations, flexibility of localizations of different organs, robustness to contrast and non-contrast CT images, and normal and abnormal patient cases, and computing efficiency were validated on the basis of a large number of torso CT scans.",3D CT torso images Inner organ localization Ensemble learning Collaborative majority voting,
110,High quality real-time Image-to-Mesh conversion for finite element simulations,Journal of Parallel and Distributed Computing,2014," In this paper, we present a parallel Image-to-Mesh Conversion (I2M) algorithm with quality and fidelity guarantees achieved by dynamic point insertions and removals. Starting directly from an image, its implementation is capable of recovering the isosurface and meshing the volume with tetrahedra of good shape. Our tightly-coupled shared-memory parallel speculative execution paradigm employs carefully designed contention managers, load balancing, synchronization and optimizations schemes. These techniques are shown to boost not only the parallel but also the single-threaded efficiency of our code. Specifically, our single-threaded performance is faster than both CGAL and TetGen, the state of the art sequential open source meshing tools we are aware of. The effectiveness of our method is demonstrated on Blacklight, the Pittsburgh Supercomputing Center’s cache-coherent NUMA machine. We observe a more than 82% strong scaling efficiency for up to 64 cores, and a more than 82% weak scaling efficiency for up to 144 cores, reaching a rate of more than 14.3 million elements per second. This is the fastest 3D Delaunay mesh generation and refinement algorithm, to the best of our knowledge.",Parallel Delaunay mesh refinement Scalability Quality Fidelity Shared-memory,
111,Segmentation of brain magnetic resonance angiography images based on MAP–MRF with multi-pattern neighborhood system and approximation of regularization coefficient,Medical Image Analysis,2013," Existing maximum a posteriori probability and Markov random field (MRF) models have limitations associated with: (1) the ordinary neighborhood system being unable to differentiate subtle changes due to several-to-one correspondence within the neighborhood; and (2) difficulty finding an appropriate parameter to balance between the spatial context and the data likelihood. Aiming at overcoming the limitations and applications to segmentation of cerebral vessels from magnetic resonance angiography images, we have proposed (1) a multi-pattern neighborhood system and corresponding energy equation to enable the MRF model for segmenting fine cerebral vessels with complicated context; and (2) an iterative approximation algorithm based on the maximum pseudo-likelihood and the space coding mode for the automatic parameter estimation of high level model of MRF. In the implementation, two computational strategies have been employed to speed up: the candidate space of cerebral vessels based on a threshold of the response to multi-scale filtering, and parallel computation of major equations. Three phantoms simulating segmentation challenges of vessels have been devised to quantitatively validate the algorithm. In addition, 10 three-dimensional clinical data sets have been used to validate the algorithm qualitatively. It has been shown that the proposed method could yield smaller error, improve the spatial resolution of MRF model, and better balance the smoothing and data likelihood than the traditional trial-and-error estimation. Comparative studies have shown that the proposed method is better than the 3 segmentation algorithms (Hassouna et al., 2006; Hao et al., 2008; Gao et al., 2011) in terms of segmentation accuracy, robustness to noise and varying curvatures as well as radii.",Cerebrovascular segmentation Markov random field Markov neighborhood system Regularization parameter Magnetic resonance angiography,
112,Adaptive particle swarm optimization for CNN associative memories design,Neurocomputing,2009,"In this paper particle swarm optimization is used to implement a synthesis procedure for cellular neural networks autoassociative memories. The use of this optimization technique allows a global search for computing the model parameters that identify designed memories, providing a synthesis procedure that takes into account the robustness of the solution. In particular, the design parameters can be modified during the convergence in order to guarantee minimum recall performances of the network in terms of robustness to noise overlapped to input patterns. Numerical results confirm the good performances of the designed networks when patterns are affected by different kinds of noise.",Particle swarm optimization Associative memories Cellular neural networks Robustness to noise,
113,Automated lung segmentation and smoothing techniques for inclusion of juxtapleural nodules and pulmonary vessels on chest CT images,Biomedical Signal Processing and Control,2014," Segmentation of the lung is often performed as an important preprocessing step for quantitative analysis of chest computed tomography (CT) imaging. However, the presence of juxtapleural nodules and pulmonary vessels, image noise or artifacts, and individual anatomical variety make lung segmentation very complex. To address these problems, a fast and fully automatic scheme based on iterative weighted averaging and adaptive curvature threshold is proposed in this study to facilitate accurate lung segmentation for inclusion of juxtapleural nodules and pulmonary vessels and ensure the smoothness of the lung boundary. Our segmentation scheme consists of four main stages: image preprocessing, thorax extraction, lung identification and lung contour correction. The aim of preprocessing stage is to encourage intra-region smoothing and preserve the inter-region edge of CT images. In the thorax extraction stage, the artifacts external to the patient's body are discarded. Then, a fuzzy-c-means clustering method is used in the thorax region and all lung parenchyma is identified according to fuzzy membership value and connectivity. Finally, the segmented lung contour is smoothed and corrected with iterative weighted averaging and adaptive curvature threshold on each axis slice, respectively. Our method was validated on 20 datasets of chest CT scans containing 65 juxtapleural nodules. Experimental results show that our method can re-include all juxtapleural nodules and achieve an average volume overlap ratio of 95.81 ± 0.89% and an average mean absolute border distance of 0.63 ± 0.09 mm compared with the manually segmented results. The average processing time for segmenting one slice was 2.56 s, which is over 20 times faster than manual segmentation.",Lung segmentation Juxtapleural nodule Pulmonary vessel Fuzzy c-means clustering Curvature threshold,
114,World Wide Web interface for advanced SPECT reconstruction algorithms implemented on a remote massively parallel computer,International Journal of Medical Informatics,1997,"Data from Single Photon Emission Computed Tomography (SPECT) studies are blurred by inevitable physical phenomena occurring during data acquisition. These errors may be compensated by means of reconstruction algorithms which take into account accurate physical models of the data acquisition procedure. Unfortunately, this approach involves high memory requirements as well as a high computational burden which cannot be afforded by the computer systems of SPECT acquisition devices. In this work the possibility of accessing High Performance Computing and Networking (HPCN) resources through a World Wide Web interface for the advanced reconstruction of SPECT data in a clinical environment was investigated. An iterative algorithm with an accurate model of the variable system response was ported on the Multiple Instruction Multiple Data (MIMD) parallel architecture of a Cray T3D massively parallel computer. The system was accessible even from low cost PC-based workstations through standard TCP/IP networking. A speedup Factor of 148 was predicted by the benchmarks run on the Gray T3D. A complete brain study of 30 (64 x 64) slices was reconstructed from a set of 90 (64 x 64) projections with ten iterations of the conjugate gradients algorithm in 9 s which corresponds to an actual speed-up factor of 135. The technique was extended to a more accurate 3D modeling of the system response for a true 3D reconstruction of SPECT data, the reconstruction time of the same data set with this more accurate model was 5 min. This work demonstrates the possibility of exploiting remote HPCN resources from hospital sites by means of low cost workstations using standard communication protocols and an user-friendly WWW interface without particular problems for routine use. (C) 1997 Elsevier Science B.V.",,
115,Anisotropic surface meshing with conformal embedding,Graphical Models,2014," This paper introduces a parameterization-based approach for anisotropic surface meshing. Given an input surface equipped with an arbitrary Riemannian metric, this method generates a metric-adapted mesh with user-specified number of vertices. In the proposed method, the edge length of the input surface is directly adjusted according to the given Riemannian metric at first. Then the adjusted surface is conformally embedded into a parametric 2D domain and a weighted Centroidal Voronoi Tessellation and its dual Delaunay triangulation are computed on the parametric domain. Finally the generated Delaunay triangulation can be mapped from the parametric domain to the original space, and the triangulation exhibits the desired anisotropic property. We compute the high-quality remeshing results for surfaces with different types of topologies and compare our method with several state-of-the-art approaches in anisotropic surface meshing by using the standard measurement criteria.",Anisotropic surface meshing Anisotropic Centroidal Voronoi Tessellation Conformal embedding,
116,Tumor segmentation from computed tomography image data using a probabilistic pixel selection approach,Computers in Biology and Medicine,2011,"Automatic segmentation of tumors is a complicated and difficult process as most tumors are rarely clearly delineated from healthy tissues. A new method for probabilistic segmentation to efficiently segment tumors within CT data and to improve the use of digital medical data in diagnosis has been developed. Image data are first enhanced by manually setting the appropriate window center and width, and if needed a sharpening or noise removal filter is applied. To initialize the segmentation process, a user places a seed point within the object of interest and defines a search region for segmentation. Based on the pixels' spatial and intensity properties, a probabilistic selection criterion is used to  pixels with a high probability of belonging to the object. To facilitate the segmentation of multiple slices, an automatic seed selection algorithm was developed to keep the seeds in the object as its shape and/or location changes between consecutive slices. The seed selection algorithm performs a greedy search by searching for pixels with matching intensity close to the location of the original seed point. A total of ten CT datasets were used as test cases, each with varying difficulty in terms of automatic segmentation. Five test cases had mean false positive error rates less than 10%, and four test cases had mean false negative error rates less than 10% when compared to manual segmentation of those tumors by radiologists.",Automatic seed relocation Computed tomography Image segmentation Probabilistic selection Tumor segmentation,
117,A general model for multiphase texture segmentation and its applications to retinal image analysis,Biomedical Signal Processing and Control,2013,"In this paper we propose a general variational segmentation model for multiphase texture segmentation based on fuzzy region competition principle. An important strength of the proposed framework is that different region terms (e.g. mutual information Kim et al. (2005) [1], local histogram Ni et al. (2009) [2] models for texture-based segmentation, and piecewise constant intensity model Chan and Vese (2001) [3] for intensity-based segmentation) can be included as appropriate to the problem. Constraints of different phases are considered by introducing Lagrangian multipliers into the energy functional, and a fast numerical solution is achieved by employing the fast dual projection algorithm Chambolle (2004) [4]. The proposed model has been applied to synthetic and natural images in order to make comparisons with other competing models in literature. Our results demonstrate superiority in dealing with multiphase texture segmentation problems. To demonstrate its usefulness in biomedical applications we have applied the new model to two retinal image segmentation problems: segmentation of capillary non-perfusion regions in fluorescein angiogram and segmentation of cellular layers of the retina in optical coherence tomography, and evaluated against the gold standard set by experts. The generalized overlap analysis shows good agreement for both applications. As a generic segmentation technique our new model has the potential to be extended for wider applications.",Multiphase segmentation Texture Active contour Total variation Fluorescein angiography Optical coherence tomography,
118,Three-dimensional segmentation of tumors from CT image data using an adaptive fuzzy system,Computers in Biology and Medicine,2009,"A new segmentation method using a fuzzy rule based system to segment tumors in a three-dimensional CT data was developed. To initialize the segmentation process, the user selects a region of interest (ROI) within the tumor in the first image of the CT study set. Using the ROI's spatial and intensity properties, fuzzy inputs are generated for use in the fuzzy rules inference system. With a set of predefined fuzzy rules, the system generates a defuzzified output for every pixel in terms of similarity to the object. Pixels with the highest similarity values are selected as tumor. This process is automatically repeated for every subsequent slice in the CT set without further user input, as the segmented region from the previous slice is used as the ROI for the current slice. This creates a propagation of information from the previous slices, used to segment the current slice. The membership functions used during the fuzzification and defuzzification processes are adaptive to the changes in the size and pixel intensities of the current ROI. The method is highly customizable to suit different needs of a user, requiring information from only a single two-dimensional image. Test cases success in segmenting the tumor from seven of the 10 CT datasets with &lt;10% false positive errors and five test cases with &lt;10% false negative errors. The consistency of the segmentation results statistics also showed a high repeatability factor, with low values of inter- and intra-user variability for both methods.",Adaptive Computed tomography Fuzzy logic Tumor segmentation,
119,Lumbar spine visualisation based on kinematic analysis from videofluoroscopic imaging,Medical Engineering & Physics,2003,"Low back pain is a significant problem and its cost is enormous to society. However, diagnosis of the underlying causes remains problematic despite extensive study. Reasons for this arise from the deep-rooted situation of the spine and also from its structural complexity. Clinicians have to mentally convert 2-D image information into a 3-D form to gain a better understanding of structural integrity. Therefore, visualisation and animation may be helpful for understanding, diagnosis and for guiding therapy. Some low back pain originates from mechanical disorders, and study of the spine kinematics may provide an insight into the source of the problem. Digital videofluoroscopy was used in this study to provide 2-D image sequences of the spine in motion, but the images often suffer due to noise, exacerbated by the very low radiation dosage. Thus determining vertebrae position within the image sequence presents a considerable challenge. This paper describes a combination of spine kinematic measurements with a solid model of the human lumbar spine for visualisation of spine motion. Since determination of the spine kinematics provides the foundation and vertebral extraction is at the core, this is discussed in detail. Edge detection is a key feature of segmentation and it is shown that phase congruency performs better than most established methods with the rather low-grade image sequences from fluoroscopy. The Hough transform is then applied to determine the positions of vertebrae in each frame of a motion sequence. In the Hough transform, Fourier descriptors are used to represent the vertebral shapes. The results show that the Hough transform is a very promising technique for vertebral extraction from videofluoroscopic images. A dynamic visualisation package has been developed in order to view the moving lumbar spine from any angle and viewpoint. Wire frame models of the vertebrae were built by using CT images from the Visible Human Project and these models are scaled to match the fluoroscopic image data. For animation, the spinal kinematic data from the motion study is incorporated.",Low back pain Visualisation and animation Digital videofluoroscopy Phase congruency Hough transform,
120,A survey of medical image registration on graphics hardware,Computer Methods and Programs in Biomedicine,2011,"The rapidly increasing performance of graphics processors, improving programming support and excellent performance-price ratio make graphics processing units (GPUs) a good option for a variety of computationally intensive tasks. Within this survey, we give an overview of GPU accelerated image registration. We address both, GPU experienced readers with an interest in accelerated image registration, as well as registration experts who are interested in using GPUs. We survey programming models and interfaces and analyze different approaches to programming on the GPU. We furthermore discuss the inherent advantages and challenges of current hardware architectures, which leads to a description of the details of the important building blocks for successful implementations.",Medical imaging Image registration Acceleration Graphics hardware GPU,
121,Distributed edge detection: issues and implementations,"Computational Science Engineering, IEEE",1997,"The article presents experiments in parallelizing an edge detection algorithm on three representative message passing architectures-a low cost, heterogeneous PVM network, an Intel iPSC/860 hypercube, and a CM-5 massively parallel multicomputer. These experiments provide insight into implementation and performance issues for image processing applications",edge detection hypercube networks parallel algorithms parallel machines CM-5 massively parallel multicomputer Intel iPSC/860 hypercube distributed edge detection edge detection algorithm heterogeneous PVM network image processing applications message passing architectures parallel algorithm performance issues Application software Computer vision Concurrent computing Image converters Image edge detection Image restoration Image segmentation Image storage Iterative methods Object detection,
122,Simulation of high-Reynolds number vascular flows,Computer Methods in Applied Mechanics and Engineering,2007,We describe a spectral element solution strategy for simulation of weakly turbulent vascular flows. Novel approaches for treatment of outflow boundary conditions and flow division are presented. We demonstrate that several million gridpoints are required to obtain converged rms statistics when uniform refinement is used and discuss possible approaches to cost reduction.,Turbulence Vascular flows Parallel computing Spectral elements,
123,Synchronized 2D/3D optical mapping for interactive exploration and real-time visualization of multi-function neurological images,Computerized Medical Imaging and Graphics,2013," Efficient software with the ability to display multiple neurological image datasets simultaneously with full real-time interactivity is critical for brain disease diagnosis and image-guided planning. In this paper, we describe the creation and function of a new comprehensive software platform that integrates novel algorithms and functions for multiple medical image visualization, processing, and manipulation. We implement an opacity-adjustment algorithm to build 2D lookup tables for multiple slice image display and fusion, which achieves a better visual result than those of using VTK-based methods. We also develop a new real-time 2D and 3D data synchronization scheme for multi-function MR volume and slice image optical mapping and rendering simultaneously through using the same adjustment operation. All these methodologies are integrated into our software framework to provide users with an efficient tool for flexibly, intuitively, and rapidly exploring and analyzing the functional and anatomical MR neurological data. Finally, we validate our new techniques and software platform with visual analysis and task-specific user studies.",Real-time Visualization Neurological image Multi-function Synchronization Optical mapping Lookup table FMRI Image fusion Registration Human–computer interaction,
124,Penalized maximum-likelihood image reconstruction using space-alternating generalized EM algorithms,"Image Processing, IEEE Transactions on",1995,"Most expectation-maximization (EM) type algorithms for penalized maximum-likelihood image reconstruction converge slowly, particularly when one incorporates additive background effects such as scatter, random coincidences, dark current, or cosmic radiation. In addition, regularizing smoothness penalties (or priors) introduce parameter coupling, rendering intractable the M-steps of most EM-type algorithms. This paper presents space-alternating generalized EM (SAGE) algorithms for image reconstruction, which update the parameters sequentially using a sequence of small âhiddenâ data spaces, rather than simultaneously using one large complete-data space. The sequential update decouples the M-step, so the maximization can typically be performed analytically. We introduce new hidden-data spaces that are less informative than the conventional complete-data space for Poisson data and that yield significant improvements in convergence rate. This acceleration is due to statistical considerations, not numerical overrelaxation methods, so monotonic increases in the objective function are guaranteed. We provide a general global convergence proof for SAGE methods with nonnegativity constraints",convergence of numerical methods image reconstruction maximum likelihood estimation stochastic processes Poisson data SAGE algorithms SAGE methods additive background effects convergence rate cosmic radiation dark current expectation-maximization algorithms hidden data spaces nonnegativity constraints objective function parameter coupling penalized maximum-likelihood image reconstruction random coincidences scatter sequential update smoothness penalties space-alternating generalized EM algorithms Acceleration Convergence Image converters Image reconstruction Optical microscopy Optical scattering Particle scattering Positron emission tomography Single photon emission computed tomography Statistics,
125,GPU-friendly gallbladder modeling in laparoscopic cholecystectomy surgical training system,Computers & Electrical Engineering,2013,"A challenge in virtual reality based laparoscopic cholecystectomy simulation is to construct a fast and accurate deformable gallbladder model. This paper proposed a multi-layered mass-spring model which can adapt well to the built-in accelerating algorithms in PhysX-Engine of Graphics Processing Unit (GPU). The gallbladder was first segmented from clinical Computed Tomography (CT) images. From the segmentation result, a surface mesh of a gallbladder was constructed. The inner layers of a mass-spring model were generated from the surface mesh based on the anatomical structure of gallbladder. We configured the parameters of the springs based on the biomechanical properties of gallbladder to ensure the reality of the deformation results. Preliminary experiments demonstrated that our model was able to achieve satisfactory results in terms of both visual perception and time performance.",,
126,A parallel implementation of the katsevich algorithm for 3-D CT image reconstruction,Journal of Supercomputing,2006,"Yu and Wang [1, 2] implemented the first theoretically exact spiral cone-beam reconstruction algorithm developed by Katsevich [3, 4]. This algorithm requires a high computational cost when the data amount becomes large. Here we study a parallel computing scheme for the Katsevich algorithm to facilitate the image reconstruction. Based on the proposed parallel algorithm, several numerical tests are conducted on a high performance computing (HPC) cluster with thirty two 64-bit AMD-based Opteron processors. The standard phantom data [5] is used to establish the performance benchmarks. The results show that our parallel algorithm significantly reduces the reconstruction time, achieving high speedup and efficiency.",,
127,ParList: A Parallel Data Structure for Dynamic Load Balancing,Journal of Parallel and Distributed Computing,1998,,,
128,Modeling non-linear viscoelastic behavior under large deformations,International Journal of Non-Linear Mechanics,2013," This paper addresses the accurate modeling of the behavior of a thin layer of natural rubber subjected to large deformations. Such a system presents a non-linear viscoelastic behavior similar to many biological soft tissues. The proposed model uses an spring-mass system, and considers a non-linear evolution of the reaction forces of the membrane, as well as viscous and Coulomb friction. In the model, the elasticity coefficient of the springs has an exponential dependence of its elongation. Three experimental tests validate the proposed model, which reaches a real-time performance using an implicit integrator.",Deformable models Exponential spring-mass model Circular membrane Soft tissue,
129,Data parallel language and compiler support for data intensive applications,Parallel Computing,2002,"Processing and analyzing large volumes of data plays an increasingly important role in many domains of scientific research. High-level language and compiler support for developing applications that analyze and process such datasets has, however, been lacking so far. In this paper, we present a set of language extensions and a prototype compiler for supporting high-level object-oriented programming of data intensive reduction operations over multidimensional data. We have chosen a dialect of Java with data-parallel extensions for specifying a collection of objects, a parallel for loop, and reduction variables as our source high-level language. Our compiler analyzes parallel loops and optimizes the processing of datasets through the use of an existing run-time system, called active data repository (ADR). We show how loop fission followed by interprocedural static program slicing can be used by the compiler to  required information for the run-time system. We present the design of a compiler/run-time interface which allows the compiler to effectively utilize the existing run-time system. A prototype compiler incorporating these techniques has been developed using the Titanium front-end from Berkeley. We have evaluated this compiler by comparing the performance of compiler generated code with hand customized ADR code for three templates, from the areas of digital microscopy and scientific simulations. Our experimental results show that the performance of compiler generated versions is, on the average 21% lower, and in all cases within a factor of two, of the performance of hand coded versions.",Data intensive applications Data parallel language Compiler techniques Run-time support Java,
130,On the convergence of an EM-type algorithm for penalized likelihood estimation in emission tomography,"Medical Imaging, IEEE Transactions on",1995,"Recently, we proposed an extension of the expectation maximization (EM) algorithm that was able to handle regularization terms in a natural way. Although very general, convergence proofs were not valid for many possibly useful regularizations. We present here a simple convergence result that is valid assuming only continuous differentiability of the penalty term and can be also extended to other methods for penalized likelihood estimation in tomography",convergence of numerical methods emission tomography maximum likelihood estimation positron emission tomography EM-type algorithm continuous differentiability convergence convergence proofs emission tomography expectation maximization algorithm penalized likelihood estimation penalty term regularization terms regularizations simple convergence result Convergence Detectors Image reconstruction Mathematics Maximum likelihood detection Maximum likelihood estimation Positron emission tomography,
131,Randomized circle detection with isophotes curvature analysis,Pattern Recognition,2015," Circle detection is a critical issue in image analysis and object detection. Although Hough transform based solvers are largely used, randomized approaches, based on the iterative sampling of the edge pixels, are object of research in order to provide solutions less computationally expensive. This work presents a randomized iterative work-flow, which exploits geometrical properties of isophotes in the image to select the most meaningful edge pixels and to classify them in subsets of equal isophote curvature. The analysis of candidate circles is then performed with a kernel density estimation based voting strategy, followed by a refinement algorithm based on linear error compensation. The method has been applied to a set of real images on which it has also been compared with two leading state of the art approaches and Hough transform based solutions. The achieved results show how, discarding up to 57% of unnecessary edge pixels, it is able to accurately detect circles within a limited number of iterations, maintaining a sub-pixel accuracy even in the presence of high level of noise.",Circle detection Sampling strategy Isophotes Density estimation,
132,Differential Evolution as a viable tool for satellite image registration,Applied Soft Computing,2008,"A software system grounded on Differential Evolution to automatically register multiview and multitemporal images is designed, implemented and tested through a set of 2D satellite images on two problems, i.e. mosaicking and changes in time. Registration is effected by looking for the best affine transformation in terms of maximization of the mutual information between the first image and the transformation of the second one, and no control points are needed in this approach. This method is compared against five widely available tools, and its effectiveness is shown.",Differential Evolution Image registration Remote sensing Affine transformation Mutual information,
133,New artificial life model for image enhancement,Expert Systems with Applications,2014," In this work, a method to enhance images based on a new artificial life model is presented. The model is inspired on the behavior of a herbivore organism, when this organism is in a certain environment and selects its food. This organism travels through the image iteratively, selecting the more suitable food and eating parts of it in each iteration. The path that the organism travels through in the image is defined by a priori knowledge about the environment and how it should move in it. Here, we modeled the control and perception centers of the organism, as well as the simulation of its actions and effects on the environment. To demonstrate the efficiency of our method quantitative and qualitative results of the enhancement of synthetic and real images with low contrast and different levels of noise are presented. Obtained results confirm the ability of the new artificial life model for improving the contrast of the objects in the input images.",Image processing Image enhancement Artificial intelligence Artificial life model,
134,Automatic Clustering Using an Improved Differential Evolution Algorithm,"Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on",2008,"Differential evolution (DE) has emerged as one of the fast, robust, and efficient global search heuristics of current interest. This paper describes an application of DE to the automatic clustering of large unlabeled data sets. In contrast to most of the existing clustering techniques, the proposed algorithm requires no prior knowledge of the data to be classified. Rather, it determines the optimal number of partitions of the data on the run."" Superiority of the new method is demonstrated by comparing it with two recently developed partitional clustering techniques and one popular hierarchical clustering algorithm. The partitional clustering algorithms are based on two powerful well-known optimization algorithms",,
135,FPGA-Accelerated Deformable Image Registration for Improved Target-Delineation During CT-Guided Interventions,"Biomedical Circuits and Systems, IEEE Transactions on",2007,"Minimally invasive image-guided interventions (IGIs) are time and cost efficient, minimize unintended damage to healthy tissue, and lead to faster patient recovery. With the advent of multislice computed tomography (CT), many IGIs are now being performed under volumetric CT guidance. Registering pre-and intraprocedural images for improved intraprocedural target delineation is a fundamental need in the IGI workflow. Earlier approaches to meet this need primarily employed rigid body approximation, which may not be valid because of nonrigid tissue misalignment between these images. Intensity-based automatic deformable registration is a promising option to correct for this misalignment; however, the long execution times of these algorithms have prevented their use in clinical workflow. This article presents a field-programmable gate array-based architecture for accelerated implementation of mutual information (Ml)-based deformable registration. The reported implementation reduces the execution time of MI-based deformable registration from hours to a few minutes. This work also demonstrates successful registration of abdominal intraprocedural noncontrast CT (iCT) images with preprocedural contrast-enhanced CT (preCT) and positron emission tomography (PET) images using the reported solution. The registration accuracy for this application was evaluated using 5 iCT-preCT and 5 iCT-PET image pairs. The registration accuracy of the hardware implementation is comparable with that achieved using a software implementation and is on the order of a few millimeters. This registration accuracy, coupled with the execution speed and compact implementation of the reported solution, makes it suitable for integration in the IGI-workflow.",biological tissues biomedical electronics computerised tomography field programmable gate arrays image enhancement image registration medical image processing positron emission tomography CT-guided interventions FPGA-accelerated deformable image registration PET abdominal intraprocedural noncontrast CT images contrast-enhanced CT field-programmable gate array-based architecture hardware implementation improved target-delineation intensity-based automatic deformable registration intraprocedural images minimally invasive image-guided interventions multislice computed tomography mutual information based deformable registration nonrigid tissue misalignment patient recovery positron emission tomography images preprocedural images registration accuracy rigid body approximation software implementation volumetric CT guidance Abdomen Acceleration Application software Computed tomography Costs Field programmable gate arrays Image registration Minimally invasive surgery Mutual information Positron emission tomography Computer tomography (CT)-guided interventions field-programmable gate arrays (FPGA) image registration mutual information,
136,A cluster computer system for the analysis and classification of massively large biomedical image data,Computers in Biology and Medicine,1998,"The current trend in medical image acquisition is towards the generation of image datasets which are massively large, either because they exhibit fine x, y, or z resolution, are volumetric, are multispectral, or a combination of all of the preceding. Such images pose a significant computational challenge in their analysis, not only in terms of data throughput, but also in terms of platform costs and simplicity. In this paper we describe the role of a cluster of workstations together with two quite different application programming interfaces (APIs) in the quantitative analysis of anatomic image data from the visible human project using an MRF-Gibbs classification algorithm. We describe the typical architecture of a cluster computer, two API options and the parallelization of the MRF-Gibbs procedure for the cluster. Finally, we show speedup results obtained on the cluster and sample classifications of visible human data.",image processing cluster computing parallel computing MRF-Gibbs classification,
137,On the representation and management of medical records in a knowledge-based system,Expert Systems with Applications,1993,"The need for adapting Hospital Information Systems (HISs) to new applications (aimed at clinical data management) and to the ever growing population requires an evolution from both the representation and the usability points of view. The goal of this paper is to discuss the interdependencies of the advanced data representation models necessary to the new HISs and the interaction requirements. In particular, we present the technical taken in the MILORD (Multimedia Interaction with Large Object-oriented Radiological and clinical Databases) project to implement a departmental environment which integrates different kinds of clinical data in a user-friendly and homogeneous way. The implementation platform of the MILORD system is based on results obtained in the KIWIS project, which produced an advanced knowledge-base environment for large database systems. The interaction with a HIS is analyzed identifying the HIS-specific interaction tasks, the typical phases of the interaction with an information system, and the necessary interaction paradigms. In particular we describe the approach adopted in KIWIS for overcoming the problem of querying object-oriented databases and we discuss the new requirements that multimediality poses on the interaction paradigms.",,
138,Data-driven estimation of cardiac electrical diffusivity from 12-lead ECG signals,Medical Image Analysis,2014," Diagnosis and treatment of dilated cardiomyopathy (DCM) is challenging due to a large variety of causes and disease stages. Computational models of cardiac electrophysiology (EP) can be used to improve the assessment and prognosis of DCM, plan therapies and predict their outcome, but require personalization. In this work, we present a data-driven approach to estimate the electrical diffusivity parameter of an EP model from standard 12-lead electrocardiograms (ECG). An efficient forward model based on a mono-domain, phenomenological Lattice-Boltzmann model of cardiac EP, and a boundary element-based mapping of potentials to the body surface is employed. The electrical diffusivity of myocardium, left ventricle and right ventricle endocardium is then estimated using polynomial regression which takes as input the QRS duration and electrical axis. After validating the forward model, we computed 9500 EP simulations on 19 different DCM patients in just under three seconds each to learn the regression model. Using this database, we quantify the intrinsic uncertainty of electrical diffusion for given ECG features and show in a leave-one-patient-out cross-validation that the regression method is able to predict myocardium diffusion within the uncertainty range. Finally, our approach is tested on the 19 cases using their clinical ECG. 84% of them could be personalized using our method, yielding mean prediction errors of 18.7 ms for the QRS duration and 6.5 ° for the electrical axis, both values being within clinical acceptability. By providing an estimate of diffusion parameters from readily available clinical data, our data-driven approach could therefore constitute a first calibration step toward a more complete personalization of cardiac EP.",Cardiac electrophysiology Statistical learning Lattice-Boltzmann method Uncertainty quantification Electrocardiogram,
139,"C++ OPPS, a new software for the interpretation of protein dynamics from nuclear magnetic resonance measurements",International Journal of Quantum Chemistry,2010,"Nuclear magnetic resonance (NMR) is a powerful tool for elucidating protein dynamics because of the possibility to interpret nuclear spin relaxation properties in terms of microdynamic parameters. Magnetic relaxation times T1, T2, and NOE depend on dipolar and quadrupolar interactions, on chemical shift anisotropy and cross-correlation effects. Within the framework of given motional model, it is possible to express the NMR relaxation times as functions of spectral densities (Abragam, The Principles of Nuclear Magnetism; Oxford University Press: Clarendon, London, 1961), obtaining the connection between macroscopic observables and microscopic properties. In this context, recently Meirovitch et al. (Shapiro et al., Biochemistry 2002, 41, 6271, Meirovitch et al., J Phys Chem B 2006, 110, 20615, Meirovitch et al., J Phys Chem B 2007, 111, 12865) applied the dynamical model introduced by Polimeno and Freed (Polimeno and Freed, Adv Chem Phys 1993, 83, 89, Polimeno and Freed, J Phys Chem 1995, 99, 10995), known as the slowly relaxing local structure (SRLS) model, to the study of NMR data.The program C++OPPS (http://www.chimica.unipd.it/licc/>http://www.chimica.unipd.it/licc/)", implements the SRLS model in an user-friendly way with a graphical user interface (GUI),
140,Segmentation by retrieval with guided random walks: Application to left ventricle segmentation in MRI,Medical Image Analysis,2013,"In this paper, a new segmentation framework with prior knowledge is proposed and applied to the left ventricles in cardiac Cine MRI sequences. We introduce a new formulation of the random walks method, coined as guided random walks, in which prior knowledge is integrated seamlessly. In comparison with existing approaches that incorporate statistical shape models, our method does not  any principal model of the shape or appearance of the left ventricle. Instead, segmentation is accompanied by retrieving the closest subject in the database that guides the segmentation the best. Using this techniques, rare cases can also effectively exploit prior knowledge from few samples in training set. These cases are usually disregarded in statistical shape models as they are outnumbered by frequent cases (effect of class population). In the worst-case scenario, if there is no matching case in the database to guide the segmentation, performance of the proposed method reaches to the conventional random walks, which is shown to be accurate if sufficient number of seeds is provided. There is a fast solution to the proposed guided random walks by using sparse linear matrix operations and the whole framework can be seamlessly implemented in a parallel architecture. The method has been validated on a comprehensive clinical dataset of 3D+t short axis MR images of 104 subjects from 5 categories (normal, dilated left ventricle, ventricular hypertrophy, recent myocardial infarction, and heart failure). The average segmentation errors were found to be 1.54 mm for the endocardium and 1.48 mm for the epicardium. The method was validated by measuring different algorithmic and physiologic indices and quantified with manual segmentation ground truths, provided by a cardiologist.",Cardiac Cine MRI Segmentation by retrieval Random walks Left ventricle segmentation Guided random walks,
141,Pulse quarantine strategy of internet worm propagation: Modeling and analysis,Computers & Electrical Engineering,2012,"Worms can spread throughout the Internet very quickly and are a great security threat. Constant quarantine strategy is a defensive measure against worms, but its reliability in current imperfect intrusion detection systems is poor. A pulse quarantine strategy is thus proposed in the current study. The pulse quarantine strategy adopts a hybrid intrusion detection system with both misuse and anomaly detection. Through analysis of corresponding worm propagation models, its stability condition is obtained: when the basic reproduction number is less than one, the model is stable at its infection-free periodic equilibrium point where worms get eliminated. Numerical and simulation experiments show that constant quarantine strategy is inefficient because of its high demand on the patching rate at “birth”, whereas the pulse quarantine strategy can lead to worm elimination with a relatively low value. As patching almost all hosts in the actual network is difficult, the pulse quarantine strategy is more effective in worm elimination.",,
142,PACS-based functional Magnetic Resonance Imaging,Computerized Medical Imaging and Graphics,2003,"The picture archiving and communication system (PACS) technology reaches its 10th anniversary. Retrospectively no one could foresee the impact the PACS would have to the health care enterprise, but it is common consent today, that PACS is the key technology crucial to daily clinical image operations and especially to image related basic and clinical research. During the past 10 years the PACS has been matured from a research and developmental stage into commercial products which are provided by all major modality and health care equipment vendors. The PACS, originally implemented in the Radiology Department, needs to grow and has already carried well beyond departmental limits conquering all image relevant areas inside the hospital. During the past 10 years a dramatic development in imaging techniques especially within MRI emerged. Advanced 3D- and 4D-MR imaging techniques result in much more images and more complex data objects than ever before which need to be implemented into the existing PACS. These new imaging techniques require intensive post-processing apart from the imaging modality which need to be integrated into the image workflow and the PACS implementation. Along with these new imaging techniques new clinical applications, e.g. stroke detection, and research applications, e.g. study of heart and brain function, in Neurology and Cardiology require changes to the traditional PACS concept. Therefore inter-disciplinary image distribution will become the high-water mark for the next 10 years in the PACS endeavor. This paper focuses on one new advanced imaging technique, functional magnetic resonance imaging (fMRI), and discusses how fMRI data is defined, what fMRI requires in terms of clinical and research applications and how to implement fMRI in the existing PACS.",Picture archiving and communication system Functional magnetic resonance imaging Digital imaging and communications in medicine Image processing,
143,Remote sensing image registration via active contour model,AEU - International Journal of Electronics and Communications,2009,"Image registration is the process by which we determine a transformation that provides the most accurate match between two images. The search for the matching transformation can be automated with the use of a suitable metric, but it can be very time-consuming and tedious. In this paper, we introduce a registration algorithm that combines active contour segmentation together with mutual information. Our approach starts with a segmentation procedure. It is formed by a novel geometric active contour, which incorporates edge knowledge, namely Edgeflow, into active contour model. Two edgemap images filled with closed contours are obtained. After ruling out mismatched curves, we use mutual information (MI) as a similarity measure to register two edgemap images. Experimental results are provided to illustrate the performance of the proposed registration algorithm using both synthetic and multisensor images. Quantitative error analysis is also provided and several images are shown for subjective evaluation.",Image registration Geometric active contour Edgeflow Edgemap Mutual information,
144,A path selection-based algorithm for real-time data staging in Grid applications,Journal of Parallel and Distributed Computing,2005,"Efficient data scheduling is becoming an important issue in distributed real-time applications that produce huge data sets. The Grid environment on which these applications may run seeks to harness the geographically distributed resources for the applications. Scheduling components should account for real-time measures of the applications and reduce communication overhead due to enormous data size experienced, especially in dissemination applications. In this study, we consider the data staging scheme to provide the dissemination of large-scale data sets for the distributed real-time applications. We propose a new path selection-based algorithm for optimizing a criterion that reflects the general satisfiability of the system. The algorithm adopts a blocking-time analysis method combined with a simple heuristic to explore the most likely regions of a search space. Two heuristics are provided for the algorithm to explore these regions of the search space. Simulation results show that the proposed algorithm together with either of the heuristic has higher performance compared to other algorithms in the literature. We also show by simulation that a new optimization criterion we proposed in this study is successful in improving the performance of the individual applications.",Data-intensive and real-time applications Data staging Data scheduling Blocking analysis Concurrency in scheduling,
145,A robust global and local mixture distance based non-rigid point set registration,Pattern Recognition,2015," We present a robust global and local mixture distance (GLMD) based non-rigid point set registration method which consists of an alternating two-step process: correspondence estimation and transformation updating. We first define two distance features for measuring global and local structural differences between two point sets, respectively. The two distances are then combined to form a GLMD based cost matrix which provides a flexible way to estimate correspondences by minimizing global or local structural differences using a linear assignment solution. To improve the correspondence estimation and enhance the interaction between the two steps, an annealing scheme is designed to gradually change the cost minimization from local to global and the thin plate spline transformation from rigid to non-rigid during registration. We test the performance of our method in contour registration, sequence images and real images, and compare with six state-of-the-art methods where our method shows the best alignments in most scenarios.",Non-rigid point set registration Global and local mixture distance Correspondence estimation Transformation updating Multi-feature based framework,
146,fMRI analysis on the GPU—Possibilities and challenges,Computer Methods and Programs in Biomedicine,2012,"Functional magnetic resonance imaging (fMRI) makes it possible to non-invasively measure brain activity with high spatial resolution. There are however a number of issues that have to be addressed. One is the large amount of spatio-temporal data that needs to be processed. In addition to the statistical analysis itself, several preprocessing steps, such as slice timing correction and motion compensation, are normally applied. The high computational power of modern graphic cards has already successfully been used for MRI and fMRI. Going beyond the first published demonstration of GPU-based analysis of fMRI data, all the preprocessing steps and two statistical approaches, the general linear model (GLM) and canonical correlation analysis (CCA), have been implemented on a GPU. For an fMRI dataset of typical size (80 volumes with 64 × 64 × 22 voxels), all the preprocessing takes about 0.5 s on the GPU, compared to 5 s with an optimized CPU implementation and 120 s with the commonly used statistical parametric mapping (SPM) software. A random permutation test with 10,000 permutations, with smoothing in each permutation, takes about 50 s if three GPUs are used, compared to 0.5–2.5 h with an optimized CPU implementation. The presented work will save time for researchers and clinicians in their daily work and enables the use of more advanced analysis, such as non-parametric statistics, both for conventional fMRI and for real-time fMRI.",Functional magnetic resonance imaging (fMRI) Graphics processing unit (GPU) CUDA General linear model (GLM) Canonical correlation analysis (CCA) Random permutation test,
147,Inexact block Newton methods for solving nonlinear equations,Applied Mathematics and Computation (New York),2005,"In the paper two parallelizable inexact block Newton methods are presented for solving large and sparse nonlinear equations. The basic idea is simple and direct. Combining the simplified Newton method with the component averaging (CAV) method [Parallel Comput. 27 (2001) 777] results in an inexact Newton method, called simplified Newton-CAV method. Parallel tests of the algorithm are implemented on a message-passing distributed-memory multiprocessor architecture such as a cluster of workstations. The results show that the new algorithm can achieve good performance. Moreover as a development of the simplified Newton-CAV method, the overlapped block Newton-CAV method is further proposed and discussed. 2004 Elsevier Inc. All rights reserved.",Nonlinear equations Algorithms Boundary value problems Computational methods Computer architecture Computer workstations Distributed computer systems Iterative methods Linear systems Matrix algebra,
148,VizIR—a framework for visual information retrieval,Journal of Visual Languages & Computing,2003,"In this paper the visual information retrieval project VizIR is presented. The goal of the project is the implementation of an open visual information retrieval (VIR) prototype as basis for further research on major problems of VIR. The motivation behind VizIR is the implementation of an open platform for supporting and facilitating research, teaching, the exchange of research results and research cooperation in the field in general. The availability of this platform could make cooperation and such research (especially for smaller institutions) easier. The intention of this paper is to inform interested researchers about the VizIR project and its design and to invite people to participate in the design and implementation process. We describe the goals of the VizIR project, the intended design of the querying framework, the user interface design and major implementation issues. The querying framework consists of classes for feature extraction, similarity measurement, media handling and database access. User interface design includes a description of visual components and their class structure, the communication between panels and the communication between visual components and query engines. The latter is based on the multimedia retrieval markup language (MRML, Website. http://www.mrml.net (last visited: 2003–03–20)). To be compatible with our querying paradigm, we extend MRML with additional elements. Implementation issues include a sketch on advantages and drawbacks of existing cross-platform media processing frameworks: Java Media Framework, OpenML and DirectX/DirectShow and details on the Java components used for user interface implementation, 3D graphics with Java and Java XML parsing.",Visual information retrieval Content-based image retrieval Content-based video retrieval Media processing,
149,Accessing medical image file with co-allocation HDFS in cloud,Future Generation Computer Systems,2015," Patient privacy has recently become the most important issue in the World Health Organization (WHO) and the United States and Europe. However, inter-hospital medical information is currently shared using paper-based operations, and this is an important research issue for the complete and immediate exchange of electronic medical records to avoid duplicate prescriptions or procedures. An electronic medical record (EMR) is a computerized medical record created by a care-giving organization, such as a hospital and doctor’s surgery. Using electronic medical records can improve patient’s privacy and health care efficiency. Although there are many advantages to electronic medical records, the problem of exchanging and sharing medical images remains to be solved. The motivation of this paper is to attempt to resolve the problems of storing and sharing electronic medical records and medical images between different hospitals. Cloud Computing is enabled by the existing parallel and distributed technology, which provides computing, storage and software services to users. Specifically, this study develops a Medical Image File Accessing System (MIFAS) based on HDFS of Hadoop in cloud. The proposed system can improve medical imaging storage, transmission stability, and reliability while providing an easy-to-operate management interface. This paper focuses on the cloud storage virtualization technology to achieve high-availability services. We have designed and implemented a medical imaging system with a distributed file system. The experimental results show that the high reliability data storage clustering and fault tolerance capabilities can be achieved.",EMR PACS Hadoop HDFS Co-allocation Cloud computing,
150,A template for developing next generation parallel Delaunay refinement methods,Finite Elements in Analysis and Design,2010,"We describe a complete solution for both sequential and parallel construction of guaranteed quality Delaunay meshes for general two-dimensional geometries. We generalize the existing sequential point placement strategies for guaranteed quality mesh refinement: instead of a specific position for a new point, we derive two types of two-dimensional regions which we call selection disks. Both types of selection disks are inside the circumdisk of a poor quality triangle, with the Type I disk always inside the Type II disk. We prove that any point placement algorithm which inserts Steiner points inside selection disks of Type I terminates, and any algorithm which inserts Steiner points inside selection disks of Type II produces an asymptotically size-optimal mesh. In the area of parallel Delaunay mesh refinement, we develop a new theoretical framework for the construction of graded meshes on parallel architectures, i.e., for parallel mesh generation with element size controlled by a user-defined criterion. Our sufficient conditions of point Delaunay-independence allow to select points for concurrent insertion in such a way that the standard sequential guaranteed quality Delaunay refinement procedures can be applied in parallel to attain the required element quality constraints. Finally, we present a novel parallel algorithm which can be used in conjunction with any sequential point placement strategy that chooses points within the selection disks. We implemented our algorithm for shared memory multi-core architectures and present the experimental results. Our data show that even on workstations with a few cores, which are now in common use, our implementation is significantly faster than the best sequential counterpart.",Delaunay triangulation Mesh generation Parallel refinement,
151,Double recurrent interaction V1–V2–V4 based neural architecture for color natural scene boundary detection and surface perception,Applied Soft Computing,2014," In this paper, a new neural model for bio-inspired processing of color images, called dPREEN (double recurrent Perceptual boundaRy dEtection Neural) model, is presented. The dPREEN model includes a double feedback among V1, V2 and V4 cortical areas, simple and double color opponent processes, orientation filtering using Gabor kernels, surround suppression in complex cells, top-down and bottom-up information fusion and chromatic diffusion, to generate contours of perceptual significance in color natural scenes. The outputs of the model are a boundary map of the scene and surface perception images. This paper incorporates a comparative analysis of the proposed model against two other contour extraction methods in the Berkeley Segmentation Dataset and Benchmark. The analysis shows favorable results to the dPREEN model. Additionally, this paper describes two parallel implementations of the model for its execution on Graphics Processing Units.",Boundary detection Neural networks Color image processing Bio-inspired models GPU CUDA,
152,Enhancing distributed differential evolution with multicultural migration for global numerical optimization,Information Sciences,2013," Differential evolution (DE) is a prominent stochastic optimization technique for global optimization. After its original definition in 1995, DE frameworks have been widely researched by computer scientists and practitioners. It is acknowledged that structuring a population is an efficient way to enhance the algorithmic performance of the original, single population (panmictic) DE. However, only a limited amount of work focused on Distributed DE (DDE) due to the difficulty of designing an appropriate migration strategy. Since a proper migration strategy has a major impact on the performance, there is a large margin of improvement for the DDE performance. In this paper, an enhanced DDE algorithm is proposed for global numerical optimization. The proposed algorithm, namely DDE with Multicultural Migration (DDEM) makes use of two migration selection approaches to maintain a high diversity in the subpopulations, Target Individual Based Migration Selection (TIBMS) and Representative Individual Based Migration Selection (RIBMS), respectively. In addition, the diversity amongst the individuals is controlled by means of the proposed Affinity Based Replacement Strategy (ABRS) mechanism. Numerical experiments have been performed on 34 diverse test problems. The comparisons have been made against DDE algorithms using classical migration strategies and three popular DDE variants. Experimental results show that DDEM displays a better or equal performance with respect to its competitors in terms of the quality of solutions, convergence, and statistical tests.",Distributed differential evolution Multicultural migration Numerical optimization,
153,A decomposition method with minimum communication amount for parallelization of multi-dimensional FFTs,Computer Physics Communications,2014," The fast Fourier transform (FFT) is undoubtedly an essential primitive that has been applied in various fields of science and engineering. In this paper, we present a decomposition method for the parallelization of multi-dimensional FFTs with the smallest communication amounts for all ranges of the number of processes compared to previously proposed methods. This is achieved by two distinguishing features: adaptive decomposition and transpose order awareness. In the proposed method, the FFT data is decomposed based on a row-wise basis that maps the multi-dimensional data into one-dimensional data, and translates the corresponding coordinates from multi-dimensions into one dimension so that the one-dimensional data can be divided and allocated equally to the processes using a block distribution. As a result and different from previous works that have the dimensions of decomposition pre-defined, our method can adaptively decompose the FFT data on the lowest possible dimensions depending on the number of processes. In addition, this row-wise decomposition provides plenty of alternatives in data transpose, and different transpose order results in different amounts of communication. We identify the best transpose orders with the smallest communication amounts for the 3-D, 4-D, and 5-D FFTs by analyzing all possible cases. We also develop a general parallel software package for the most popular 3-D FFT based on our method using the 2-D domain decomposition. Numerical results show good performance and scaling properties of our implementation in comparison with other parallel packages. Given both communication efficiency and scalability, our method is promising in the development of highly efficient parallel packages for the FFT.",Fast Fourier transform Multi-dimensional FFTs Domain decomposition for FFTs Domain decomposition method Parallel FFTs,
154,Assessment of surgical effects on patients with obstructive sleep apnea syndrome using computational fluid dynamics simulations,Mathematics and Computers in Simulation,2014,"Obstructive sleep apnea syndrome is one of the most common sleep disorders. To treat patients with this health problem, it is important to detect the severity of this syndrome and occlusion sites in each patient. The goal of this study is to test the hypothesis that the cure of obstructive sleep apnea syndrome by maxillomandibular advancement surgery can be predicted by analyzing the effect of anatomical airway changes on the pressure effort required for normal breathing using a high-fidelity, 3-D numerical model. The employed numerical model consists of: (1) 3-D upper airway geometry construction from patient-specific computed tomographic scans using an image segmentation technique, (2) mixed-element mesh generation of the numerically constructed airway geometry for discretizing the domain of interest, and (3) computational fluid dynamics simulations for predicting the flow field within the airway and the degree of severity of breathing obstruction. In the present study, both laminar and turbulent flow simulations were performed to predict the flow field in the upper airway of the selected patients before and after maxillomandibular advancement surgery. Patients of different body mass indices were also studied to assess their effects. The numerical results were analyzed to evaluate the pressure gradient along the upper airway. The magnitude of the pressure gradient is regarded as the pressure effort required for breathing, and the extent of reduction of the pressure effort is taken to measure the success of the surgery. The description of the employed numerical model, numerical results from simulations of various patients, and suggestion for future work are detailed in this paper.",Obstructive sleep apnea syndrome (OSAS) Sleep apnea Computational fluid dynamics (CFD) Mesh generation Maxillomandibular advancement (MMA),
155,A reliable surface reconstruction system in biomedicine,Computer Methods and Programs in Biomedicine,2007,"For common biomedical imaging facilities, such as CT, MRI, and confocal microscopy, the acquired scans are sequential parallel sections. The object of interest in each section image can be extracted by segmentation procedure to form serial parallel planar contours. How to reconstruct a trustworthy surface from these contours is a crucial issue in biomedical 3D visualization. In this paper, we propose an automatic, fast, and reliable surface reconstruction system. An improved correspondence-determining algorithm is proposed in the system to provide more reasonable contour-correspondences than the existing algorithms. It can handle more general input data, and does not produce wrong reconstruction results. A hybrid tiling algorithm is presented to tile the corresponding contours without the requirement of a contour-matching procedure. It can also handle the branching problem without any modification. For degenerate cases and branches, intermediate contours are introduced by means of contour interpolation to enhance the reconstruction results. The surface area and volume are also calculated to facilitate the practical applications.",Surface reconstruction Contour-correspondence Tiling Branching Degenerate,
156,Template-driven segmentation of confocal microscopy images,Computer Methods and Programs in Biomedicine,2008,"High quality 3D visualization of anatomic structures is necessary for many applications. The anatomic structures first need to be segmented. A variety of segmentation algorithms have been developed for this purpose. For confocal microscopy images, the noise introduced during the specimen preparation process, such as the procedure of penetration or staining, may cause images to be of low contrast in some regions. This property will make segmentation difficult. Also, the segmented structures may have rugged surfaces in 3D visualization. In this paper, we present a hybrid method that is suitable for segmentation of confocal microscopy images. A rough segmentation result is obtained from the atlas-based segmentation via affine registration. The boundaries of the segmentation result are close to the object boundaries, and are regarded as the initial contours of the active contour models. After convergence of the snake algorithm, the resulting contours in regions of low contrast are locally refined by parametric bicubic surfaces to alleviate the problem of incorrect convergence. The proposed method increases the accuracy of the snake algorithm because of better initial contours. Besides, it can provide smoother segmented results in 3D visualization.",Segmentation Confocal microscopy Active contour model Template-driven,
157,A comparison of 3D cone-beam Computed Tomography (CT) image reconstruction performance on homogeneous multi-core processor and on other processors,Measurement,2011,"Computed Tomography (CT) has become an important noninvasive tool in diagnostic medicine. In fact, the use of cone-beam CT is growing in the clinical area due to its ability to provide 3D volumetric information. This study aims to parallelize an analytical 3D image reconstruction algorithm and then implement it on a commercial homogeneous multi-core processor. It is indicated from our results that the homogeneous multi-core processor actually achieved faster reconstruction than the heterogeneous processor-based or the FPGA-based implementation. Our results demonstrated that the shared cache mechanism associated with the homogeneous processor would represent a significant benefit to performing high-speed CT reconstruction or other High Performance Computing (HPC) applications. In addition, while the Graphic Processing Unit (GPU) is considered the most powerful processor nowadays, our experimental results further indicated that the joint use of the multi-core CPU and GPU could enhance the processing power as well as improve the computational efficiency.",Computed Tomography (CT) Graphic Processing Unit (GPU) Hardware acceleration 3D reconstruction Multi-core processor Parallel processing,
158,Competitive neural network to solve scheduling problems,Neurocomputing,2001,"Most scheduling problems have been demonstrated to be NP-complete problems. The Hopfield neural network is commonly applied to obtain an optimal solution in various different scheduling applications, such as the traveling salesman problem (TSP), a typical discrete combinatorial problem. Hopfield neural networks, although providing rapid convergence to the solution, require extensive effort to determine coefficients. A competitive learning rule provides a highly effective means of attaining a sound solution and can reduce the effort of obtaining coefficients. Restated, the competitive mechanism reduces the network complexity. This important feature is applied to the Hopfield neural network to derive a new technique, i.e. the competitive Hopfield neural network technique. This investigation employs the competitive Hopfield neural network to resolve a multiprocessor problem with no process migration, time constraints (execution time and deadline), and limited resources. Simulation results demonstrate that the competitive Hopfield neural network imposed on the proposed energy function ensures an appropriate approach to solving this class of scheduling problems.",Scheduling Winner-take-all Competitive learning Hopfield neural network,
159,Image compression on a VLSI neural-based vector quantizer,Information Processing & Management,1992,A self-organization algorithm for image compression and the associated VLSI architecture are presented. A frequency upper-threshold is effectively used in the centroid learning method. Performances of the self-organization networks and traditional nonself-organization algorithms for vector quantization are compared. This new algorithm is quite efficient and can achieve near-optimal results. A trainable VLSI neuroprocessor based upon this new self-organization network has been developed for high-speed and high-ratio image compression applications. This neural-based vector quantization design includes a fully parallel vector quantizer and a pipelined codebook generator which obtains a time complexity O (1) for each quantization vector. A 5 × 5-dimensional vector quantizer prototype chip has been designed and fabricated. It contains 64 inner-product neural units and an extendable winner-take-all block. This mixed-signal chip occupies a compact silicon area of 4.6 × 6.8 mm2 in a 2.0-μm scalable CMOS technology. It provides a computing capability as high as 3.33 billion connections per second. It can achieve a speedup factor of 110 compared with a SUN-4/75 workstation for a compression ratio of 33. Real-time adaptive VQ on industrial 1024 × 1024 pixel images is feasible using multiple neuroprocessor chips. An industrial-level design to achieve 104 billion connections per second for the 1024-codevector vector quantizer can be fabricated in a 125 mm2 chip through a 1 μm CMOS technology.,Image processing VLSI Vector quantization Neural network Video processing,
160,A distributed and interactive three-dimensional medical image system,Computerized Medical Imaging and Graphics,1994,"Three-dimensional (3D) arrays of digital data representing spatial volumes arise in many scientific applications, such as computed tomography (CT) and magnetic resonance imaging (MRI) created by imaging a series of cross sections of human bodies in medical applications. In this article, a software system architecture, called DISCOVER (a Distributed Interactive Scientific Computing and Visualization EnviRonment), which can take advantage of the power of parallel processing, is proposed and implemented for interactive visualization and manipulation of the 3D digital data. The surface-rendering and the volume-rendering algorithms are implemented. The same software program can be executed on several different hardware platforms. We also propose a new rendering algorithm, called volume-surface rendering, for medical applications. The algorithm enables users to visualize the external and internal structures of medical objects simultaneously. The network version of the DISCOVER, as it stands today, is in practical use in the Hospital of National Cheng Kung University in Taiwan for real clinical applications.",3D medical image Distributed computing Parallel processing Surface rendering Volume rendering 3D manipulation User interaction Scientific visualization,
161,Lattice Boltzmann method for filtering and contour detection of the natural images,Computers & Mathematics with Applications,2014," In this paper, the lattice Boltzmann method (LBM) is extended to study the filtering and contour detection of natural images, and a new lattice Boltzmann model is proposed for more complicated image processing model, like the Ambrosio and Tortorelli (A–T) model that contains two coupled nonlinear partial differential equations. The numerical results of image filtering and contour detection show that the noises in the image can be removed greatly, and simultaneously, important contours of the image are protected well. To improve the computational efficiency, we implement the developed lattice Boltzmann model on Graphic Processing Unit (GPU), and find that, compared to the CPU based algorithm, the GPU based LBM can gain more than 25 × speedup, which is very important in the further lattice Boltzmann study of large-scale image processing problems. And finally, these numerical results also show that the LBM is a feasible and efficient approach for filtering and contour detection of the natural images.",Lattice Boltzmann method Graphic processing unit Image filtering Contour detection Edge map,
162,Constructing trusted virtual execution environment in P2P grids,Future Generation Computer Systems,2010,"P2P grid is a natural merger of grid computing and P2P computing. Currently, P2P grids are hard to be commercially adopted because user programs and sensitive data are compromised easily and no trusted execution environment is provided on P2P grid nodes. Virtualization technologies become more and more popular, which allows one computer system to function as multiple virtual systems. When a P2P grid node is equipped with virtualization technologies, the virtual machine monitor (VMM) under the operating system is more secure than the OS because the VMM is much less complicated than the OS, and trusted platform module (TPM) embedded into the underlying hardware can provide integrity protection for the VMM. In this paper, we introduce how to construct a trusted execution environment on P2P grid nodes equipped with secure VMM. The VM images used for deploying virtual execution environment are protected and verified. A VM image will be selected and deployed onto a P2P grid node according to the job requirement and node situation, such as node performance and node reputation. Finally, the overhead of trusted image store and deployment is analyzed.",P2P grid Virtual machine Trusted computing Trusted execution environment Trusted deployment,
163,Efficient and robust large medical image retrieval in mobile cloud computing environment,Information Sciences,2014," This paper presents an efficient and robust content-based large medical image retrieval method in mobile Cloud computing environment, called the Mirc. The whole query process of the Mirc is composed of three steps. First, when a clinical user submits a query image Iq, a parallel image set reduction process is conducted at a master node. Then the candidate images are transferred to the slave nodes for a refinement process to obtain the answer set. The answer set is finally transferred to the query node. The proposed method including an priority-based robust image block transmission scheme is specifically designed for solving the instability and the heterogeneity of the mobile cloud environment, and an index-support image set reduction algorithm is introduced for reducing the data transfer cost involved. We also propose a content-aware and bandwidth-conscious multi-resolution-based image data replica selection method and a correlated data caching algorithm to further improve the query performance. The experimental results show that the performance of our approach is both efficient and effective, minimizing the response time by decreasing the network transfer cost while increasing the parallelism of I/O and CPU.",Medical image Multi-resolution Mobile cloud,
164,Implementation of a medical image file accessing system in co-allocation data grids,Future Generation Computer Systems,2010,"There are two challenges of using the PACS (Picture Archiving and Communications System). First, PACS are limited to certain bandwidths and locations. Second, the high cost of maintaining Web PACS and the difficult management of Web PACS servers. Besides, the quality of transporting images and the bandwidth of accessing large files from different locations are difficult to guarantee. For instance, radiologists make use of PACS information system for achieving high-speed accessing medical images. Physicians, on the other hand, utilize web browsers to indirectly access the PACS information system via non-high-speed network. The insufficient bandwidth may cause bottleneck under a host of querying and accessing. As hospitals exchange large files such as medical images with each other via WANs, the bandwidth cannot support the huge amount of file transportation. In this paper, we propose a PACS based on data grids, and utilize MIFAS (Medical Image File Accessing System) to perform querying and retrieving medical images from the co-allocation data grid. MIFAS is also suitable for data grid environments with a server node and several client nodes. MIFAS can take advantage of the co-allocation modules to reduce the medical image transfer time. Also, we provide experiments to show the performance of MIFAS. Furthermore, in order to enhance the security, stability and reliability in the PACS, we also provide the user-friendly management interface.",Medical images Data grid Grid computing Co-allocation File transferring,
165,Real-Time 3-D Ultrasound Scan Conversion Using a Multicore Processor,"Information Technology in Biomedicine, IEEE Transactions on",2009,"Real-time 3-D ultrasound scan conversion (SC) in software has not been practical due to its high computation and I/O data handling requirements. In this paper, we describe software-based 3-D SC with high volume rates using a multicore processor, Cell. We have implemented both 3-D SC approaches: 1) the separable 3-D SC where two 2-D coordinate transformations in orthogonal planes are performed in sequence and 2) the direct 3-D SC where the coordinate transformation is directly handled in 3-D. One Cell processor can scan-convert a 192 times 192 times 192 16-bit volume at 87.8 volumes/s with the separable 3-D SC algorithm and 28 volumes/s with the direct 3-D SC algorithm.",biology computing biomedical ultrasonics 3-D SC algorithm Cell processor multicore processor orthogonal planes real-time 3D ultrasound scan conversion 3-D scan conversion (SC) 3-D ultrasound Cell processor multicore real time Algorithms Image Processing Computer-Assisted Imaging Three-Dimensional Software Ultrasonography,
166,Acceleration of fluoro-CT reconstruction for a mobile C-arm on GPU and FPGA hardware: A simulation study - art. no. 61424L,,2006,"CT imaging in interventional and minimally-invasive surgery requires high-performance computing solutions that meet operational room demands, healthcare business requirements, and the constraints of a mobile C-arm system. The computational requirements of clinical procedures using CT-like data are increasing rapidly, mainly due to the need for rapid access to medical imagery during critical surgical procedures. The highly parallel nature of Radon transform and CT algorithms enables embedded computing solutions utilizing a parallel processing architecture to realize a significant gain of computational intensity with comparable hardware and program coding/testing expenses. In this paper, using a sample 2D and 3D CT problem, we explore the programming challenges and the potential benefits of embedded computing using commodity hardware components. The accuracy and performance results obtained on three computational platforms: a single CPU, a single GPU, and a solution based on FPCA technology have been analyzed. We have shown that hardware-accelerated CT image reconstruction can be achieved with similar levels of noise and clarity of feature when compared to program execution on a CPU, but gaining a performance increase at one or more orders of magnitude faster. 3D cone-beam or helical CT reconstruction and a variety of volumetric image processing applications will benefit from similar accelerations.",,
167,Statistical models of sets of curves and surfaces based on currents,Medical Image Analysis,2009,"Computing, visualizing and interpreting statistics on shapes like curves or surfaces is a real challenge with many applications ranging from medical image analysis to computer graphics. Modeling such geometrical primitives with currents avoids to base the comparison between primitives either on a selection of geometrical measures (like length, area or curvature) or on the assumption of point-correspondence. This framework has been used relevantly to register brain surfaces or to measure geometrical invariants. However, while the state-of-the-art methods efficiently perform pairwise registrations, new numerical schemes are required to process groupwise statistics due to an increasing complexity when the size of the database is growing. In this paper, we propose a Matching Pursuit Algorithm for currents, which allows us to approximate, at any desired accuracy, the mean and modes of a population of geometrical primitives modeled as currents. This leads to a sparse representation of the currents, which offers a way to visualize, and hence to interpret, such statistics. More importantly, this tool allows us to build atlases from a population of currents, based on a rigorous statistical model. In this model, data are seen as deformations of an unknown template perturbed by random currents. A Maximum A Posteriori approach is used to estimate consistently the template, the deformations of this template to each data and the residual perturbations. Statistics on both the deformations and the residual currents provide a complete description of the geometrical variability of the structures. Eventually, this framework is generic and can be applied to a large range of anatomical data. We show the relevance of our approach by describing the variability of population of sulcal lines, surfaces of internal structures of the brain and white matter fiber bundles. Complementary experiments on simulated data show the potential of the method to give anatomical characterization of pathologies in the context of supervised learning.",Currents Curves Surfaces Statistics Matching pursuit algorithm Approximation Sparse decomposition Atlas estimation Template Registration Shape space Anatomical variability Group classification Computational anatomy,
168,High-speed visualization of time-varying data in large-scale structural dynamic analyses with a GPU,Automation in Construction,2014," Large-scale structural dynamic analyses generally produce massive amount of time-varying data. Inefficient rendering of these data seriously affects the quality of display of and user interaction with the analysis results. A high-speed visualization solution using a GPU (graphics processing unit) is thus developed in this study. Based on the clustering concept, a key frame extraction algorithm specific to the GPU-based rendering is proposed, which significantly reduces the data size to satisfy the GPU memory requirement. Using the key frames, a GPU-based parallel frame interpolation algorithm is also proposed to reconstruct the complete structural dynamic process. Particularly, a novel data access model considering the features of time-varying data and GPU memory is designed to improve the interpolation efficiency. Two case studies including an arch bridge and a high-rise building are presented, confirming the ability of the proposed solution to provide a high-speed and interactive visualization environment for large-scale structural dynamic analyses.",High-speed visualization Time-varying data Large-scale structural dynamic analyses Key frame extraction GPU Frame interpolation,
169,Surface Function Actives,Journal of Visual Communication and Image Representation,2009,"Deformable models have been widely used in image segmentation since the introduction of the snakes. Later the introduction of level set frameworks to solve the energy minimization problem associated with the deformable model overcame some limitations of the parametric active contours with respect to topological changes by embedding surface representations into higher dimensional functions. However, this may also bring in more computational load so that recent advances in spatio-temporal resolutions of 3D/4D imaging raised some challenges for real-time segmentation, especially for interventional imaging. In this context, a novel segmentation framework, Surface Function Actives (SFA), is proposed for real-time segmentation purpose. SFA has great advantages in terms of potential efficiency, based on its dimensionality reduction for the surface representation. Utilizing implicit representations with variational framework also provides flexibility and benefits currently shared by level set frameworks. An application for minimally-invasive intervention is shown to illustrate the potential applications of this framework.",Surface Function Actives Image segmentation Deformable model Real-time segmentation Variational approach Interface representation,
170,On the efficiency of iterative ordered subset reconstruction algorithms for acceleration on GPUs,Computer Methods and Programs in Biomedicine,2010,"Expectation Maximization (EM) and the Simultaneous Iterative Reconstruction Technique (SIRT) are two iterative computed tomography reconstruction algorithms often used when the data contain a high amount of statistical noise, have been acquired from a limited angular range, or have a limited number of views. A popular mechanism to increase the rate of convergence of these types of algorithms has been to perform the correctional updates within subsets of the projection data. This has given rise to the method of Ordered Subsets EM (OS-EM) and the Simultaneous Algebraic Reconstruction Technique (SART). Commodity graphics hardware (GPUs) has shown great promise to combat the high computational demands incurred by iterative reconstruction algorithms. However, we find that the special architecture and programming model of GPUs add extra constraints on the real-time performance of ordered subsets algorithms, counteracting the speedup benefits of smaller subsets observed on CPUs. This gives rise to new relationships governing the optimal number of subsets as well as relaxation factor settings for obtaining the smallest wall-clock time for reconstruction—a factor that is likely application-dependent. In this paper we study the generalization of SIRT into Ordered Subsets SIRT and show that this allows one to optimize the computational performance of GPU-accelerated iterative algebraic reconstruction methods.",Iterative reconstruction Computed tomography Commodity graphics hardware GPU,
171,A study on performance of dynamic file replication algorithms for real-time file access in Data Grids,Future Generation Computer Systems,2009,"Real-time Grid applications are emerging in many disciplines of science and engineering. In order to run these applications while meeting the associated real-time constraints with them, the Grid infrastructure should be designed to respect these constraints and allocate its computing, networking, storage, and the other resources accordingly. Furthermore, these applications involve a large number of data intensive jobs and require to access terabytes of data in real-time. On the other hand, a variety of dynamic file replication algorithms were proposed for the best-effort Data Grid environments in an attempt to decrease job completion times and save network bandwidth. Until now, there is no study in the literature which tries to elaborate on the real-time performance of these dynamic file replication algorithms. Based on this motivation, in this study, the performance of eight dynamic replication algorithms are evaluated under various Data Grid settings. For this evaluation, a process oriented and discrete-event driven simulator called DGridSim is developed. A detailed set of simulation studies are conducted using DGridSim and the results obtained are presented to reveal the real-time performance of the dynamic file replication algorithms.",Data grid Dynamic file replication Real-time Discrete-event simulation,
172,Fast implementation of iterative reconstruction with exact ray-driven projector on GPUs,Tsinghua Science and Technology,2010,"Iterative methods are popular choices in image reconstruction fields due to their capability of recovering object information from incomplete acquisition data. However, the computation process involves frequent uses of forward and backward projections that are computationally expensive. Past research has proved that a forward projector that can produce high quality images is crucial to achieve a good convergence rate. In this paper a high performance iterative reconstruction framework is introduced, where two most popular iterative algorithms: Simultaneous Algebraic Reconstruction Technique (SART) and Ordered-subsets Expectation Maximization (OSEM) are supported. The framework utilizes Siddon's ray-driven method to generate forward projected images. Benefited from functionalities offered by current generation of graphics processing units (GPUs), it achieves better performance when compared to previous GPU implementations that use grid-interpolated methods, on top of the significant speedups over CPU-based solutions.",Graphics processing unit Image quality Image reconstruction Interpolation Iterative methods Phantoms Tomography graphics processing units image reconstruction tomography,
173,Shear modulus estimation using parallelized partial volumetric reconstruction,"Medical Imaging, IEEE Transactions on",2004,"Magnetic resonance elastography can be limited by the computationally intensive nonlinear inversion schemes that are sometimes employed to estimate shear modulus from externally induced internal tissue displacements. Consequently, we have developed a parallelized partial volume reconstruction approach to overcome this limitation. In this paper, we report results from experiments conducted on breast phantoms and human volunteers to validate the proposed technique. More specifically, we demonstrate that computational cost is linearly related to the number of subzones used during image recovery and that both subzone parallelization and partial volume domain reduction decrease execution time accordingly. Importantly, elastograms computed based on the parallelized partial volume technique are not degraded in terms of either image quality or accuracy relative to their full volume counterparts provided that the estimation domain is sufficiently large to negate the effects of boundary conditions. The clinical results presented in this paper are clearly preliminary; however, the parallelized partial volume reconstruction approach performs sufficiently well to warrant more in-depth clinical evaluation.",biological organs biological tissues biomechanics biomedical MRI image reconstruction medical image processing shear modulus breast phantoms computationally intensive nonlinear inversion schemes externally induced internal tissue displacements image recovery magnetic resonance elastography parallelized partial volumetric reconstruction shear modulus subzone parallelization Boundary conditions Breast Computational efficiency Concurrent computing Degradation Humans Image quality Image reconstruction Imaging phantoms Magnetic resonance Breast cancer detection elasticity imaging elastography inverse elasticity problem magnetic resonance imaging parallel computing shear modulus estimation Breast Neoplasms Computing Methodologies Elasticity Humans Image Interpretation Computer-Assisted Imaging Three-Dimensional Magnetic Resonance Imaging Phantoms Imaging Reproducibility of Results Sensitivity and Specificity Shear Strength,
174,Multi-scale computational model of three-dimensional hemodynamics within a deformable full-body arterial network,Journal of Computational Physics,2013,"In this article, we present a computational multi-scale model of fully three-dimensional and unsteady hemodynamics within the primary large arteries in the human. Computed tomography image data from two different patients were used to reconstruct a nearly complete network of the major arteries from head to foot. A linearized coupled-momentum method for fluid–structure-interaction was used to describe vessel wall deformability and a multi-domain method for outflow boundary condition specification was used to account for the distal circulation. We demonstrated that physiologically realistic results can be obtained from the model by comparing simulated quantities such as regional blood flow, pressure and flow waveforms, and pulse wave velocities to known values in the literature. We also simulated the impact of age-related arterial stiffening on wave propagation phenomena by progressively increasing the stiffness of the central arteries and found that the predicted effects on pressure amplification and pulse wave velocity are in agreement with findings in the clinical literature. This work demonstrates the feasibility of three-dimensional techniques for simulating hemodynamics in a full-body compliant arterial network.",Hemodynamics Wave propagation Pulse wave velocity Fluid–structure interaction Multiscale modeling Hypertension Arterial stiffening Full-body arterial model,
175,Incremental Hough transform: an improved algorithm for digital device implementation,Real-Time Imaging,2004,"Hough Transform (HT) is one of the most common methods for detecting shapes (lines, circles, etc.) in binary or edge images. Its advantage is its ability to detect discontinuous patterns in noisy images, but it requires a large amount of computing power. This paper presents a fast and enhanced version for computing the Incremental HT that was developed for digital device implementation. This algorithm does not require any Look Up Table or trigonometric calculations at run time. A number of the ρ values generated in parallel is defined at the beginning of this algorithm. This algorithm leads to a significant reduction of the HT computation time and can be therefore used in real-time applications. This paper also provides some results obtained by this algorithm on several images.",,
176,A strain energy filter for 3D vessel enhancement with application to pulmonary CT images,Medical Image Analysis,2011,"The traditional Hessian-related vessel filters often suffer from detecting complex structures like bifurcations due to an over-simplified cylindrical model. To solve this problem, we present a shape-tuned strain energy density function to measure vessel likelihood in 3D medical images. This method is initially inspired by established stress–strain principles in mechanics. By considering the Hessian matrix as a stress tensor, the three invariants from orthogonal tensor decomposition are used independently or combined to formulate distinctive functions for vascular shape discrimination, brightness contrast and structure strength measuring. Moreover, a mathematical description of Hessian eigenvalues for general vessel shapes is obtained, based on an intensity continuity assumption, and a relative Hessian strength term is presented to ensure the dominance of second-order derivatives as well as suppress undesired step-edges. Finally, we adopt the multi-scale scheme to find an optimal solution through scale space. The proposed method is validated in experiments with a digital phantom and non-contrast-enhanced pulmonary CT data. It is shown that our model performed more effectively in enhancing vessel bifurcations and preserving details, compared to three existing filters.",Vessel enhancement Strain energy density Shape discrimination Pulmonary image analysis,
177,A national computing initiative: a summary,Future Generation Computer Systems,1989,,,
178,Using UMLS metathesaurus concepts to describe medical images: dermatology vocabulary,Computers in Biology and Medicine,2006,"Web servers at the National Library of Medicine (NLM) displayed images of ten skin lesions to practicing dermatologists and provided an online form for capturing text they used to describe the pictures. The terms were submitted to the UMLS Metathesaurus (Meta). Concepts retrieved, their semantic types, definitions and synonyms, were returned to each subject in a second web-based form. Subjects rated the concepts against their own descriptive terms. They submitted 825 terms, 346 of which were unique and 300 mapped to UMLS concepts. The dermatologists rated 295 concepts as ‘Exact Match’ and they accomplished both tasks in about 30 min.",Vocabulary Dermatology UMLS Metathesaurus Indexing WWW Internet Semantics Concept name,
179,Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment,Current cardiology reports,2014,"Although advances in information technology in the past decade have come in quantum leaps in nearly every aspect of our lives, they seem to be coming at a slower pace in the field of medicine. However, the implementation of electronic health records (EHR) in hospitals is increasing rapidly, accelerated by the meaningful use initiatives associated with the Center for Medicare & Medicaid Services EHR Incentive Programs. The transition to electronic medical records and availability of patient data has been associated with increases in the volume and complexity of patient information, as well as an increase in medical alerts, with resulting alert fatigue"" and increased expectations for rapid and accurate diagnosis and treatment. Unfortunately", artificial intelligence (AI)/machine learning will likely assist physicians with differential diagnosis of disease,
180,A novel statistical cerebrovascular segmentation algorithm with particle swarm optimization,Neurocomputing,2015," We present an automatic statistical intensity-based approach to  the 3D cerebrovascular structure from time-of flight (TOF) magnetic resonance angiography (MRA) data. We use the finite mixture model (FMM) to fit the intensity histogram of the brain image sequence, where the cerebral vascular structure is modeled by a Gaussian distribution function and the other low intensity tissues are modeled by Gaussian and Rayleigh distribution functions. To estimate the parameters of the FMM, we propose an improved particle swarm optimization (PSO) algorithm, which has a disturbing term in speeding updating the formula of PSO to ensure its convergence. We also use the ring shape topology of the particles neighborhood to improve the performance of the algorithm. Computational results on 34 test data show that the proposed method provides accurate segmentation, especially for those blood vessels of small sizes.",Cerebrovascular segmentation Finite mixture model Particle swarm optimization Intensity histogram Rayleigh distribution,
181,Ensemble based system for whole-slide prostate cancer probability mapping using color texture features,Computerized Medical Imaging and Graphics,2011,"We present a tile-based approach for producing clinically relevant probability maps of prostatic carcinoma in histological sections from radical prostatectomy. Our methodology incorporates ensemble learning for feature selection and classification on expert-annotated images. Random forest feature selection performed over varying training sets provides a subset of generalized CIEL*a*b* co-occurrence texture features, while sample selection strategies with minimal constraints reduce training data requirements to achieve reliable results. Ensembles of classifiers are built using expert-annotated tiles from training images, and scores for the probability of cancer presence are calculated from the responses of each classifier in the ensemble. Spatial filtering of tile-based texture features prior to classification results in increased heat-map coherence as well as AUC values of 95% using ensembles of either random forests or support vector machines. Our approach is designed for adaptation to different imaging modalities, image features, and histological decision domains.",Ensemble classification Prostate cancer detection Random forest feature selection Histology Digital pathology,
182,Efficient image segmentation using partial differential equations and morphology,Pattern Recognition,2001,"The goal of this paper is to investigate segmentation methods that combine fast preprocessing algorithms using partial differential equations (PDEs) with a watershed transformation with region merging. We consider two well-founded PDE methods: a nonlinear isotropic diffusion filter that permits edge enhancement, and a convex nonquadratic variational image restoration method which gives good denoising. For the diffusion filter, an efficient algorithm is applied using an additive operator splitting (AOS) that leads to recursive and separable filters. For the variational restoration method, a novel algorithm is developed that uses AOS schemes within a Gaussian pyramid decomposition. Examples demonstrate that preprocessing by these PDE techniques significantly improves the watershed segmentation, and that the resulting segmentation method gives better results than some traditional techniques. The algorithm has linear complexity and it can be used for arbitrary dimensional data sets. The typical CPU time for segmenting a 2562 image on a modern PC is far below 1 s.",Nonlinear diffusion Variational methods Image restoration Additive operator splitting Gaussian pyramid Watershed segmentation Region merging,
183,Adaptive Frequency Weighting for High-Performance Video Coding,"Circuits and Systems for Video Technology, IEEE Transactions on",2012,"The benefits of utilizing the spatial-frequency sensitivity of the human visual system in video coding have been known for decades and are incorporated into video standards, such as MPEG2, MPEG4, and H.264/AVC, as well as into still image-coding standards, such as JPEG and JPEG-2000. However, all of these standards utilize frequency weighting that is adaptive only at the picture level. In contrast with such picture-level adaptive frequency-weighting algorithms, we describe a coding approach in which a macroblock-level adaptive frequency-weighting (MBAFW) algorithm is used. We show that for the , quality levels, and resolutions typically associated with high-definition video, MBAFW can lead to overall  reductions typically in the 5%-9% range over the methods used in state-of-the-art video-coding standards.",adaptive signal processing video coding high definition video high performance video coding human visual system macroblock level adaptive frequency weighting algorithm quality level spatial-frequency sensitivity Image edge detection Inductors Quantization Sensitivity Standards Video coding Visualization Frequency weighting human visual system (HVS) perceptual coding quantization matrix,
184,A review on biomedical image processing and future trends,Computer Methods and Programs in Biomedicine,1990,"The last two decades have witnessed a revolutionary development in the field of biomedical and diagnostic imaging. Imaging procedures and modalities which were only in the experimental research phase in the early part of the last two decades, have now become universally accepted clinical procedures. They include computerized tomography (CT), magnetic resonance imaging, ultrasound imaging, nuclear medicine imaging, computerized hematological cell analysis, etc. In the past, the conventional and relatively simple image processing techniques such as image enhancement, gray-level mapping, spectral analysis, region extraction, etc. have been modified for biomedical images and successfully applied for processing and analysis. The role of image enhancement, gray-level mapping, and image reconstruction from projections algorithms in CT and other radiological imaging modalities is well evident. Recently, many advances in biomedical image processing, analysis, and understanding algorithms have shown a great potential for enhancing and interpreting useful diagnostic information from these images more accurately. This paper presents a review on the current state-of-the-art techniques in biomedical image processing and comments on future trends.",Image processing Biomedicine Diagnostic (Review),
185,Micro-genetic approach for surface meshing on a set of unorganized points,Computer Methods in Applied Mechanics and Engineering,2007,"Surface meshing plays a crucial role in mesh generation. Usually, surface meshing methods in three dimensions generate meshes relying on prescribed patch interpolation. In this study, an approach of surface meshing directly on a set of unorganized points is developed, which consists of a mesh triangulation and a conversion scheme for primary triangular and quadrilateral surface meshes, a high order C1 surface function reconstruction, and a micro-genetic algorithm (MGA) to smooth the meshes. Practical cases are given to demonstrate its successful performance and its versatility.",Surface meshing Unorganized points C1 surface function reconstruction Micro-genetic algorithm,
186,Building a reference multimedia database for interstitial lung diseases,Computerized Medical Imaging and Graphics,2012,"This paper describes the methodology used to create a multimedia collection of cases with interstitial lung diseases (ILDs) at the University Hospitals of Geneva. The dataset contains high-resolution computed tomography (HRCT) image series with three-dimensional annotated regions of pathological lung tissue along with clinical parameters from patients with pathologically proven diagnoses of ILDs. The motivations for this work is to palliate the lack of publicly available collections of ILD cases to serve as a basis for the development and evaluation of image-based computerized diagnostic aid. After 38 months of data collection, the library contains 128 patients affected with one of the 13 histological diagnoses of ILDs, 108 image series with more than 41 l of annotated lung tissue patterns as well as a comprehensive set of 99 clinical parameters related to ILDs. The database is available for research on request and after signature of a license agreement.",Interstitial lung diseases Multimedia database High-resolution computed tomography Computer-aided diagnosis Content-based image retrieval Case-based retrieval,
187,A wavelet-based regularized reconstruction algorithm for SENSE parallel MRI with applications to neuroimaging,Medical Image Analysis,2011,"To reduce scanning time and/or improve spatial/temporal resolution in some Magnetic Resonance Imaging (MRI) applications, parallel MRI acquisition techniques with multiple coils acquisition have emerged since the early 1990s as powerful imaging methods that allow a faster acquisition process. In these techniques, the full FOV image has to be reconstructed from the resulting acquired undersampled k-space data. To this end, several reconstruction techniques have been proposed such as the widely-used SENSitivity Encoding (SENSE) method. However, the reconstructed image generally presents artifacts when perturbations occur in both the measured data and the estimated coil sensitivity profiles. In this paper, we aim at achieving accurate image reconstruction under degraded experimental conditions (low magnetic field and high reduction factor), in which neither the SENSE method nor the Tikhonov regularization in the image domain give convincing results. To this end, we present a novel method for SENSE-based reconstruction which proceeds with regularization in the complex wavelet domain by promoting sparsity. The proposed approach relies on a fast algorithm that enables the minimization of regularized non-differentiable criteria including more general penalties than a classical ℓ1 term. To further enhance the reconstructed image quality, local convex constraints are added to the regularization process. In vivo human brain experiments carried out on Gradient-Echo (GRE) anatomical and Echo Planar Imaging (EPI) functional MRI data at 1.5 T indicate that our algorithm provides reconstructed images with reduced artifacts for high reduction factors.",Parallel MRI SENSE reconstruction Regularization Wavelet transform Convex optimization,
188,Realization of integration and working procedure on digital hospital information system,Computer Standards & Interfaces,2003,"The integrating method and working procedure of a digital hospital information system were discussed in this paper. It adopts a unique modularized structure that allows interplatform data exchange among different hospital information systems (HIS, RIS and PACS) through the seamless integration of the above-mentioned three systems according to the international standards (DICOM, HL7 and TC251). The realization of communication interface standardization, function modularisation, common sharing of medical information resources and adaptation to local circumstances enables the system function, management function, information processing and communication function to be achieved on a complete platform, which provides such advantages as common sharing, openness, security, extensibility and simple operation, and offers completeness to present hospital management and future medical environment. Currently, it is being successfully applied at many hospitals in China such as the 5th People Hospital of Zhengzhou City to realize the digitized, network-dependent and film-independent modern hospital management.",Digital Hospital Integration Information Sharing Modality Worklist Modularization,
189,GPU acceleration of nonlinear diffusion tensor estimation using CUDA and MPI,Neurocomputing,2014," Diffusion MRI is a non-invasive magnetic resonance technique and has been increasingly used in imaging neuroscience. It is currently the only method capable of depicting the complex structure of white matter of the brain in vivo. One of the most popular techniques is diffusion tensor imaging (DTI) which is commonly used clinically to produce in vivo images of biological tissues with local microstructural characteristics such as water diffusion. Diffusion tensor maps are usually computed on a voxel-by-voxel basis by fitting the signal intensities of diffusion weighted images as a function of their corresponding data acquisition parameters (b-matrices). This processing is computation-intensive and time-consuming which can constraint the clinical practice of DTI. This study presents the application of using high performance GPU clusters in addition to CPUs for diffusion tensor estimation by accelerating the multivariate non-linear regression. The results are tested using both simulated and clinical diffusion datasets and show significant performance gain in tensor fitting. The proposed GPU implementation framework can significantly reduce the processing time of DTI data especially for the datasets with high spatial and temporal resolution, and hence further promote the clinical use of DTI. It also can be used to accelerate statistical analysis of DTI where Monte Carlo simulations are employed, be readily applied to quantitative assessment of DTI using bootstrap analysis, robust diffusion tensor estimation and should be applicable to other MR imaging techniques that use univariate or multivariate regression to fit MRI data to a model.",Diffusion tensor imaging (DTI) Graphics processing unit (GPU) Nonlinear diffusion tensor estimation Levenberg–Marquardt algorithm High performance clusters Load balance,
190,GPU-friendly multi-view stereo reconstruction using surfel representation and graph cuts,Computer Vision and Image Understanding,2011,"In this paper, we present a new surfel (surface element) based multi-view stereo algorithm that runs entirely on GPU. We utilize the flexibility of surfel-based 3D shape representation and global optimization by graph cuts in the same framework. Unlike previous works, the algorithm is optimized to massive parallel processing on GPU. First, we construct surfel candidates by local stereo matching and voting. After refining the position and orientation of the surfel candidates, we  the optimal surfels by employing graph cuts under photo-consistency and surfel orientation constraints. In contrast to the conventional voxel based methods, the proposed algorithm utilizes more accurate photo-consistency and reconstructs the 3D shape up to sub-voxel accuracy. The orientation of the constructed surfel candidates imposes an effective constraint that reduces the effect of the minimal surface bias. The entire processing pipeline is implemented on the latest GPU to significantly speed up the processing. The experimental results show that the proposed approach reconstructs the 3D shape of an object accurately and efficiently, which runs more than 100 times faster than on CPU.",Multi-view stereo 3D shape reconstruction Surfel Graph cuts GPU Massive parallel processing,
191,A Theory of High-Order Statistics-Based Virtual Dimensionality for Hyperspectral Imagery,"Geoscience and Remote Sensing, IEEE Transactions on",2014,"Virtual dimensionality (VD) has received considerable interest in its use of specifying the number of spectrally distinct signatures present in hyperspectral data. Unfortunately, it never defines what such a signature is. For example, various targets of interest, such as anomalies and endmembers, should be considered as different types of spectrally distinct signatures and have their own different values of VD. Specifically, these targets are insignificant in terms of signal energies due to their relatively small populations. Accordingly, their contributions to second-order statistics (2OS) are rather limited. In this case, 2OS-based methods such as eigen-approaches to determine VD may not be effective in determining how many such type of signal sources as spectrally distinct signatures are. This paper develops a new theory that expands 2OS-VD theory to a high-order statistics (HOS)-based VD, called HOS-VD theory. Since there is no counterpart of the characteristic polynomial equation used to find eigenvalues in 2OS available for HOS, a direct extension is inapplicable. This paper re-invents a wheel by finding actual targets directly from the data rather than eigenvectors/singular vectors used in 2OS-VD theory which do not represent any real targets in the data. Consequently, comparing to 2OS-VD theory which can only be used to estimate the value of VD without finding real targets, the developed HOS-VD theory can accomplish both of tasks at the same time, i.e., determining the value of VD as well as finding actual targets directly from the data.",eigenvalues and eigenfunctions geophysical signal processing higher order statistics hyperspectral imaging polynomials remote sensing spectral analysis HOS based VD HOS-VD theory characteristic polynomial equation eigenvalues high order statistics hyperspectral data hyperspectral imagery spectrally distinct signatures virtual dimensionality Eigenvalues and eigenfunctions Equations Estimation Hybrid fiber coaxial cables Hyperspectral imaging Testing Vectors Harsanyi-Farrand-Chang (HFC) method Neyman-Pearson detection (NPD) high-order statistics (HOS) high-order statistics-based Harsanyi-Farrand-Chang (HOS-HFC) method high-order statistics-based VD (HOS-VD) theory maximum orthogonal complement algorithm (MOCA) second-order statistics-based VD (2OS) theory virtual dimensionality (VD),
192,2-phase GA-based image registration on parallel clusters,Future Generation Computer Systems,2001,"Genetic algorithms (GAs) are known to be robust for search and optimization problems. Image registration can take advantage of the robustness of GAs in finding best transformation between two images, of the same location with slightly different orientation, produced by moving spaceborne remote sensing instruments. In this paper, we present 2-phase sequential and coarse-grained parallel image registration algorithms using GAs as optimization mechanism. In its first phase, the algorithm finds a small set of good solutions using low-resolution versions of the images. Based on these candidate low-resolution solutions, the algorithm uses the full resolution image data to refine the final registration results in the second phase. Experimental results are presented and revealed that our algorithms yield very accurate registration results for LandSat Thematic Mapper images, and the parallel algorithm scales quite well on the Beowulf parallel cluster.",Genetic algorithm Image registration Parallel cluster,
193,3-D object segmentation using ant colonies,Pattern Recognition,2010,"3-D object segmentation is an important and challenging topic in computer vision that could be tackled with artificial life models. A Channeler Ant Model (CAM), based on the natural ant capabilities of dealing with 3-D environments through self-organization and emergent behaviours, is proposed. Ant colonies, defined in terms of moving, pheromone laying, reproduction, death and deviating behaviours rules, is able to segment artificially generated objects of different shape, intensity, background. The model depends on few parameters and provides an elegant solution for the segmentation of 3-D structures in noisy environments with unknown range of image intensities: even when there is a partial overlap between the intensity and noise range, it provides a complete segmentation with negligible contamination (i.e., fraction of segmented voxels that do not belong to the object). The CAM is already in use for the automated detection of nodules in lung Computed Tomographies.",Artificial life Ant colony Image processing 3-D object segmentation,
194,Component averaging: An efficient iterative parallel algorithm for large and sparse unstructured problems,Parallel Computing,2001,"Component averaging (CAV) is introduced as a new iterative parallel technique suitable for large and sparse unstructured systems of linear equations. It simultaneously projects the current iterate onto all the system's hyperplanes, and is thus inherently parallel. However, instead of orthogonal projections and scalar weights (as used, for example, in Cimmino's method), it uses oblique projections and diagonal weighting matrices, with weights related to the sparsity of the system matrix. These features provide for a practical convergence rate which approaches that of algebraic reconstruction technique (ART) (Kaczmarz's row-action algorithm) – even on a single processor. Furthermore, the new algorithm also converges in the inconsistent case. A proof of convergence is provided for unit relaxation, and the fast convergence is demonstrated on image reconstruction problems of the Herman head phantom obtained within the SNARK93 image reconstruction software package. Both reconstructed images and convergence plots are presented. The practical consequences of the new technique are far reaching for real-world problems in which iterative algorithms are used for solving large, sparse, unstructured and often inconsistent systems of linear equations.",Component averaging Iterative techniques Oblique projections Sparse systems Sparsity-related weights,
195,Fast ray-tracing of human eye optics on Graphics Processing Units,Computer Methods and Programs in Biomedicine,2014," We present a new technique for simulating retinal image formation by tracing a large number of rays from objects in three dimensions as they pass through the optic apparatus of the eye to objects. Simulating human optics is useful for understanding basic questions of vision science and for studying vision defects and their corrections. Because of the complexity of computing such simulations accurately, most previous efforts used simplified analytical models of the normal eye. This makes them less effective in modeling vision disorders associated with abnormal shapes of the ocular structures which are hard to be precisely represented by analytical surfaces. We have developed a computer simulator that can simulate ocular structures of arbitrary shapes, for instance represented by polygon meshes. Topographic and geometric measurements of the cornea, lens, and retina from keratometer or medical imaging data can be integrated for individualized examination. We utilize parallel processing using modern Graphics Processing Units (GPUs) to efficiently compute retinal images by tracing millions of rays. A stable retinal image can be generated within minutes. We simulated depth-of-field, accommodation, chromatic aberrations, as well as astigmatism and correction. We also show application of the technique in patient specific vision correction by incorporating geometric models of the orbit reconstructed from clinical medical images.",Human eye optics Ray tracing Computational simulation Patient specific modeling and simulation Vision defects GPU programming,
196,A study on scale factor in distributed differential evolution,Information Sciences,2011,"This paper proposes the employment of multiple scale factor values within distributed differential evolution structures. Four different scale factor schemes are proposed, tested, compared and analyzed. Two schemes simply employ multiple scale factor values and two also include an update logic during the evolution. The four schemes have been integrated for comparison within three recently proposed distributed differential evolution structures and tested on several various test problems. Numerical results show that, on average, the employment of multiple scale factors is beneficial since in most cases it leads to significant improvements in performance with respect to standard distributed algorithms. Although proper choice of a scale factor scheme appears to be dependent on the distributed structure, any of the proposed simple schemes has proven to significantly improve upon the single scale factor distributed differential evolution algorithms.",Differential evolution Evolutionary algorithms Distributed algorithms Scale factor Structured populations Optimization algorithms Computational intelligence optimization,
197,A high performance computing approach to the registration of medical imaging data,Parallel Computing,1998,"A novel automatic registration algorithm for the alignment of medical imaging data was developed. The algorithm measures alignment by comparison of dense feature sets (tissue labels) and optimum alignment is found by minimizing the mismatch of tissue segmentations. A parallel implementation that distributes resampling and comparison operations across a cluster of symmetric multiprocessors achieves execution times in a clinically compatible range (5–10 min). Each node executes a parallelized resample and compare operation implemented with POSIX threads, and work is dynamically load balanced across the cluster with communication implemented with MPI. The quality of the registration algorithm and the performance characteristics of the parallel implementation were investigated for typical registration problems. The algorithm has been used to successfully achieve intrapatient and interpatient registration of tissue segmentations without any manual intervention for over three hundred scans of the brain.",Medical Interpatient Registration Cluster Symmetric multiprocessor,
198,Capturing intraoperative deformations: research experience at Brigham and Women’s hospital,Medical Image Analysis,2005,"During neurosurgical procedures the objective of the neurosurgeon is to achieve the resection of as much diseased tissue as possible while achieving the preservation of healthy brain tissue. The restricted capacity of the conventional operating room to enable the surgeon to visualize critical healthy brain structures and tumor margin has lead, over the past decade, to the development of sophisticated intraoperative imaging techniques to enhance visualization. However, both rigid motion due to patient placement and nonrigid deformations occurring as a consequence of the surgical intervention disrupt the correspondence between preoperative data used to plan surgery and the intraoperative configuration of the patient’s brain. Similar challenges are faced in other interventional therapies, such as in cryoablation of the liver, or biopsy of the prostate. We have developed algorithms to model the motion of key anatomical structures and system implementations that enable us to estimate the deformation of the critical anatomy from sequences of volumetric images and to prepare updated fused visualizations of preoperative and intraoperative images at a rate compatible with surgical decision making. This paper reviews the experience at Brigham and Women’s Hospital through the process of developing and applying novel algorithms for capturing intraoperative deformations in support of image guided therapy.",,
199,"Adaptive, template moderated, spatially varying statistical classification",Medical Image Analysis,2000,"A novel image segmentation algorithm was developed to allow the automatic segmentation of both normal and abnormal anatomy from medical images. The new algorithm is a form of spatially varying statistical classification, in which an explicit anatomical template is used to moderate the segmentation obtained by statistical classification. The algorithm consists of an iterated sequence of spatially varying classification and nonlinear registration, which forms an adaptive, template moderated (ATM), spatially varying statistical classification (SVC). Classification methods and nonlinear registration methods are often complementary, both in the tasks where they succeed and in the tasks where they fail. By integrating these approaches the new algorithm avoids many of the disadvantages of each approach alone while exploiting the combination. The ATM SVC algorithm was applied to several segmentation problems, involving different image contrast mechanisms and different locations in the body. Segmentation and validation experiments were carried out for problems involving the quantification of normal anatomy (MRI of brains of neonates) and pathology of various types (MRI of patients with multiple sclerosis, MRI of patients with brain tumors, MRI of patients with damaged knee cartilage). In each case, the ATM SVC algorithm provided a better segmentation than statistical classification or elastic matching alone.",Segmentation Elastic matching Nonlinear registration Nearest neighbor classification Multiple sclerosis Knee cartilage Neonate Brain Tumor,
200,A high performance computing approach to the registration of medical imaging data,Parallel Computing,1998,"A novel automatic registration algorithm for the alignment of medical imaging data was developed. The algorithm measures alignment by comparison of dense feature sets (tissue labels) and optimum alignment is found by minimizing the mismatch of tissue segmentations. A parallel implementation that distributes resampling and comparison operations across a cluster of symmetric multiprocessors achieves execution times in a clinically compatible range (5-10 min). Each node executes a parallelized resample and compare operation implemented with POSIX threads, and work is dynamically load balanced across the cluster with communication implemented with MPI. The quality of the registration algorithm and the performance characteristics of the parallel implementation were investigated for typical registration problems. The algorithm has been used to successfully achieve intrapatient and interpatient registration of tissue segmentations without any manual intervention for over three hundred scans of the brain. (C) 1998 Elsevier Science B.V. All rights reserved.",,
201,"Digital image correlation in experimental mechanics and image registration in computer vision: Similarities, differences and complements",Optics and Lasers in Engineering,2015," Digital image correlation and image registration or matching are among the most widely used techniques in the fields of experimental mechanics and computer vision, respectively. Despite their applications in separate fields, both techniques primarily involve detecting the same physical points in two or more images. In this paper, a brief technical comparison of the two techniques is reviewed, and their similarities and differences as well as complements are presented. It is shown that some concepts from the image registration or matching technique can be applied to the digital image correlation technique to substantially enhance its performance, which can help broaden the applications of digital image correlation in scientific research and engineering practice.",Digital image correlation Image registration Image matching Experimental mechanics Computer vision,
202,Sparse representation-based MRI super-resolution reconstruction,Measurement,2014," Magnetic Resonance Imaging (MRI) data collection is influenced by SNR, hardware, image time, and other factors. The super-resolution analysis is a critical way to improve the imaging quality. This work presents a framework of super-resolution MRI via sparse reconstruction, and this method is promising to solve the data collection limitations. A novel dictionary training method for sparse reconstruction for enhancing the similarity of sparse representations between the low resolution and high resolution MRI block pairs through simultaneous training two dictionaries. Low resolution MRI blocks generate the high resolution MRI blocks with proposed sparse representation (SR) coefficients. Comprehensive evaluations are implemented to test the feasibility and performance of the SR–MRI method on the real database.",Super-resolution Sparse representation analysis Magnetic resonance imaging,
203,Real-time stereographic rendering and display of medical images with programmable GPUs,Computerized Medical Imaging and Graphics,2008,The study was to explore the power and feasibility of using programmable graphics processing units (GPUs) for real-time rendering and displaying large 3D medical datasets for stereoscopic display workstation. Lung cancer screening CT images were used for developing GPU-based stereo rendering and displaying. The study was run on a personal computer with a 128 MB NVIDIA Quadro FX 1100 graphics card. The performance of rendering and displaying was measured and compared between GPU-based and central processing unit (CPU)-based programming. The results indicate that GPU-based programming was capable of rendering large 3D datasets at real-time interactive rates with stereographic displays.,Programmable graphics processing units Stereoscopic Display Real time Image rendering,
204,An image score inference system for RNAi genome-wide screening based on fuzzy mixture regression modeling,Journal of Biomedical Informatics,2009,"With recent advances in fluorescence microscopy imaging techniques and methods of gene knock down by RNA interference (RNAi), genome-scale high-content screening (HCS) has emerged as a powerful approach to systematically identify all parts of complex biological processes. However, a critical barrier preventing fulfillment of the success is the lack of efficient and robust methods for automating RNAi image analysis and quantitative evaluation of the gene knock down effects on huge volume of HCS data. Facing such opportunities and challenges, we have started investigation of automatic methods towards the development of a fully automatic RNAi–HCS system. Particularly important are reliable approaches to cellular phenotype classification and image-based gene function estimation. We have developed a HCS analysis platform that consists of two main components: fluorescence image analysis and image scoring. For image analysis, we used a two-step enhanced watershed method to  cellular boundaries from HCS images. Segmented cells were classified into several predefined phenotypes based on morphological and appearance features. Using statistical characteristics of the identified phenotypes as a quantitative description of the image, a score is generated that reflects gene function. Our scoring model integrates fuzzy gene class estimation and single regression models. The final functional score of an image was derived using the weighted combination of the inference from several support vector-based regression models. We validated our phenotype classification method and scoring system on our cellular phenotype and gene database with expert ground truth labeling. We built a database of high-content, 3-channel, fluorescence microscopy images of Drosophila Kc167 cultured cells that were treated with RNAi to perturb gene function. The proposed informatics system for microscopy image analysis is tested on this database. Both of the two main components, automated phenotype classification and image scoring system, were evaluated. The robustness and efficiency of our system were validated in quantitatively predicting the biological relevance of genes.",High-content screening Image score inference,
205,"GIST: an interactive, GPU-based level set segmentation tool for 3D medical images",Medical Image Analysis,2004,"While level sets have demonstrated a great potential for 3D medical image segmentation, their usefulness has been limited by two problems. First, 3D level sets are relatively slow to compute. Second, their formulation usually entails several free parameters which can be very difficult to correctly tune for specific applications. The second problem is compounded by the first. This paper describes a new tool for 3D segmentation that addresses these problems by computing level-set surface models at interactive rates. This tool employs two important, novel technologies. First is the mapping of a 3D level-set solver onto a commodity graphics card (GPU). This mapping relies on a novel mechanism for GPU memory management. The interactive rates level-set PDE solver give the user immediate feedback on the parameter settings, and thus users can tune free parameters and control the shape of the model in real time. The second technology is the use of intensity-based speed functions, which allow a user to quickly and intuitively specify the behavior of the deformable model. We have found that the combination of these interactive tools enables users to produce good, reliable segmentations. To support this observation, this paper presents qualitative results from several different datasets as well as a quantitative evaluation from a study of brain tumor segmentations.",,
206,The virtual microscope,"Information Technology in Biomedicine, IEEE Transactions on",2003,"We present the design and implementation of the virtual microscope, a software system employing a client/server architecture to provide a realistic emulation of a high power light microscope. The system provides a form of completely digital telepathology, allowing simultaneous access to archived digital slide images by multiple clients. The main problem the system targets is storing and processing the extremely large quantities of data required to represent a collection of slides. The virtual microscope client software runs on the end user's PC or workstation, while database software for storing, retrieving and processing the microscope image data runs on a parallel computer or on a set of workstations at one or more potentially remote sites. We have designed and implemented two versions of the data server software. One implementation is a customization of a database system framework that is optimized for a tightly coupled parallel machine with attached local disks. The second implementation is component-based, and has been designed to accommodate access to and processing of data in a distributed, heterogeneous environment. We also have developed caching client software, implemented in Java, to achieve good response time and portability across different computer platforms. The performance results presented show that the Virtual Microscope systems scales well, so that many clients can be adequately serviced by an appropriately configured data server.",PACS biomedical optical imaging diseases optical microscopy virtual reality Java client software client/server architecture computer platforms configured data server data server software database software digital slide images digital telepathology high power light microscope local disks microscope image data multiple clients software system virtual microscope design Computer architecture Concurrent computing Database systems Emulation Image databases Image retrieval Information retrieval Microscopy Software systems Workstations Computer Graphics Computer Simulation Database Management Systems Environment Equipment Design Equipment Failure Analysis Image Enhancement Image Interpretation Computer-Assisted Information Storage and Retrieval Microscopy Software Software Design Systems Integration Telepathology User-Computer Interface,
207,SIGMCC: A system for sharing meta patient records in a Peer-to-Peer environment,Future Generation Computer Systems,2008,"This paper considers the interoperability and information sharing between health care providers. It proposes a distributed Peer-to-Peer (P2P) based framework that enables health operators of different hospitals to share and aggregate clinical information about patients and therapy effects. Patient records are mapped into a simple XML-based meta-Electronic Patient Record (meta-EPR). The meta-EPR is not a standard EPR proposal, but it is a lightweight data structure defined to contain relevant and aggregate information extracted from the different EPRs adopted by each hospital. Hospital operators formulate queries against meta-EPR schema; queries are then distributed to the connected hospitals hosting meta-EPR instances, through a P2P infrastructure. The presented framework has been fully implemented in a system called SIGMCC, which offers an Application Programming Interface (API) for query formulation, data loading and updating. As a case study, an application of the proposed meta-EPR to the cancer medical domain has been developed. Finally, SIGMCC implements a view mechanism to allow personal (patient) information protection against unauthorized users.",EPR XML Peer-to-Peer Distributed database,
208,Parallel data intensive computing in scientific and commercial applications,Parallel Computing,2002,"Applications that explore, query, analyze, visualize, and, in general, process very large scale data sets are known as Data Intensive Applications. Large scale data intensive computing plays an increasingly important role in many scientific activities and commercial applications, whether it involves data mining of commercial transactions, experimental data analysis and visualization, or intensive simulation such as climate modeling. By combining high performance computation, very large data storage, high bandwidth access, and high-speed local and wide area networking, data intensive computing enhances the technical capabilities and usefulness of most systems. The integration of parallel and distributed computational environments will produce major improvements in performance for both computing intensive and data intensive applications in the future. The purpose of this introductory article is to provide an overview of the main issues in parallel data intensive computing in scientific and commercial applications and to encourage the reader to go into the more in-depth articles later in this special issue.",Data intensive algorithms Parallel computing Parallel databases Parallel I/O,
209,Monte Carlo simulation on heterogeneous distributed systems: A computing framework with parallel merging and checkpointing strategies,Future Generation Computer Systems,2013,"This paper introduces an end-to-end framework for efficient computing and merging of Monte Carlo simulations on heterogeneous distributed systems. Simulations are parallelized using a dynamic load-balancing approach and multiple parallel mergers. Checkpointing is used to improve reliability and to enable incremental results merging from partial results. A model is proposed to analyze the behavior of the proposed framework and help tune its parameters. Experimental results obtained on a production grid infrastructure show that the model fits the real makespan with a relative error of maximum 10%, that using multiple parallel mergers reduces the makespan by 40% on average, that checkpointing enables the completion of very long simulations and that it can be used without penalizing the makespan.",Grid computing Monte Carlo Merge Checkpointing Workflow Dynamic parallelization,
210,Direct Volume Rendering: A 3D Plotting Technique for Scientific Data,Computing in Science Engineering,2008,"The use of plotting techniques to comprehend scalar functions is ubiquitous in science and engineering. An effective plot uses features such as first and second derivatives to convey critical and inflection points, which help portray the overall behavior of functions around a region of interest. Direct volume rendering is an effective method for plotting 3D scientific data, but it's not used as frequently as it could be. In this article, the authors summarize direct volume rendering and discuss barriers to taking advantage of this powerful technique.",data visualisation natural sciences computing pattern classification rendering (computer graphics) 3D scientific data plotting technique classification data visualization direct volume rendering Biomedical optical imaging Computed tomography Data visualization Equations Hardware Level set Optical scattering Packaging Power engineering and energy Stimulated emission 3D plots scientific visualizations vistrails visualization corner volume rendering,
211,Dynamic meshing for deformable image registration,Computer-Aided Design,2015," Finite element method (FEM) is commonly used for deformable image registration. However, there is no existing literature studying how the superimposed mesh structure would influence the image registration process. We study this problem in this paper, and propose a dynamic meshing strategy to generate mesh structure for image registration. To construct such a dynamic mesh during image registration, three steps are performed. Firstly, a density field that measures the importance of a pixel/voxel’s displacement to the registration process is computed. Secondly, an efficient contraction–optimization scheme is applied to compute a discrete Centroidal Voronoi Tessellation of the density field. Thirdly, the final mesh structure is constructed by its dual triangulation, with some post-processing to preserve the image boundary. In each iteration of the deformable image registration, the mesh structure is efficiently updated with GPU-based parallel implementation. We conduct experiments of the new dynamic mesh-guided registration framework on both synthetic and real medical images, and compare our results with the other state-of-the-art FEM-based image registration methods.",Deformable image registration Centroidal Voronoi Tessellation (CVT) Dynamic meshing GPU,
212,CodeCloud: A platform to enable execution of programming models on the Clouds,Journal of Systems and Software,2014," This paper presents a platform that supports the execution of scientific applications covering different programming models (such as Master/Slave, Parallel/MPI, MapReduce and Workflows) on Cloud infrastructures. The platform includes (i) a high-level declarative language to express the requirements of the applications featuring software customization at runtime, (ii) an approach based on virtual containers to encapsulate the logic of the different programming models, (iii) an infrastructure manager to interact with different IaaS backends, (iv) a configuration software to dynamically configure the provisioned resources and (v) a catalog and repository of virtual machine images. By using this platform, an application developer can adapt, deploy and execute parallel applications agnostic to the Cloud backend.",Cloud computing Virtual infrastructures Elasticity,
213,Evolution of grid-based services for Diffusion Tensor Image analysis,Future Generation Computer Systems,2012,"Analyzing Diffusion Tensor Image data of the human brain of large study groups is complex and demands new, sophisticated and computationally intensive pipelines that can efficiently be executed. We present our progress over the past five years in the development and porting of the DTI analysis pipeline to a grid infrastructure. Starting with simple jobs submitted from the command-line, we moved towards a workflow-based implementation and finally into the e-BioInfra Gateway, which offers a web interface for the execution of selected biomedical data analysis software on the Dutch Grid. This gateway is currently being actively used by neuroscientists and for educational purposes.",Diffusion Tensor Imaging Grid computing e-science Workflow e-infrastructure Grid front-end Medical imaging Scientific gateway,
214,From error probability to information theoretic (multi-modal) signal processing,Signal Processing,2005,"We propose an information theoretic model that unifies a wide range of existing information theoretic signal processing algorithms in a compact mathematical framework. It is mainly based on stochastic processes, Markov chains and error probabilities. The proposed framework will allow us to discuss revealing analogies and differences between several well-known algorithms and to propose interesting extensions resulting directly from our formalism. We will then describe how the theory can be applied to the rapidly emerging field of multi-modal signal processing: we will show how our framework can be efficiently used for multi-modal medical image processing and for joint analysis of multi-media sequences (audio and video).",Stochastic process Information theory Markov chain Error probability Multi-modal Multi-media Medical images,
215,ProClusEnsem: Predicting membrane protein types by fusing different modes of pseudo amino acid composition,Computers in Biology and Medicine,2012,"Knowing the type of an uncharacterized membrane protein often provides a useful clue in both basic research and drug discovery. With the explosion of protein sequences generated in the post genomic era, determination of membrane protein types by experimental methods is expensive and time consuming. It therefore becomes important to develop an automated method to find the possible types of membrane proteins. In view of this, various computational membrane protein prediction methods have been proposed. They  protein feature vectors, such as PseAAC (pseudo amino acid composition) and PsePSSM (pseudo position-specific scoring matrix) for representation of protein sequence, and then learn a distance metric for the KNN (K nearest neighbor) or NN (nearest neighbor) classifier to predicate the final type. Most of the metrics are learned using linear dimensionality reduction algorithms like Principle Components Analysis (PCA) and Linear Discriminant Analysis (LDA). Such metrics are common to all the proteins in the dataset. In fact, they assume that the proteins lie on a uniform distribution, which can be captured by the linear dimensionality reduction algorithm. We doubt this assumption, and learn local metrics which are optimized for local subset of the whole proteins. The learning procedure is iterated with the protein clustering. Then a novel ensemble distance metric is given by combining the local metrics through Tikhonov regularization. The experimental results on a benchmark dataset demonstrate the feasibility and effectiveness of the proposed algorithm named ProClusEnsem.",Membrane protein types prediction Pseudo amino acid composition Linear dimensionality reduction Distance metric Clustering Tikhonov regularization,
216,On the Convergence of Generalized Simultaneous Iterative Reconstruction Algorithms,"Image Processing, IEEE Transactions on",2007,"In this paper, we generalize the widely used simultaneous block iterative reconstruction algorithm and show that it converges, at a linear rate, to a weighted least-squares and weighted minimum-norm reconstruction. Our theoretical result provides a much simpler proof of the convergence properties obtained by Jiang and Wang and covers a much more general class of algorithms. The frequency domain iterative reconstruction algorithm is then introduced as a special application of our theory",image reconstruction iterative methods least squares approximations medical image processing biomedical imaging convergence properties frequency domain iterative algorithm iterative reconstruction algorithms weighted least-squares weighted minimum-norm reconstruction Computed tomography Convergence Frequency domain analysis Image reconstruction Iterative algorithms Iterative methods Partitioning algorithms Reconstruction algorithms Size measurement Subspace constraints Biomedical imaging image reconstruction iterative methods signal reconstruction tomography Algorithms Image Enhancement Image Interpretation Computer-Assisted Imaging Three-Dimensional Information Storage and Retrieval Reproducibility of Results Sensitivity and Specificity,
217,VCells: Simple and Efficient Superpixels Using Edge-Weighted Centroidal Voronoi Tessellations,"Pattern Analysis and Machine Intelligence, IEEE Transactions on",2012,"VCells, the proposed Edge-Weighted Centroidal Voronoi Tessellations (EWCVTs)-based algorithm, is used to generate superpixels, i.e., an oversegmentation of an image. For a wide range of images, the new algorithm is capable of generating roughly uniform subregions and nicely preserving local image boundaries. The undersegmentation error is effectively limited in a controllable manner. Moreover, VCells is very efficient with core computational cost at cal O(Ksqrtn_ccdot N) in which K, n_c, and N are the number of iterations, superpixels, and pixels, respectively. Extensive qualitative discussions are provided, together with the high-quality segmentation results of VCells on a wide range of complex images. The simplicity and efficiency of our model are demonstrated by complexity analysis, time, and accuracy evaluations.",Clustering algorithms Computational efficiency Image color analysis Image edge detection Image segmentation Partitioning algorithms Shape Superpixels centroidal Voronoi tessellations clustering. image labeling image segmentation k-means,
218,Novel recovery mechanism for the restoration of image contents in teleconsultation sessions,Computer Methods and Programs in Biomedicine,2012,"In teleconsultation sessions, a critical dependency exists between the image contents and the type and sequential order of the image processing commands used by the various participants. Accordingly, for re-entrant/late users, a significant challenge exists in restoring the image contents of the teleconsultation session in such a way that all the participants maintain a consistent view of the medical images. In this paper, this problem is resolved using a novel recovery mechanism comprising two major components, namely an enhanced content-recording scheme designated as three-level indexing hierarchy (TIH) and a prioritized recovery policy. TIH maintains a record of all the commands which affect the appearance of each of medical images such that when a restoration process is required, these image-affect commands can be rapidly identified and transmitted to the user. As a result, a significant reduction can be gained in both the command identification/transmission time and the image restoration time compared to traditional recovery schemes, which restore the contents by re-executing all of the commands invoked during the course of the session. The prioritized recovery policy further reduces the time required for re-entrant/late users to catch up with the on-going session by utilizing the cross-linkage design within the TIH architecture to restore the foreground image (i.e. the image under current discussion) before the background images are restored (i.e. the remaining images in the session). To resolve the problem which arises when a background image is selected as the new foreground image before the restoration process is completed, the prioritized recovery policy maintains a set of resuming pointers for each re-entrant/late user to facilitate the process of suspending the current restoration process and switching to the restoration of the new foreground image. The evaluation results confirm that the TIH architecture and prioritized recovery policy yield a significant reduction in the recovery-latency delay compared to that required by traditional message-logging restoration systems.",Recovery mechanism Teleconsultation Content-recording Indexing Recovery policy,
219,Classified self-organizing map with adaptive subcodebook for edge preserving vector quantization,Neurocomputing,2009,This paper presents a novel classified self-organizing map method for edge preserving quantization of images using an adaptive subcodebook and weighted learning rate. The subcodebook sizes of two classes are automatically adjusted in training iterations based on modified partial distortions that can be estimated incrementally. The proposed weighted learning rate updates the neuron efficiently no matter of how large the weighting factor is. Experimental results show that the new method achieves better quality of reconstructed edge blocks and more spread out codebook and incurs a significantly less computational cost as compared to the competing methods.,Edge preserving Vector quantization Self-organizing map Adaptive learning Partial distortion theorem,
220,Automatic elastic image registration by interpolation of 3D rotations and translations from discrete rigid-body transformations,Medical Image Analysis,2006,"We present an algorithm for automatic elastic registration of three-dimensional (3D) medical images. Our algorithm initially recovers the global spatial mismatch between the reference and floating images, followed by hierarchical octree-based subdivision of the reference image and independent registration of the floating image with the individual subvolumes of the reference image at each hierarchical level. Global as well as local registrations use the six-parameter full rigid-body transformation model and are based on maximization of normalized mutual information (NMI). To ensure robustness of the subvolume registration with low voxel counts, we calculate NMI using a combination of current and prior mutual histograms. To generate a smooth deformation field, we perform direct interpolation of six-parameter rigid-body subvolume transformations obtained at the last subdivision level. Our interpolation scheme involves scalar interpolation of the 3D translations and quaternion interpolation of the 3D rotational pose. We analyzed the performance of our algorithm through experiments involving registration of synthetically deformed computed tomography (CT) images. Our algorithm is general and can be applied to image pairs of any two modalities of most organs. We have demonstrated successful registration of clinical whole-body CT and positron emission tomography (PET) images using this algorithm. The registration accuracy for this application was evaluated, based on validation using expert-identified anatomical landmarks in 15 CT–PET image pairs. The algorithm’s performance was comparable to the average accuracy observed for three expert-determined registrations in the same 15 image pairs.",Elastic image registration Volume subdivision Quaternion,
221,High-performance medical image registration using new optimization techniques,"Information Technology in Biomedicine, IEEE Transactions on",2006,"Optimization of a similarity metric is an essential component in intensity-based medical image registration. The increasing availability of parallel computers makes parallelizing some registration tasks an attractive option to increase speed. In this paper, two new deterministic, derivative-free, and intrinsically parallel optimization methods are adapted for image registration. DIviding RECTangles (DIRECT) is a global technique for linearly bounded problems, and multidirectional search (MDS) is a recent local method. The performance of DIRECT, MDS, and hybrid methods using a parallel implementation of Powell's method for local refinement, are compared. Experimental results demonstrate that DIRECT and MDS are robust, accurate, and substantially reduce computation time in parallel implementations",image registration medical image processing optimisation DIRECT MDS Powell's direction set method dividing rectangle technique high-performance medical image registration linearly bounded problem multidirectional search optimization parallel computing Biomedical imaging Concurrent computing Convergence Image registration Interpolation Memory architecture Optimization methods Parallel processing Robustness Technological innovation DIviding RECTangles (DIRECT) medical image registration multidirectional search (MDS) optimization parallel computing,
222,Combining global and local parallel optimization for medical image registration,,2005,"Optimization is an important component in linear and nonlinear medical image registration. While common non-derivative approaches such as Powell's method are accurate and efficient, they cannot easily be adapted for parallel hardware. In this paper, new optimization strategies are proposed for parallel, shared-memory (SM) architectures. The Dividing Rectangles (DIRECT) global method is combined with the local Generalized Pattern Search (GPS) and Multidirectional Search (AIDS) and to improve efficiency on multiprocessor systems. These methods require no derivatives, and can be used with all similarity metrics. In a multiresolution framework, DIRECT is performed with relaxed convergence criteria, followed by local refinement with AIDS or GPS. In 3D-3D MRI rigid registration of simulated MS lesion volumes to normal brains with varying noise levels, DIRECT/MDS had the highest success rate, followed by DIRECT/GPS. DIRECT/GPS was the most efficient (5-10 seconds with 8 CPUs, and 10-20 seconds with 4 CPUs). DIRECT followed by AIDS or GPS greatly increased efficiency while maintaining accuracy. Powell's method generally required more than 30 seconds (1 CPU) with a low success rate (0.3 or lower). This work indicates that parallel optimization on shared memory systems can markedly improve registration speed and accuracy, particularly for large initial misorientations.",,
223,Performance prediction of large MIMD systems for parallel neural network simulations,Future Generation Computer Systems,1995,"In this paper, we present a performance prediction model for indicating the performance range of MIMD parallel processor systems for neural network simulations. The model expresses the total execution time of a simulation as a function of the execution times of a small number of kernel functions, which have to be measured on only one processor and one physical communication link. The functions depend on the type of neural network, its geometry, decomposition and the connection structure of the MIMD machine. Using the model, the execution time, speedup, scalability and efficiency of large MIMD systems can be predicted. The model is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Measurements are taken from network simulations decomposed via dataset and network decomposition techniques. Agreement of the model with the measurements is within 1–14%. Estimates are given for the performances that can be expected for the new T9000 transputer systems. The presented method can also be used for other application areas such as image processing.",Performance prediction Parallel neural network simulations MIMD transputer systems,
224,DICOM Image Communication in Globus-Based Medical Grids,"Information Technology in Biomedicine, IEEE Transactions on",2008,"Grid computing, the collaboration of distributed resources across institutional borders, is an emerging technology to meet the rising demand on computing power and storage capacity in fields such as high-energy physics, climate modeling, or more recently, life sciences. A secure, reliable, and highly efficient data transport plays an integral role in such grid environments and even more so in medical grids. Unfortunately, many grid middleware distributions, such as the well-known Globus Toolkit, lack the integration of the world-wide medical image communication standard Digital Imaging and Communication in Medicine (DICOM). Currently, the DICOM protocol first needs to be converted to the file transfer protocol (FTP) that is offered by the grid middleware. This effectively reduces most of the advantages and security an integrated network of DICOM devices offers. In this paper, a solution is proposed that adapts the DICOM protocol to the Globus grid security infrastructure and utilizes routers to transparently route traffic to and from DICOM systems. Thus, all legacy DICOM devices can be seamlessly integrated into the grid without modifications. A prototype of the grid routers with the most important DICOM functionality has been developed and successfully tested in the MediGRID test bed, the German grid project for life sciences.",biomedical communication biomedical imaging grid computing medical computing middleware security of data telecommunication network routing telecommunication traffic transport protocols visual communication DICOM protocol Digital Imaging and Communication in Medicine German grid project MediGRID test bed data security file transfer protocol globus-based medical grids grid computing grid middleware distributions life sciences traffic routing Biomedical imaging DICOM Grid-Computing Image communication grid computing image communication Database Management Systems Image Interpretation Computer-Assisted Information Dissemination Information Storage and Retrieval Internet Radiology Information Systems,
225,A parallel implementation of exact Euclidean distance transform based on exact dilations,Microprocessors and Microsystems,2004,"This article reports the effective implementation of the exact Euclidean distance transform in a distributed system based on standard PCs, by using a simple data exchange protocol. The approach is based on the concept of exact distances, namely the denumerable set of distances found on the orthogonal lattice. The exact dilation algorithm, introduced recently, involves the successive scanning of the image elements for consecutive exact distance values, while assigning these values to empty neighboring pixels. The use of data compression methodology, as well as the quantitative characterization of the parallel efficiency, are also investigated and discussed considering several image sizes and the quantity of foreground elements. Among the obtained results, we have that the latter parameter strongly affects the overall performance and that the compressing strategy represents a potentially useful resource for increasing the overall processing efficiency.",Euclidean distance transform Distributed system Parallel algorithms Image processing,
226,Knowledge-based client-server approach to structural information retrieval: the digital anatomist browser,Computer Methods and Programs in Biomedicine,1993,"Structural information can be defined as data and knowledge about biological objects ranging in size from molecules to the whole body. A framework is described for organizing structural information around a well-defined set of terminology and semantic relationships, and for disseminating multimedia structural information by means of a wide-area information server that is accessible over the internet. A Macintosh-based client of this server, called the Digital Anatomist Browser, has been used to teach neuroanatomy for the last 2 years. The client-server approach provides each student unlimited access to a rapidly growing knowledge base of structural biology that, while immediately useful for anatomy teaching, has the potential to be an organizing framework for other kinds of medical knowledge as well.",Medical informatics Structural informatics Anatomy Client-server Knowledge-based systems,
227,Morphometric analysis of sonographic images by spatial geometric modeling,Computerized Medical Imaging and Graphics,1992,A methodology able to derive spatial geometric models from input sequences of sonographic slices is proposed. The developed modeling procedure can be utilized to perform computer-assisted anatomic 3D analysis both on all echo space and selected subregions. The modeling procedure is mainly composed of three sequential phases: a) automatic acquisition and preprocessing of time sequences of 2D echotomograms; b) 3D reconstruction of images and computing of discrete distance maps of selected echoes according to predefined projective laws; and c) generation of a spatial geometric model of the examined object starting from the previously computed maps.,Ultrasound Digital image processing Three-dimensional reconstruction Morphometric analysis Geometric modeling,
228,Fast algorithm for the 3-D DCT-II,"Signal Processing, IEEE Transactions on",2004,"Recently, many applications for three-dimensional (3-D) image and video compression have been proposed using 3-D discrete cosine transforms (3-D DCTs). Among different types of DCTs, the type-II DCT (DCT-II) is the most used. In order to use the 3-D DCTs in practical applications, fast 3-D algorithms are essential. Therefore, in this paper, the 3-D vector-radix decimation-in-frequency (3-D VR DIF) algorithm that calculates the 3-D DCT-II directly is introduced. The mathematical analysis and the implementation of the developed algorithm are presented, showing that this algorithm possesses a regular structure, can be implemented in-place for efficient use of memory, and is faster than the conventional row-column-frame (RCF) approach. Furthermore, an application of 3-D video compression-based 3-D DCT-II is implemented using the 3-D new algorithm. This has led to a substantial speed improvement for 3-D DCT-II-based compression systems and proved the validity of the developed algorithm.",data compression discrete cosine transforms image coding multidimensional signal processing video coding 3D DCT-II 3D image discrete cosine transforms fast algorithm fast multidimensional algorithms row-column-frame approach vector-radix decimation-in-frequency algorithm video compression Application software Discrete cosine transforms Discrete transforms Fast Fourier transforms Image coding Multidimensional systems Polynomials Video coding Video compression Virtual reality,
229,Parallel c-means algorithm for image segmentation on a reconfigurable mesh computer,Parallel Computing,2011,"In this paper, we propose a parallel algorithm for data classification, and its application for Magnetic Resonance Images (MRI) segmentation. The studied classification method is the well-known c-means method. The use of the parallel architecture in the classification domain is introduced in order to improve the complexities of the corresponding algorithms, so that they will be considered as a pre-processing procedure. The proposed algorithm is assigned to be implemented on a parallel machine, which is the reconfigurable mesh computer (RMC). The image of size (m × n) to be processed must be stored on the RMC of the same size, one pixel per processing element (PE).",Image segmentation Classification MRI image Parallel algorithm c-means,
230,Parallel CBIR implementations with load balancing algorithms,Journal of Parallel and Distributed Computing,2006,"The purpose of content-based information retrieval (CBIR) systems is to retrieve, from real data stored in a database, information that is relevant to a query. When large volumes of data are considered, as it is very often the case with databases dealing with multimedia data, it may become necessary to look for parallel solutions in order to store and gain access to the available items in an efficient way. Among the range of parallel options available nowadays, clusters stand out as flexible and cost effective solutions, although the fact that they are composed of a number of independent machines makes it easy for them to become heterogeneous. This paper describes a heterogeneous cluster-oriented CBIR implementation. First, the cluster solution is analyzed without load balancing, and then, a new load balancing algorithm for this version of the CBIR system is presented. The load balancing algorithm described here is dynamic, distributed, global and highly scalable. Nodes are monitored through a load index which allows the estimation of their total amount of workload, as well as the global system state. Load balancing operations between pairs of nodes take place whenever a node finishes its job, resulting in a receptor-triggered scheme which minimizes the system's communication overhead. Globally, the CBIR cluster implementation together with the load balancing algorithm can cope effectively with varying degrees of heterogeneity within the cluster; the experiments presented within the paper show the validity of the overall strategy. Together, the CBIR implementation and the load balancing algorithm described in this paper span a new path for performant, cost effective CBIR systems which has not been explored before in the technical literature.",Parallel implementations CBIR systems Load balancing algorithms,
231,On the advantages of combining differential algorithms and log-polar vision for detection of self-motion from a mobile robot,Robotics and Autonomous Systems,2001,"This paper describes the design and implementation on programmable hardware (FPGAs) of an algorithm for the detection of self-mobile objects as seen from a mobile robot. In this context, ‘self-mobile’ refers to those objects that change in the image plane due to their own movement, and not to the movement of the camera on board of the mobile robot. The method consists on adapting the original algorithm from Chen and Nandhakumar [A simple scheme for motion boundary detection, in: Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, 1994] by using foveal images obtained with a special camera whose optical axis points towards the direction of advance. It is shown that the use of log-polar geometry simplifies the original formulation and highly reduces the volume of data to be treated. Limitations of the algorithm due to the differential nature of the approach are discussed, relating them with the parameters of the system. Experiments are shown in which a self-mobile object is detected in several conditions.",Motion detection Log-polar vision Visual navigation Reconfigurable hardware architectures,
232,Rise of the Graphics Processor,Proceedings of the IEEE,2008,"The modern graphics processing unit (GPU) is the result of 40 years of evolution of hardware to accelerate graphics processing operations. It represents the convergence of support for multiple market segments: computer-aided design, medical imaging, digital content creation, document and presentation applications, and entertainment applications. The exceptional performance characteristics of the GPU make it an attractive target for other application domains. We examine some of this evolution, look at the structure of a modern GPU, and discuss how graphics processing exploits this structure and how nongraphical applications can take advantage of this capability. We discuss some of the technical and market issues around broader adoption of this technology.",computer graphics coprocessors graphics processing unit graphics processor Acceleration Application software Computational modeling Computer architecture Computer graphics Evolution (biology) Image generation Layout Medical simulation Rendering (computer graphics) Computer architecture computer graphics parallel processing,
233,Multi-GPU accelerated multi-spin Monte Carlo simulations of the 2D Ising model,Computer Physics Communications,2010,"A Modern Graphics Processing unit (GPU) is able to perform massively parallel scientific computations at low cost. We extend our implementation of the checkerboard algorithm for the two-dimensional Ising model [T. Preis et al., Journal of Chemical Physics 228 (2009) 4468–4477] in order to overcome the memory limitations of a single GPU which enables us to simulate significantly larger systems. Using multi-spin coding techniques, we are able to accelerate simulations on a single GPU by factors up to 35 compared to an optimized single Central Processor Unit (CPU) core implementation which employs multi-spin coding. By combining the Compute Unified Device Architecture (CUDA) with the Message Parsing Interface (MPI) on the CPU level, a single Ising lattice can be updated by a cluster of GPUs in parallel. For large systems, the computation time scales nearly linearly with the number of GPUs used. As proof of concept we reproduce the critical temperature of the 2D Ising model using finite size scaling techniques.",Monte Carlo simulation GPU computing Ising model Phase transition Finite size scaling,
234,Cell Accelerated Cryoablation Simulation,Computer Methods and Programs in Biomedicine,2010,"Tumor cryoablation is a clinical procedure where supercooled probes are used to destroy cancerous lesions. Cryoablation is a safe and effective palliative treatment for skeletal metastases, providing immediate and long term pain relief, increasing mobility and improving quality of life. Ideally, lesions are encompassed by an ice ball and frozen to a sufficiently low temperature to ensure cell death. “Lethal ice” is the term used to describe regions within the ice ball where cell death occurs. Failure to achieve lethal ice in all portions of a lesion may explain the high recurrence rate currently observed. Tracking growth of lethal ice is critical to success of percutaneous ablations, however, no practical methods currently exist for non-invasive temperature monitoring. Physicians lack planning tools which provide accurate estimation of the ice formation. Simulation of ice formation, while possible, is computationally demanding and too time consuming to be of clinical utility. We developed the computational framework for the simulation, acceleration strategies for multicore Intel x86 and IBM Cell architectures, and performed preliminary validation of the simulation. Our results demonstrate that the streaming SIMD implementation has better performance and scalability. Both accelerated and non-accelerated algorithms demonstrate good agreement between simulation and manually identified ice ball boundaries in phantom and patient images. Our results show promise for the development of novel cryoablation planning tools with real-time monitoring capability for clinical use.",Percutaneous cryoablation Simulation Cell Broadband Engine High-performance computing,
235,A new in vivo technique for determination of femoro-tibial and femoro-patellar 3D kinematics in total knee arthroplasty,Journal of Biomechanics,2007,"Aim was to develop an in vivo technique which allows determination of femoro-tibial and of femoro-patellar 3D-kinematics in TKA simultaneously. The knees of 20 healthy volunteers and of eight patients with TKA (PCR, rotating platform) were investigated. Kinematics analysis was performed in an open MR-system at different flexion angles with external loads being applied. The TKA components were identified using a 3D-fitting technique, which allows an automated 3D-3D-registration of the TKA. Femoro-patellar and femoro-tibial 3D-kinematics were analyzed by image postprocessing. The validity of the postprocessing technique demonstrated a coefficient of determination of 0.98 for translation and of 0.97 for rotation. The reproducibility yielded a coefficient of variation (CV%) for patella kinematics between 0.17% (patello-femoral angle) and 6.8% (patella tilt). The femoro-tibial displacement also showed a high reproducibility with CV% of 4.0% for translation and of 7.1% for rotation. While in the healthy knees the typical screw-home mechanism was observed, a paradoxical anterior translation of the femur relative to the tibia combined with an external rotation occurred after TKA. Fifty percent of the TKA's experienced a condylar lift-off of &gt;1 mm predominately on the medial side. Regarding patellar kinematics significant changes were found in both planes in TKA with an increased patella height in the sagittal plane and patella tilt and shift in the transversal plane. The results demonstrate that the presented 3D MR-open based method is highly reproducible and valid for image acquisition and postprocessing and provides—for the first time—in vivo data of 3D-kinematics of the tibio-femoral and simultaneously of the patello-femoral joint during knee flexion.",Knee Kinematics TKA Open MR imaging,
236,An extended JADE-S based framework for developing secure Multi-Agent Systems,Computer Standards & Interfaces,2009,"Agent communities are self-organized virtual spaces consisting of a large number of agents and their dynamic environments. Within a community, agents group together offering special e-services for effective, reliable, and mutual benefits. Usually, an agent community is composed of specialized agents performing one or more tasks in a single domain/sub-domain, or in highly intersecting domains. However, secure Multi-Agent Systems require severe mechanisms in order to prevent malicious attacks. Several limits affect exiting secure agents platform, such as the lack of a strong authentication system, the lack of a flexible distributed mechanism for access control and the lack of a system for storing past behaviors of agent/user. Biometric owner agents authentication, agent/users policies to regulate agent's behavior and actions, and agent/users reputation level to select trusted agents can be used to overcome the above limits and enhance the level of security for these applications. In this paper an extended JADE-S based framework for developing secure Multi-Agent Systems is proposed. The framework functionalities are extended by self-contained FPGA biometric sensors providing secure and fast user authentication service. Each agent owner, by means of biometric authentication, acquires his/her own X.509v3 digital certificate. Policy files and a flexible, fast distributed Access Control Mechanism can regulate behavior and actions of any users/agent inside the platform. In addition, a mechanism based on the agent reputation is used: reputation is an attribute associated to each owner and/or agent on the basis of its past behavior and integrity. In order to prove the feasibility of the proposed framework, we have developed a multi-agent e-Banking system. System goal deals with e-Banking services such as bank account statements, account transactions and so on. In the paper, the experimental features of the biometric self-contained sensors are also outlined.",Multi-Agent Systems (MASs) MAS Security JADE-S Biometric Authentication FPGA Prototyping e-Banking System,
237,Cloud federation in a layered service model,Journal of Computer and System Sciences,2012,"We show how a layered Cloud service model of software (SaaS), platform (PaaS), and infrastructure (IaaS) leverages multiple independent Clouds by creating a federation among the providers. The layered architecture leads naturally to a design in which inter-Cloud federation takes place at each service layer, mediated by a broker specific to the concerns of the parties at that layer. Federation increases consumer value for and facilitates providing IT services as a commodity. This business model for the Cloud is consistent with broker mediated supply and service delivery chains in other commodity sectors such as finance and manufacturing. Concreteness is added to the federated Cloud model by considering how it works in delivering the Weather Research and Forecasting service (WRF) as SaaS using PaaS and IaaS support. WRF is used to illustrate the concepts of delegation and federation, the translation of service requirements between service layers, and inter-Cloud broker functions needed to achieve federation.",Cloud computing Service layers SaaS AaaS PaaS IaaS Interoperability Service delegation Federation of Clouds,
238,A pipelined VLSI-based structure for the reconstruction of three-dimensional images from projections,Microprocessing and Microprogramming,1987,The backprojection problem for medical image processing is described for the special case of a HDPC position camera. Unrealistic high processing times prompt the introduction of parallel computing. a pipelined parallel architecture with processors attached to slices of the reconstruction cube is described. A standard cell ASIC implementation for the processor is on its way and preliminary results are reported.,,
239,ANN based integrated security assessment of power system using parallel computing,International Journal of Electrical Power & Energy Systems,2012,"This paper presents the application of cascade neural network (CANN) based approach for integrated security (voltage and line flow security) assessment. The developed cascade neural network is a combination of one screening module and two ranking modules, which are Levenberg–Marquardt Algorithm based neural networks (LMANNs). All the single line outage contingency cases are applied to the screening module, which is 3-layered feed-forward ANN having two outputs. The screening module is trained to classify them either in critical contingency class or in non-critical contingency class from the viewpoint of voltage/line loading. The screened critical contingencies are passed to the corresponding ranking modules, which are developed simultaneously by using parallel computing. Parallel computing deals with the development of programs where multiple concurrent processes cooperate in the fulfillment of a common task. For contingency screening and ranking, two performance indices: one based on voltage security of power system (VPI) and other based on line flow (MWPI) are used. Effectiveness of the proposed cascade neural network based approach has been demonstrated by applying it for contingency selection and ranking at different loading conditions for IEEE 30-bus and a practical 75-bus Indian system. The results obtained clearly indicate the superiority of the proposed approach in terms of speedup in training time of neural networks as compared to the case when the two ranking neural networks were developed sequentially to estimate VPI and MWPI.",Contingency screening and ranking Parallel computing Voltage performance index MW performance index Screening neural network Ranking neural network,
240,VLSI Implementation of Discrete Wavelet Transform for Lossless Compression of Medical Images,Real-Time Imaging,2001,"This paper presents a VLSI architecture to implement the forward and inverse two dimensional Discrete Wavelet Transform (DWT), to compress medical images for storage and retrieval. Lossless compression is usually required in the medical image field. The word length required for lossless compression makes too expensive the area cost of the architectures that appear in the literature. Thus, there is a clear need for designing a cost-effective architecture to implement the lossless compression of medical images using DWT. The data path word length has been selected to ensure the lossless accuracy criteria leading a high speed implementation with small chip area. The pyramid algorithm is reorganized and the algorithm locality is improved in order to obtain an efficient hardware implementation. The result is a pipelined architecture that supports single chip implementation in VLSI technology. The implementation employs only one multiplier and 352 memory elements to compute all scales what results in a considerable smaller chip area (45 mm2) than former implementations. The hardware design has been captured by means of the VHDL language and simulated on data taken from random images. Implemented in a 0.7 μm technology, it can compute both the forward and inverse DWT at a rate of 3.5 512×512 12 bit images/s corresponding to a clock speed of 33 MHz. This chip is the core of a PCI board that will speedup the DWT computation on desktop computers.",,
241,Particle Swarm Optimization and Differential Evolution for model-based object detection,Applied Soft Computing,2013,"Automatically detecting objects in images or video sequences is one of the most relevant and frequently tackled tasks in computer vision and pattern recognition. The starting point for this work is a very general model-based approach to object detection. The problem is turned into a global continuous optimization one: given a parametric model of the object to be detected within an image, a function is maximized, which represents the similarity between the model and a region of the image under investigation. In particular, in this work, the optimization problem is tackled using Particle Swarm Optimization (PSO) and Differential Evolution (DE). We compare the performances of these optimization techniques on two real-world paradigmatic problems, onto which many other real-world object detection problems can be mapped: hippocampus localization in histological images and human body pose estimation in video sequences. In the former, a 2D deformable model of a section of the hippocampus is fit to the corresponding region of a histological image, to accurately localize such a structure and analyze gene expression in specific sub-regions. In the latter, an articulated 3D model of a human body is matched against a set of images of a human performing some action, taken from different perspectives, to estimate the subject's posture in space. Given the significant computational burden imposed by this approach, we implemented PSO and DE as parallel algorithms within the nVIDIA™ CUDA computing architecture.",Object detection Pose estimation Deformable models Articulated models Particle Swarm Optimization Differential Evolution Global continuous optimization,
242,Switch fabric design for high performance IP routers: A survey,Journal of Systems Architecture,2005,"Traditionally, besides vendor product descriptions on high performance Internet Protocol (IP) router hardware (HW) architectures, materials on this subject area seldom appear in research literature. Recently, we introduced an architectural concept of HW scalability and bi-directional HW reconfigurability for high performance IP routers. Application of these two conceptual attributes enables router HW flexibility to adapt to today’s IP network environment with rapid changes in capacity and traffic characteristics. We analyzed 10 switch fabrics (SFs), selected and also presented brief survey of HW architectural techniques that enable the attributes for three candidates that can serve such a router. In this paper, we present a full survey of these 10 SFs. The intention is to provide background reference material on an area not yet frequently visited in formal literature.",IP router Switch fabric Hardware architecture Hardware scalability Bi-directional hardware reconfigurability,
243,A fuzzy evolutionary framework for combining ensembles,Applied Soft Computing,2013,"We propose an evolutionary framework for the production of fuzzy rule bases where each rule executes an ensemble of predictors. The architecture, the rule base and the composition of the ensembles are evolved over time. To achieve this, we employ a context-free grammar within a hybrid genetic programming system using a multi-population model. As base predictors, multilayer perceptron neural networks and support vector machines are available. We apply the system to several function approximation and regression tasks and compare the results with recent research and state-of-the-art models. We conclude that the proposed architecture is competitive and has a number of very desirable features supporting automation of predictive model building and their adaptation over time. Finally, we suggest further potential research directions.",Ensemble systems Function approximation Fuzzy rule based systems Genetic programming,
244,Major line removal morphological hough transform on a hybrid system,Journal of Parallel and Distributed Computing,2004,"This paper describes an implementation of a novel major line removal hough transform on a new parallel architectural system, Hybrid System. A Hybrid System is a combination of single instruction multiple data (SIMD) and multiple instruction multiple data (MIMD) systems processing at the same time. The line removal algorithm, which is used for detecting lines in an image, strips away major lines so that minor lines can become more easily detectable. The algorithm is implemented and evaluated on the hybrid system. Being a new extended architecture, we also established the derivation for the hybrid system speedup. We were able to obtain speedup that surpasses that of MIMD systems with the same number of PCs. In the paper, we also introduce a new SIMD concept.",Hough transform SIMD MIMD Hybrid Speedup Dilation Erosion,
245,Overlay striping and optimal parallel I/O for modern applications,Parallel Computing,1998,"Disk array systems are rapidly becoming the secondary-storage media of choice for many emerging applications with large storage and high bandwidth requirements. Striping data across the disks of a disk array introduces significant performance benefits mainly because the effective transfer rate of the secondary storage is increased by a factor equal to the stripe width. However, the choice of the optimal stripe width is an open problem: no general formal analysis has been reported and intuition alone fails to provide good guidelines. As a result one may find occasionally contradictory recommendations in the literature. With this work we first contribute an analytical calculation of the optimal stripe width. Second, we recognize that the optimal stripe width is sensitive to the multiprogramming level, which is not known a priori and fluctuates with time. Thus, calculations of the optimal stripe width are, by themselves only, of little practical use. For this reason we propose a novel striping technique, called overlay striping, which allows objects to be retrieved using a number of alternative stripe widths. We provide the detailed algorithms for our overlay striping method and we study the associated storage overhead and performance improvements and we show that we can achieve near optimal performance for very wide ranges of the possible multiprogramming levels, while incurring small storage overheads.",Disk subsystems Parallel I/O Overlay striping,
246,MIMD–SIMD hybrid system––towards a new low cost parallel system,Parallel Computing,2003,"This paper describes a new parallel architectural system which we have called an MIMD–SIMD hybrid system. As the name implies, MIMD–SIMD hybrid system (also denoted as hybrid system in this paper) is a combination of both SIMD and MIMD systems working concurrently to produce an optimal architecture. This new parallel architecture has the capability of achieving speedup rates more than its corresponding MIMD architecture can achieve alone. We introduce our new SIMD concept and also show the contribution of the SIMD on this hybrid system. We have also developed a general formula for determining the speedup of the hybrid system so that accurate predictions can be made on the performance of the hybrid system. A MIMD–SIMD hybrid system was constructed and was used to implement on a visualization algorithm.",Visualization Parallelization SIMD MIMD Hybrid system Speedup,
247,Surfing the optimization space of a multiple-GPU parallel implementation of a X-ray tomography reconstruction algorithm,Journal of Systems and Software,2014," The increasing popularity of massively parallel architectures based on accelerators have opened up the possibility of significantly improving the performance of X-ray computed tomography (CT) applications towards achieving real-time imaging. However, achieving this goal is a challenging process, as most CT applications have not been designed for exploiting the amount of parallelism existing in these architectures. In this paper we present the massively parallel implementation and optimization of Mangoose++, a CT application for reconstructing 3D volumes from 2D images collected by scanners based on cone-beam geometry. The main contribution of this paper are the following. First, we develop a modular application design that allows to exploit the functional parallelism inside the application and to facilitate the parallelization of individual application phases. Second, we identify a set of optimizations that can be applied individually and in combination for optimally deploying the application on a massively parallel multi-GPU system. Third, we present a study of surfing the optimization space of the modularized application and demonstrate that a significant benefit can be obtained from employing the adequate combination of application optimizations.",CT reconstruction Tomography GPGPU Optimization Paralellism,
248,A single mediaprocessor-based programmable ultrasound system,"Information Technology in Biomedicine, IEEE Transactions on",2003,"We have developed a programmable ultrasound imaging system using a single commercially available mediaprocessor. We have efficiently mapped all of the necessary B-mode processing algorithms on the underlying processor architecture, including envelope detection, dynamic range compression, lateral and axial filtering, persistence processing, and scan conversion. Our system can handle varying specifications ranging from 128 vectors and 512 samples per vector to more than 256 vectors and 1024 samples per vector. For an image size of 330 vectors and 512 samples per vector, it can process 30 frames per second using a 300-MHz MAP-CA mediaprocessor from Hitachi/Equator Technologies. This programmable ultrasound machine will not only offer significant advantages in terms of low cost, portability, scalability, and reduced development time, but also provide a flexible platform for developing and deploying new clinical applications to aid the clinicians and improve the quality of healthcare to patients.",biomedical ultrasonics medical image processing microprocessor chips parallel architectures performance evaluation B-mode processing algorithms Hitachi Equator Technologies VLIW axial filtering data-level parallelism direct memory access dynamic range compression envelope detection healthcare instruction parallelism lateral filtering low cost mediaprocessor medical imaging persistence processing portability processor architecture programmable ultrasound imaging system scalability scan conversion vectors Biomedical imaging Computed tomography Costs Field programmable gate arrays Flexible printed circuits Hardware Magnetic resonance imaging Parallel processing Positron emission tomography Ultrasonic imaging Algorithms Ultrasonography,
249,Content-based organisation of virtual repositories of DICOM objects,Future Generation Computer Systems,2009,"The integration of multi-centre medical image data to create knowledge repositories for research and training activities has been an aim targeted since long ago. This paper presents an environment to share, to process and to organise medical imaging data according to a structured framework in which the image reports play a key role. This environment has been validated on a clinical environment, facing problems such as firewalls and security restrictions, in the frame of the CVIMO (Valencian Cyberinfrastructure of Medical Imaging in Oncology) project. The environment uses a middleware called TRENCADIS (Towards a Grid Environment for Processing and Sharing DICOM Objects) that provides users with the management of multiple administrative domains, data encryption and decryption on the fly and semantic indexation of images. Data is structured into four levels: Global data available, virtual federated storages of studies shared across a vertical domain, subsets for projects or experiments on the virtual storage and individual searches on these subsets. This structure of levels gives the needed flexibility for organising authorisation, and hides data that are not relevant for a given experiment. The main components and interactions are shown in the document, outlining the workflows and explaining the different approaches considered, including the protocols used and the difficulties met.",Grid architecture DICOM objects Virtual storages Ontologies,
250,An iterative approach for minimax design of multidimensional quadrature mirror filters,Signal Processing,2011,"In this paper, a nest of iterative techniques is proposed for the minimax design of quadrature mirror filter (QMF) banks. The method can be generalized such that multidimensional QMF banks can be designed by the proposed algorithm. For a given weighting function, an iterative method is used to minimize the objective error function in the inner iterations. To further reduce the peak error of overall magnitude response, an iterative weighting-updated technique is adopted in the outer iterations. Comparing with the existing works concern the design of perfect-reconstruction QMF banks, only one of the filters is needed to be designed under the cost of magnitude distortion, but the system complexity can be reduced drastically. Several examples, including design of 2-D and 3-D QMF banks, will be presented to demonstrate the effectiveness of the proposed method.",Quadrature mirror filter (QMF) bank Two-channel filter bank Multirate system Multidimensional digital filter Iterative weighted least-squares method Diamond-shaped filter,
251,GPU-based iterative transmission reconstruction in 3D ultrasound computer tomography,Journal of Parallel and Distributed Computing,2014," As today’s standard screening methods frequently fail to detect breast cancer before metastases have developed, early diagnosis is still a major challenge. With the promise of high-quality volume images, three-dimensional ultrasound computer tomography is likely to improve this situation, but has high computational needs. In this work, we investigate the acceleration of the ray-based transmission reconstruction by a GPU-based implementation of the iterative numerical optimization algorithm TVAL3. We identified the regular and transposed sparse-matrix–vector multiply as the performance limiting operations. For accelerated reconstruction we propose two different concepts and devise a hybrid scheme as optimal configuration. In addition we investigate multi-GPU scalability and derive the optimal number of devices for our two primary use-cases: a fast preview mode and a high-resolution mode. In order to achieve a fair estimation of the speedup, we compare our implementation to an optimized CPU version of the algorithm. Using our accelerated implementation we reconstructed a preview 3D volume with 24,576 unknowns, a voxel size of (8 mm)3 and approximately 200,000 equations in 0.5 s. A high-resolution volume with 1,572,864 unknowns, a voxel size of (2mm)3 and approximately 1.6 million equations was reconstructed in 23 s. This constitutes an acceleration of over one order of magnitude in comparison to the optimized CPU version.",GPU Ultrasound imaging Application acceleration SpMV Numerical optimization,
252,Moment-based approaches in imaging part 3: computational considerations [A Look at...],"Engineering in Medicine and Biology Magazine, IEEE",2008,This paper discusses the computation of moments in imaging. Faster algorithms are needed to decrease the computational complexity for real-time environment applications.,medical image processing method of moments computation of moments computational complexity fast algorithm image analysis method imaging moment-based approach real-time environment application Collaboration Image sequence analysis Image sequences Interpolation Jacobian matrices Noise robustness Polynomials Spline Time factors Two dimensional displays Algorithms Image Enhancement Image Interpretation Computer-Assisted Information Storage and Retrieval Reproducibility of Results Sensitivity and Specificity Signal Processing Computer-Assisted,
253,Evaluation of performance and architectural efficiency of FPGAs and GPUs in the 40 and 28&#xa0;nm generations for algorithms in 3D ultrasound computer tomography,Computers & Electrical Engineering,2014," In heterogeneous computing, application developers have to identify the best-suited target platform from a variety of alternatives. In this work, we compare performance and architectural efficiency of Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs) for two algorithms taken from a novel medical imaging method named 3D ultrasound computer tomography. From the 40 nm and 28 nm generations, we use top-notch devices and those with similar power consumption values. For our two benchmark algorithms from the signal processing and imaging domain, the results show that if power consumption is not considered, the GPU and FPGA from the 40nm generation give both, a similar performance and efficiency per transistor. In the 28 nm process, in contrast, the FPGA is superior to its GPU counterpart by 86% and 39%, depending on the algorithm. If power is limited, FPGAs outperform GPUs in each investigated case by at least a factor of four.",,
254,Biomedical imaging and the evolution of medical informatics,Computerized Medical Imaging and Graphics,1996,,,
255,Computer vision guided virtual craniofacial reconstruction,Computerized Medical Imaging and Graphics,2007,"The problem of virtual craniofacial reconstruction from a sequence of computed tomography (CT) images is addressed and is modeled as a rigid surface registration problem. Two different classes of surface matching algorithms, namely the data aligned rigidity constrained exhaustive search (DARCES) algorithm and the iterative closest point (ICP) algorithm are first used in isolation. Since the human bone can be reasonably approximated as a rigid body, 3D rigid surface registration techniques such as the DARCES and ICP algorithms are deemed to be well suited for the purpose of aligning the fractured bone fragments. A synergistic combination of these two algorithms, termed as the hybrid DARCES–ICP algorithm, is proposed. The hybrid algorithm is shown to result in a more accurate mandibular reconstruction when compared to the individual algorithms used in isolation. The proposed scheme for virtual reconstructive surgery would prove to be of tremendous benefit to the operating surgeons as it would allow them to pre-visualize the reconstructed mandible (i.e., the end-product of their work), before performing the actual surgical procedure. Experimental results on both phantom and real (human) patient datasets are presented.",DARCES ICP Bipartite graph matching Computed tomography,
256,Generalized symmetric interpolating wavelets,Computer Physics Communications,1999,"A new class of biorthogonal wavelets-interpolating distributed approximating functional (DAF) wavelets are proposed as a powerful basis for scale-space functional analysis and approximation. The important advantage is that these wavelets can be designed with infinite smoothness in both time and frequency spaces, and have as well symmetric interpolating characteristics. Boundary adaptive wavelets can be implemented conveniently by simply shifting the window envelope. As examples, generalized Lagrange wavelets and generalized Sinc wavelets are presented and discussed in detail. Efficient applications in computational science and engineering are explored.",Interpolating distributed approximating functional wavelets,
257,Optimizing parameters of a motion detection system by means of a distributed genetic algorithm,Image and Vision Computing,2005,"The main task of traffic monitoring applications is to identify moving targets. Usually, these applications require that a large number of parameters is tuned in order to work properly. In the motion detection system we have developed, about thirty parameters have been required to be optimized. This paper shows how a distributed implementation of a Genetic Algorithm (GA) over a network of workstations can successfully accomplish the parameter optimization task within a reduced amount of time. Accurate experiments accomplished on a challenging training sequence yield optimal parameter values. Four more test sequences allow us to assess the generality of the results previously attained.",Motion detection Parameter optimization Traffic monitoring Visual surveillance Distributed genetic algorithm,
258,A survey of GPU-based medical image computing techniques,Quantitative imaging in medicine and surgery,2012,"Medical imaging currently plays a crucial role throughout the entire clinical applications from medical scientific research to diagnostics and treatment planning. However, medical imaging procedures are often computationally demanding due to the large three-dimensional (3D) medical datasets to process in practical clinical applications. With the rapidly enhancing performances of graphics processors, improved programming support, and excellent price-to-performance ratio, the graphics processing unit (GPU) has emerged as a competitive parallel computing platform for computationally expensive and demanding tasks in a wide range of medical image applications. The major purpose of this survey is to provide a comprehensive reference source for the starters or researchers involved in GPU-based medical image processing. Within this survey, the continuous advancement of GPU computing is reviewed and the existing traditional applications in three areas of medical image processing, namely, segmentation, registration and visualization, are surveyed. The potential advantages and associated challenges of current GPU-based medical imaging are also discussed to inspire future applications in medicine.",,
259,Two Complementary Approaches for Integrating a Lattice Boltzmann Flow Solver into Simulation Frameworks,Procedia Computer Science,2011,"Computer simulation plays an increasingly important role in the area of medical physics. The coupled simulation of blood flow, species transport and biological processes in patient specific vessel geometries is a rapidly growing field with the ambitious goal to help clinicians with risk prediction and treatment planning, for example in the case of cardio-vascular diseases. In this paper we describe two complementary approaches of embedding a Lattice Boltzmann flow solver into environments requiring CFD-simulations in the framework of medical physics: a tool chain for performing blood flow simulation within cerebral aneurysms, and a framework for a coupled multi-physics multi-scale simulation of flow-induced biological processes. While for the blood flow simulation the challenge is to set-up an automated tool chain for performing a large number of robust CFD-simulations with a minimum of human interaction, the coupling-framework requires the communication of the LB solver within a complex software environment. We compare both approaches and identify possible synergies, leading towards powerful simulation systems.",Lattice Boltzmann coupled simulation multi-scale cerebral aneurysm,
260,FR-KECA: Fuzzy robust kernel entropy component analysis,Neurocomputing,1415-1423,,,
261,Non-Newtonian blood flow simulation in cerebral aneurysms,Computers & Mathematics with Applications,2009,"Computer simulations play an increasingly important role in the area of medical physics, from fundamental research to patient specific treatment planning. One particular application we address in this paper is the simulation of blood flow in cerebral aneurysms in domains created from medical images. Our focus is on accurately capturing the rheology for this type of application, particularly the difference in wall shear stress between Newtonian and non-Newtonian fluids. The numerical method applied for our investigation is a performance optimized Lattice Boltzmann solver with a Carreau–Yasuda model to capture non-Newtonian rheology.",Cerebral aneurysm Non-Newtonian rheology Blood flow Lattice Boltzmann,
262,Consensus fingerprint matching with genetically optimised approach,Pattern Recognition,2009,"Fingerprint matching has been approached using various criteria based on different extracted features. However, robust and accurate fingerprint matching is still a challenging problem. In this paper, we propose an improved integrated method which operates by first suggesting a consensus matching function, which combines different matching criteria based on heterogeneous features. We then devise a genetically guided approach to optimise the consensus matching function for simultaneous fingerprint alignment and verification. Since different features usually offer complementary information about the matching task, the consensus function is expected to improve the reliability of fingerprint matching. A related motivation for proposing such a function is to build a robust criterion that can perform well over a variety of different fingerprint matching instances. Additionally, by employing the global search functionality of a genetic algorithm along with a local matching operation for population initialisation, we aim to identify the optimal or near optimal global alignment between two fingerprints. The proposed algorithm is evaluated by means of a series of experiments conducted on public domain collections of fingerprint images and compared with previous work. Experimental results show that the consensus function can lead to a substantial improvement in performance while the local matching operation helps to identify promising initial alignment configurations, thereby speeding up the verification process. The resulting algorithm is more accurate than several other proposed methods which have been implemented for comparison.",Alignment Fingerprints Genetic algorithms (GAs) Verification,
263,Chaste: A case study of parallelisation of an open source finite-element solver with applications to computational cardiac electrophysiology simulation,International Journal of High Performance Computing Applications,2014,"The simulation of cardiac electrophysiology is a mature field in computational physiology. Recent advances in medical imaging, high-performance computing and numerical methods mean that computational models of electrical propagation in human heart tissue are ripe for use in patient-specific simulation for diagnosis, for prognosis and for selection of treatment methods. However, in order to move in this direction, it is necessary to make efficient use of modern petascale computing resources. This paper focuses on an existing open source simulation framework (Chaste) and documents work done to improve the parallel scaling on a small range of electrophysiology benchmark problems. These benchmarks involve the numerical solution of the monodomain or bidomain equations via the finite-element method. At the beginning of this study the electrophysiology libraries within Chaste were already enabled to run in parallel and were able to solve for electrical propagation using the monodomain or bidomain equations, but parallel efficiency dropped rapidly when run on more than about 64 processors. Throughout the course of the study, improvements were made to problem definition input; geometric mesh partitioning; finite-element assembly of large, sparse linear systems; problem-specific matrix preconditioning; numerical solution of the linear system; and output of the approximate solution. The consequence of these improvements is that, at the end of the study, Chaste is able to solve a monodomain benchmark problem in close to real time. While some of the improvements made to the parallel Chaste code are specific to cardiac electrophysiology, many of the techniques documented in this paper are generic to the parallel finite-element method in other scientific application areas.",,
264,3D medical image coding using separable 3D wavelet decomposition and lattice vector quantization,Signal Processing,1997,"An original 3D subband coding scheme is proposed. A separable 3D wavelet, taking full advantage of the 3D structures correlation, decomposes the original volume into subvolumes which can be separately quantized by a uniform scalar quantizer or by a 3D lattice vector quantizer. Concentric hyper-pyramids lying on the cubic lattice are used for searching codewords. A distortion minimization algorithm both selects the best number of decomposition and the best set of quantizers in order to minimize the overall mean square error. The whole algorithm is applied on a 3D image data base issued from the Morphometer (a new true 3D X-Ray scanner). The results presented include traditional signal-to-noise ratio performances and a subjective evaluation made by radiologists.",3D image coding wavelet transform 3D X-ray scanner bit allocation scalar quantization pyramidal vector quantization,
265,"Service monitoring and differentiation techniques for resource allocation in the grid, on the basis of the level of service",Future Generation Computer Systems,2011,"The study of meta-scheduling of jobs in the Grid has been a recurrent topic on the literature. Many tools have been developed for allocating the jobs to the most appropriate resources according to specific application needs while balancing the workload among resources. However, few of them focus on evaluating the level of service attained by providers of Grid services. On the contrary, Grid generally offers best-effort service to all the applications. Without Quality of Service (QoS), the jobs are executed without any guarantee on the execution time or throughput. Also, the support for qualitative attributes, such as security, is limited to the information provided by the resources. The uncertainty on the final performance is an issue that affects user satisfaction, reducing the interest of Grid infrastructures. In this paper, we present GRIDIFF, a software architecture that covers all necessary steps to integrate QoS into the process of allocating resources for the execution of jobs in the Grid. GRIDIFF has been used in practice to differentiate the resources according to the fulfillment of the 100% of the QoS considering three groups with failure rates of 9.1%, 13.3% and 64.1%, respectively. Scalability has been evaluated through simulation in up to 7000 nodes that can handle up to 250 monitoring agents per node.",Grid computing Quality of service Resource allocation Service differentiation Monitoring,
266,An integrative approach to high-performance biomedical problem solving environments on the Grid,Parallel Computing,2004,"We conduct computer simulation experiments in pre-operative planning of vascular reconstruction with a physician in the experimental loop. We constructed a problem solving environment, which offers an integrative approach for constructing and running complex interactive systems. Grid resources are used for access to medical image repositories, segmentation services, simulation of blood flow, and visualization in virtual environments of the simulated results together with medical data obtained from MRI/CT scanners. This case study in vascular reconstruction planning has been validated via contextual analysis in a number of hospitals, resulting in a set of new requirements gathered for future versions of the system.",Grid computing Integrative approach Biomedical applications,
267,A flexible 3D cerebrovascular extraction from TOF-MRA images,Neurocomputing,2013," For accurately extracting 3D cerebral tree from time-of-flight magnetic resonance angiography (TOF-MRA) images, a novel active contour model is presented by combining the statistical information and the vessel shape information in a variational level set framework. Firstly, a finite mixture model of four Gaussian distributions is proposed to model the statistical distribution of the density of TOF-MRA data, which can excellently describe the distribution of the cerebral vessels and background. Secondly, a vascular vector field, derived from the eigenanalysis of the Hessian matrix, is employed to model vessel shape information, which can guide the contour evolving along the vessel center line towards the thin or weak vessels. Thirdly, an automatic method of initializing the contour, derived from enhancement of the vessel shape, is proposed, which is capable of setting the initial contour efficiently and automatically. Furthermore, the narrow band technique is adopted, which effectively reduces the computation. Finally, a speedup strategy in the implementation is developed by labeling the steadily-evolved points, which avoids the repeated computation of these points in the later iterations. Experiments are made with several classical models over 9 MRA datasets, and the results show that the proposed model outperforms the classical models.",Cerebrovascular extraction Active contour model TOF-MRA images Mixture model Vessel shape,
268,Computationally efficient filtered-backprojection algorithm for tomographic image reconstruction using Walsh transform,Journal of Visual Communication and Image Representation,2006,"In this paper, we discuss the implementation of the filtered-backprojection (FBP) algorithm for tomographic image reconstruction using Walsh transform to exploit its fast computational ability. Walsh transform is the fastest unitary transform known so far. The major advantage of Walsh transform is that it involves only real additions and subtractions whereas Fourier transform involves complex multiplications and additions. Implementation of the proposed algorithm necessitates the design of an appropriate filter in Walsh domain. In this research, the known Fourier filter coefficients have been transformed into Walsh domain, thereby the 1 × N Fourier filter coefficients were converted into an N × N sparse matrix with nonzero elements in a special pattern. The proposed algorithm has been implemented by taken into account of the special nature of the Walsh domain filter coefficients and tested for its performance using the well-known ‘Shepp-Logan head phantom’ test image. The results demonstrate that the reconstruction strategy has comparable performance with a significant reduction of computing time. For example, with a 128 × 128-pixel image and 180 views, the speedup achieved is fourfold, with reconstructions qualitatively and visually the same as that of FBP algorithm in the Fourier domain.",Tomography Walsh transform Fast algorithm Filtered-backprojection algorithm,
269,Utilizing Radio-Frequency Interconnect for a Many-DIMM DRAM System,"Emerging and Selected Topics in Circuits and Systems, IEEE Journal on",2012,"The demand for capacity and off-chip bandwidth to dynamic random-access memory (DRAM) will continue to grow as we integrate more cores onto a die. However, as the data rate of DRAM has increased, the number of dual in-line memory modules (DIMMs) supported on a multi-drop bus has decreased. Therefore, traditional memory systems are not sufficient to meet both these demands. We propose the DIMM tree architecture for better scalability by connecting the DIMMs as a tree. The DIMM tree architecture is able to grow the number of DIMMs exponentially with each level of latency in the tree. We also propose application of multiband radio-frequency interconnect (MRF-I) to the DIMM tree architecture for even greater scalability and higher throughput. The DIMM tree architecture without MRF-I was able to scale up to 64 DIMMs with only an 8% degradation in throughput over an ideal system. The DIMM tree architecture with MRF-I was able to increase throughput by 68% (up to 200%) on a 64-DIMM system over a 4-DIMM system. Finally, we propose the partitioned DIMM tree, which allows the scaling of a main memory system to a many-DIMM memory system while still maintaining high throughput. The partitioned DIMM tree is able to improve throughput by an average of 19% up to 35% over the DIMM tree with 256 DIMMs on a single channel.",DRAM chips radiofrequency interconnections 64-DIMM system DIMM tree architecture dual in-line memory modules dynamic random access memory many-DIMM DRAM system memory system multiband radio frequency interconnect multidrop bus off-chip bandwidth DRAM chips Pins Radio frequency Throughput Transceivers Vegetation Integrated circuit interconnections memory architecture radio-frequency integrated circuits random access memory,
270,Robust reversible watermarking scheme using Slantlet transform matrix,Journal of Systems and Software,2014," The need for a robust reversible watermarking method has recently attracted more attention. This paper presents a novel robust reversible watermarking scheme based on using the Slantlet transform matrix to transform small blocks of the original image and hiding the watermark bits by modifying the mean values of the carrier subbands. The problem of overflow/underflow has been avoided by using histogram modification process. Extensive experimental tests based on 100 general images and 100 medical images demonstrate the efficiency of the proposed scheme. The proposed scheme has robustness against different kinds of attacks and the results prove that it is completely reversible with improved capacity, robustness, and invisibility in comparison with the previous methods.",Robust reversible watermarking (RRW) Histogram modification Slantlet transform (SLT),
271,Sequentially-Coupled Arterial Fluid–Structure Interaction (SCAFSI) technique,Computer Methods in Applied Mechanics and Engineering,2009,"The Sequentially-Coupled Arterial Fluid–Structure Interaction (SCAFSI) technique is one of the special techniques developed recently by the Team for Advanced Flow Simulation and Modeling (T☆AFSM) for FSI modeling of blood flow and arterial dynamics. The SCAFSI technique, which was introduced as an approximate FSI approach in arterial fluid mechanics, is based on the assumption that the arterial deformation during a cardiac cycle is driven mostly by the blood pressure. In the SCAFSI, first we compute a “reference” arterial deformation as a function of time, driven only by the blood pressure profile of the cardiac cycle. Then we compute a sequence of updates involving mesh motion, fluid dynamics calculations, and recomputing the arterial deformation. Although the SCAFSI technique was developed and tested in conjunction with the stabilized space–time FSI (SSTFSI) technique, it can also be used in conjunction with other FSI modeling techniques categorized as moving-mesh methods. The SSTFSI technique is based on the Deforming-Spatial-Domain/Stabilized Space–Time (DSD/SST) formulation and includes the enhancements introduced recently by the T☆AFSM. The arterial structures can be modeled with the membrane or continuum elements, both of which are geometrically nonlinear, and the continuum element can be made of linearly-elastic or hyperelastic material (Mooney–Rivlin or Fung). Here we provide an overview of the SCAFSI technique and present a number of test computations for abdominal aortic and cerebral aneurysms, where the arterial geometries used in the computations are close approximations to the patient-specific image-based data.",Cardiovascular fluid mechanics Fluid–Structure Interactions Finite elements Space–time methods Sequentially-Coupled Arterial FSI,
272,Efficient irregular wavefront propagation algorithms on hybrid CPU–GPU machines,Parallel Computing,2013," We address the problem of efficient execution of a computation pattern, referred to here as the irregular wavefront propagation pattern (IWPP), on hybrid systems with multiple CPUs and GPUs. The IWPP is common in several image processing operations. In the IWPP, data elements in the wavefront propagate waves to their neighboring elements on a grid if a propagation condition is satisfied. Elements receiving the propagated waves become part of the wavefront. This pattern results in irregular data accesses and computations. We develop and evaluate strategies for efficient computation and propagation of wavefronts using a multi-level queue structure. This queue structure improves the utilization of fast memories in a GPU and reduces synchronization overheads. We also develop a tile-based parallelization strategy to support execution on multiple CPUs and GPUs. We evaluate our approaches on a state-of-the-art GPU accelerated machine (equipped with three GPUs and two multicore CPUs) using the IWPP implementations of two widely used image processing operations: morphological reconstruction and euclidean distance transform. Our results show significant performance improvements on GPUs. The use of multiple CPUs and GPUs cooperatively attains speedups of 50× and 85× with respect to single core CPU executions for morphological reconstruction and euclidean distance transform, respectively.",Irregular wavefront propagation pattern GPGPU Cooperative CPU–GPU execution Heterogeneous environments Morphological reconstruction Euclidean distance transform,
273,Generalized EM-Type Reconstruction Algorithms for Emission Tomography,"Medical Imaging, IEEE Transactions on",2012,"We provide a general form for many reconstruction estimators of emission tomography. These estimators include Shepp and Vardi's maximum likelihood (ML) estimator, the quadratic weighted least squares (WLS) estimator, Anderson's WLS estimator, and Liu and Wang's multi-objective estimator, and others. We derive a generic update rule by constructing a surrogate function. This work is inspired by the ML-EM (EM, expectation maximization), where the latter naturally arises as a special case. A regularization with a specific form can also be incorporated by De Pierro's trick. We provide a general and quite different convergence proof compared with the proofs of the ML-EM and De Pierro. Theoretical analysis shows that the proposed algorithm monotonically decreases the cost function and automatically meets nonnegativity constraints. We have introduced a mechanism to provide monotonic, self-constraining, and convergent algorithms, from which some interesting existing and new algorithms can be derived. Simulation results illustrate the behavior of these algorithms in term of image quality and resolution-noise tradeoff.",convergence emission tomography expectation-maximisation algorithm image denoising image reconstruction image resolution maximum likelihood estimation medical image processing Anderson WLS estimator De Pierro trick Liu multiobjective estimator Shepp maximum likelihood estimator Vardi maximum likelihood estimator Wang multiobjective estimator convergence proof convergent algorithms cost function emission tomography expectation maximization generalized EM-type reconstruction algorithms image quality monotonic algorithms quadratic weighted least squares estimator reconstruction estimators resolution-noise tradeoff self-constraining algorithms surrogate function theoretical analysis Algorithm design and analysis Convergence Cost function Image reconstruction Maximum likelihood estimation Q measurement Reconstruction algorithms Global convergence KuhnâTucker (KT) conditions regularization technique surrogate function Algorithms Humans Image Processing Computer-Assisted Phantoms Imaging Thorax Tomography Emission-Computed,
274,Using Data Mining Techniques to Support Value Management Workshops in Construction,Tsinghua Science & Technology,2008,"Problem-solving processes in value management (VM) workshops in the construction industry are experience-based, and the quality of these workshops depends very much on the experience of the team members. The efficiency and effectiveness of VM workshops can be improved by better reusing the experience of previous VM cases and field knowledge. This paper describes a new approach to facilitate VM workshops in the construction industry using data mining (DM) techniques. The feasibility of integrating DM techniques with VM workshops in the construction industry is demonstrated in case studies. Examples are presented to illustrate different methods of applying DM tools in VM workshops. The results show that DM techniques can help team members in VM workshops to understand their problems more clearly and to generate more ideas for current problems.",biomass fuel ethanol biodiesel diesel blending emissions,
275,Statistical data mining of streaming motion data for activity and fall recognition in assistive environments,Neurocomputing,2013,"The analysis of human motion data is interesting in the context of activity recognition or emergency event detection, especially in the case of elderly or disabled people living independently in their homes. Several techniques have been proposed for identifying such distress situations using either motion, audio and video sensors on the monitored subject (wearable sensors) or devices installed at the surrounding environment. Visual data captured from the user's environment, using overhead cameras along with motion data, which are collected from accelerometers on the subject's body, can be fed to activity detection systems that can detect emergency situations like falls and injuries. The output of these sensors is data streams that require real time recognition, especially in such emergency situations. In this paper, we study motion and activity related streaming data and we propose classification schemes using traditional classification approaches. However, such approaches may not be always applicable for immediate alarm triggering and fall prevention or when CPU power and memory resources are limited (e.g. running the detection algorithm on a mobile device such as smartphones). To this end, we also propose a statistical mining methodology that may be used for real time motion data processing. The paper includes details of the stream data analysis methodology incorporated in the activity recognition and fall detection system along with an initial evaluation of the achieved accuracy in detecting falls. The results are promising and indicate that using the proposed methodology real time fall detection is feasible.",Streaming motion data Fall detection Visual data Cumulative sum (CUSUM) algorithm,
276,HAMMER: hierarchical attribute matching mechanism for elastic registration,"Medical Imaging, IEEE Transactions on",2002,"A new approach is presented for elastic registration of medical images, and is applied to magnetic resonance images of the brain. Experimental results demonstrate very high accuracy in superposition of images from different subjects. There are two major novelties in the proposed algorithm. First, it uses an attribute vector, i.e., a set of geometric moment invariants (GMIs) that are defined on each voxel in an image and are calculated from the tissue maps, to reflect the underlying anatomy at different scales. The attribute vector, if rich enough, can distinguish between different parts of an image, which helps establish anatomical correspondences in the deformation procedure; it also helps reduce local minima, by reducing ambiguity in potential matches. This is a fundamental deviation of our method, referred to as the hierarchical attribute matching mechanism for elastic registration (HAMMER), from other volumetric deformation methods, which are typically based on maximizing image similarity. Second, in order to avoid being trapped by local minima, i.e., suboptimal poor matches, HAMMER uses a successive approximation of the energy function being optimized by lower dimensional smooth energy functions, which are constructed to have significantly fewer local minima. This is achieved by hierarchically selecting the driving features that have distinct attribute vectors, thus, drastically reducing ambiguity in finding correspondence. A number of experiments demonstrate that the proposed algorithm results in accurate superposition of image data from individuals with significant anatomical differences.",biomedical MRI brain image matching image registration medical image processing vectors HAMMER accurate superposition attribute vectors average brain deformable registration distinct attribute vectors geometric moment invariants hierarchical attribute matching mechanism for elastic registration hierarchical deformation mechanism image data image voxel local minima lower dimensional smooth energy functions multigrid formulation significant anatomical differences statistical atlases suboptimal poor matches volumetric deformation methods Aging Anatomy Biomedical computing Biomedical imaging Biomedical measurements Brain Image analysis Magnetic resonance Neuroimaging Radiology Aged Algorithms Atrophy Brain Elasticity Humans Image Enhancement Magnetic Resonance Imaging Pattern Recognition Automated Quality Control Reproducibility of Results Sensitivity and Specificity Stochastic Processes Subtraction Technique,
277,A survey of pipelined workflow scheduling: Models and algorithms,ACM Comput. Surv.,2013,,,
278,Parallel computation of mutual information on the GPU with application to real-time registration of 3D medical images,Computer Methods and Programs in Biomedicine,2010,"Due to processing constraints, automatic image-based registration of medical images has been largely used as a pre-operative tool. We propose a novel method named sort and count for efficient parallelization of mutual information (MI) computation designed for massively multi-processing architectures. Combined with a parallel transformation implementation and an improved optimization algorithm, our method achieves real-time (less than 1 s) rigid registration of 3D medical images using a commodity graphics processing unit (GPU). This represents a more than 50-fold improvement over a standard implementation on a CPU. Real-time registration opens new possibilities for development of improved and interactive intraoperative tools that can be used for enhanced visualization and navigation during an intervention.",Image registration Mutual information (MI) Histogram Parallel processing Graphics processor unit (GPU),
279,Visualization in Computational Science,Procedia Computer Science,2010,"This section and the papers that follow relate to to the workshop on Visualization in Computational Science which is held at the Tenth International Conference on Computational Science (ICCS2010) in Amsterdam, the Netherlands. This workshop brings together experts from visualization research and end-users to illustrate the state of the art and advance potential areas of common interest.",,
280,On optimization of finite-difference time-domain (FDTD) computation on heterogeneous and GPU clusters,Journal of Parallel and Distributed Computing,2011,"A model for the computational cost of the finite-difference time-domain (FDTD) method irrespective of implementation details or the application domain is given. The model is used to formalize the problem of optimal distribution of computational load to an arbitrary set of resources across a heterogeneous cluster. We show that the problem can be formulated as a minimax optimization problem and derive analytic lower bounds for the computational cost. The work provides insight into optimal design of FDTD parallel software. Our formulation of the load distribution problem takes simultaneously into account the computational and communication costs. We demonstrate that significant performance gains, as much as 75%, can be achieved by proper load distribution.",Finite-difference time-domain (FDTD) Heterogeneous computing Parallel processing Graphics Processing Unit (GPU) Optimization,
281,Numerical identification method for the non-linear viscoelastic compressible behavior of soft tissue using uniaxial tensile tests and image registration – Application to rat lung parenchyma,Journal of the Mechanical Behavior of Biomedical Materials,2014," This paper presents an improved identification method of the constitutive properties of lung parenchyma. We aim to determine the non-linear viscoelastic behavior of lung parenchyma with a particular focus on the compressible properties - i.e. the ability to change volume. Uniaxial tensile tests are performed on living precision-cut rat lung slices. Image registration is used to compute the displacement field at the surface of the sample. The constitutive model consists of a hyperelastic potential split into volumetric and isochoric contributions and a viscous contribution. This allows for the description of the experimentally observed hysteresis loop. The identification is performed numerically: each test is simulated using the realistic geometry of the sample; the difference between the measured and computed displacements is minimized with an optimization algorithm. We compare several hyperelastic potentials and we can determine the most suitable law for rat lung parenchyma. An exponential potential or a polynomial potential with a first order term and a third or higher order term give similarly satisfactory results. The identified parameters are: for the volumetric contribution: κ =7.25e4 Pa, for the exponential form: k1=4.34e3 Pa, k2=5.92, for the polynomial form: C1=2.87e3 Pa, C3=3.83e4 Pa. The identification of the time parameter for the viscous contribution shows that it depends on the loading frequency (0.2 Hz: τ =0.257 s, 0.4 Hz: τ =0.123 s, 0.8 Hz: τ =0.050 s). Adding a viscous contribution significantly increases the accuracy of the identification.",Lung parenchyma Non-linear viscoelasticity Numerical identification Image registration Uniaxial tensile test Compressibility,
282,A Survey of Medical Image Registration on Multicore and the GPU,"Signal Processing Magazine, IEEE",2010,"In this article, we look at early, recent, and state-of-the-art methods for registration of medical images using a range of high-performance computing (HPC) architectures including symmetric multiprocessing (SMP), massively multiprocessing (MMP), and architectures with distributed memory (DM), and nonuniform memory access (NUMA). The article is designed to be self-sufficient. We will take the time to define and describe concepts of interest, albeit briefly, in the context of image registration and HPC. We provide an overview of the registration problem and its main components in the section Registration."" Our main focus will be HPC-related aspects",,
283,Object shape dependent PSF model for SPECT imaging,"Nuclear Science, IEEE Transactions on",1993,"An analytical expression for the point spread function (PSF) and the line spread function (LSF) of a parallel hole gamma camera is presented, for homogeneous media and for photons having mainly Compton interactions in the object. The PSF of scattered photons is described by convolving a zeroth-order modified Bessel function of the second kind with the unscattered PSF, which is approximated by a Gaussian. The complete PSF (scatter plus nonscatter) depends on the source distance ( z) and depth (d) of the source in the object. For convex-shaped emitting objects, the dependence of the PSF on the object contour can be incorporated by a simple correction. Thus, a complete mathematical model of the imaging of an activity distribution in a homogeneous medium is obtained. The model has been evaluated for 99mTechnetium line sources by using a LEAP collimator at various energy windows (ÎE). It is shown that the model is valid to a high accuracy at ÎE=15%, for a large range of values of z and d",cameras functions gamma-ray apparatus gamma-ray production radioisotopes technetium 99Tc line source Bessel function LEAP collimator SPECT imaging convex object homogeneous media line spread function object shape parallel hole gamma camera point spread function scattered photons source depth source distance Biomedical imaging Brain modeling Cameras Electromagnetic scattering Image reconstruction Mathematical model Optical imaging Particle scattering Shape measurement Single photon emission computed tomography,
284,Ultrasound color-flow imaging on a programmable system,"Information Technology in Biomedicine, IEEE Transactions on",2004,"Color-flow imaging is a well-established ultrasound mode and very valuable for visualizing in real time the distribution of blood flow in a specific region of interest. However, it is computationally quite expensive. To meet the large computational need in color-flow imaging, most ultrasound systems have been designed using fixed-function hardware. In this paper, we present a system where all the color-flow processing is supported on a programmable platform. About 95% of the processing modules were programmed in C language. On a single processor, we were able to achieve 7.9 frames/s, when the input data consist of 192 Ã 512 Ã 8 (ensemble size) samples for color flow and 384 Ã 512 for B mode and the output image size is 600 Ã 420. Additional processors can be added to handle more input data and/or support higher frame rates. Our results demonstrate that a programmable ultrasound system can provide the same functionality for clinical use as conventional ultrasound systems. However, it is more flexible and efficient due to its programmability.",C language biomedical ultrasonics flow visualisation haemodynamics medical signal processing program processors 192 pixel 384 pixel 420 pixel 512 pixel 600 pixel 8 pixel C language programming blood flow distribution blood-flow processing algorithm color-flow processing fixed-function hardware processing modules programmable platform programmable ultrasound system ultrasound color-flow imaging ultrasound mode Acoustic beams Acoustic imaging Biomedical imaging Computer architecture Field programmable gate arrays Hardware Real time systems Streaming media Ultrasonic imaging Visualization Algorithms Blood Flow Velocity Equipment Design Equipment Failure Analysis Image Enhancement Image Interpretation Computer-Assisted Reproducibility of Results Sensitivity and Specificity Signal Processing Computer-Assisted Ultrasonography Doppler Color,
285,On variable blocking factor in a parallel dynamic block-Jacobi SVD algorithm,Parallel Computing,2003,"The parallel two-sided block-Jacobi singular value decomposition SVD algorithm with dynamic ordering, originally proposed in (Parallel Comput. 28 (200) 243-262), has been extended with respect to the blocking factor l. Unlike the unique blocking factor l = 2p in the original algorithm running on p processors, the current blocking factor is a variable parameter that covers the values in two different regions-namely, l = p/k and l = 2kp for some integer k. Two new parallel two-sided block-Jacobi SVD algorithms with dynamic ordering are described in detail. They arise in those two regions and differ in the logical data arrangement and communication complexity of the reordering step. For the case of l = 2kp, it is proved that a designed point-to-point communication algorithm is optimal with respect to the amount of communication required per processor as well as to the amount of overall communication. Using the message passing programming model for distributed memory machines, new parallel block-Jacobi SVD algorithms were implemented on an SGI-Cray Origin 2000 parallel computer. Numerical experiments were performed on p = 12 and 24 processors using a set of six matrices of order 4000 and blocking factor l {2, p, 2p} can be recommended for matrices with distinct singular values. However, for matrices with a multiple minimal singular value, the total parallel execution time may monotonically increase with l. In this case, the recommended Jacobi method with l = 2 is just the ScaLAPACK routine with some additional matrix multiplications, and it computes the SVD in one parallel iteration step.",communication complexity message passing parallel algorithms singular value decomposition,
286,"Erratum to Continuous genetic networks"" [Parallel Comput. 27 (5) (2001) 663-683]""",Parallel Computing,2002,"The authors wish to correct a statement in their paper. The correction concerns the last eight lines of page 677, in paragraph 'Alternative B'. In the paper we noticed that, in the case of alternative B, the asymptotic behaviours which have been observed were fixed points. Indeed, after removal of a programming bug, we have been able to observe also cyclic behaviours. We confirm that fixed points are also found, with activation values which are intermediate between the extremes 0 and 1. The major difference between the cycles which are found in alternative B, with respect to those of alternative A, is that nodes often take activation values which are not on the corners of the unit hypercube. It is also worth mentioning that the presence of multiple cyclic attractors makes alternative B better suited for modelling gene regulatory networks than it were if it only admitted fixed point attractors. Alternative B is indeed a candidate to model gene networks where the activation of some genes takes intermediate values, a situation which is often found in real cells.",computer networks parallel programming,
287,Operational research techniques in medical treatment and diagnosis: A review,European Journal of Operational Research,2000,"Many modern techniques for the diagnosis of pathological states in humans and for their subsequent treatment can be posed as nonlinear identification problems of essentially nonlinear dynamic systems or as nonlinear optimal control problems. It can be shown that the linearised versions of such models are inadequate and do not represent at all well the complexity of the problem. Thus, nonlinear estimation and control techniques are required for progress to be made in this field. The aim of this review is to examine some models suggested in the medical literature for the modelling of certain medical treatments and diagnoses. Then examine how these models can be enriched by using Operational Research techniques so that a better control is provided on the diagnosis and the treatment, as well as the formulation of more precise models of the phenomenon. The review will present some applications both therapeutic and diagnostic that have appeared in the literature. Special interest will be bestowed on hyperthermic systems in oncological treatment and glucose–insulin dynamics for diabetic patients, while heart dynamics and magnetic resonance imaging will also receive attention. These applications are good examples to show the advantages of Operational Research methods in this field of endeavour. The outline of the paper is the following. After the introduction, in section two a brief description of nonlinear system models of phenomena will be given, for definitional and descriptive purposes. In section three a discussion of how to apply System theory in the medical field will be presented, together with an analysis of the possible benefits. In section four some applications of dynamical models to medical diagnosis and treatment will be described, while in section five the appropriate conclusions will be stated.",Dynamic systems Optimal control Medical modelling Simultaneous identification and optimization,
288,Computer assisted medical diagnosis using the Web,International Journal of Medical Informatics,1997,"The ADM (Aide au diagnostic Médical) project was started 15 years ago and was the first telematic project for physicians in France using the MINITEL terminal. The knowledge base contains information on more than 10 000 diseases from all pathological fields, using more than 100 000 signs or symptoms. The ADM system has two main functionalities for physicians: consultation of diseases descriptions and list of diseases containing one or more symptoms. The ADM knowledge base is supported by a relational database management system (DBMS ORACLE) and we developed a Web interface using the Perl language to produce HTML pages for the web server. We will describe our experience on redesigning a large existing medical knowledge base for diffusion on the web Internet.",ADM Computer assisted medical diagnosis World Wide Web,
289,Technology transfer within the ProHPC TTN at ENS Lyon,Future Generation Computer Systems,1999,"This article is devoted to the description of our activities related to transferring the HPCN technology to SMEs. This work is performed in the framework of the French TTN ProHPC which we briefly describe in the beginning of the paper. Then we move to a more technical description of our activities, which include seven projects within the awareness campaign PHPC-ACRA and the participation to a Best Practice prototype.",High-performance computing Parallel algorithms Parallel applications Industrial transfer Research and development SME,
290,Application of Quasi-Static Magnetic Reciprocity to Finite Element Models of the MEG Lead-Field,"Biomedical Engineering, IEEE Transactions on",2007,"The reconstruction of neuronal current sources from magneto- and/or electroencephalography (MEG/EEG) measurements is referred to as an inverse problem. A precursor to most inverse algorithms is a forward transfer, or lead-field, matrix, in which the rows correspond to MEG and/or EEG measurement sites, and each column captures the linear response to a particular unit source. Simple models of the head, such as concentric spheres, result in analytic expressions for the lead-field. More realistic head models, such as those based on medical imagery, require numeric simulations. A straightforward, though inefficient, way to obtain the lead-field is to perform one forward simulation for each source, resulting in one column of the lead-field. For MEG/EEG inverse problems, however, the potential sources (rows) far outnumber the measurement sites (columns). Two approaches have been described for computing the EEG lead-field with a number of forward simulations equal to the number of measurement rows, rather than the number of source columns. One of these approaches is based on the principle of electric reciprocity, and the other approach is based on linear-algebraic manipulations of the forward problem. For the MEG lead-field, only a linear-algebraic approach has been described for numeric approaches such as the finite element method. This paper describes a reciprocal approach for the MEG lead-field and discusses implementation details for both approaches.",electroencephalography finite element analysis inverse problems magnetoencephalography EEG MEG lead field electric reciprocity electroencephalography finite element model head model inverse problem magnetoencephalography medical imagery neuronal current source quasi-static magnetic reciprocity Brain modeling Computational modeling Current measurement Electroencephalography Finite element methods Image reconstruction Inverse problems Lead Magnetic heads Medical simulation Finite element methods inverse problems lead-field basis magnetoencephalography reciprocity Algorithms Brain Brain Mapping Computer Simulation Evoked Potentials Finite Element Analysis Humans Magnetoencephalography Models Neurological,
291,Geometric active curve for selective entropy optimization,Neurocomputing,2014," Recently, with the development of high dimensional large-scale medical imaging devices, the need of fast, robust and accurate segmentation methods is increasing. In this paper, we propose a new level set method (LSM) for image segmentation. The basic idea is to design a selective entropy-based energy functional which is effective and robust against noise, from which we will derive the level set equations and a new selective entropy external forces for the lattice Boltzmann D2Q5 partial differential equation (PDE) solver. The method is accurate and highly parallelizable. The local nature of the lattice Boltzmann method (LBM) allows it to be suitable for fast segmentation methods implemented using some parallel devices such as the graphics processing unit. The proposed algorithm is effective, robust against noise and highly parallelizable. Furthermore, the method can easily be extended to perform an effective image filtering based on Gaussian fuzzy selection. Experimental results on medical images demonstrate subjectively and objectively the performance of the proposed method.",Lattice Boltzmann method Selective entropy Level set method Image segmentation Graphics processing units,
292,Editorial commentary: Aims and tasks of medical informatics,International Journal of Medical Informatics,1997,,,
293,Image multi-thresholding by combining the lattice Boltzmann model and a localized level set algorithm,Neurocomputing,2012,"During the last decades, the development of high dimensional large-scale imaging devices increases the need of fast, accurate and parallelizable segmentation methods. Due to its intrinsic advantages such as its ability to handle complex shapes, the level set method (LSM) has been widely used. Nevertheless, the method is computational expensive in image segmentation, which limits its use in real-time systems and volume images segmentation. In this paper we propose an adaptive image multi-thresholding method which uses a localized level set method to detect automatically the best thresholds values from some initial given values. Instead to solve the level set equation (LSE) by using the traditional methods based on some finite difference or finite volume, we use the highly parallelizable lattice Boltzmann method (LBM). All the more, the method is faster since it is solved in histogram domain rather than the pixel domain. The time complexity is therefore considerably reduced since the number of gray levels is generally much smaller than the size of the image. The method is efficient, highly parallelizable and faster than those based on the LSM. Experiments on synthetic, real-world, medical and man-made object images demonstrate the performance of the proposed method.",Level set method Lattice Boltzmann model Image histogram Multi-thresholding Image segmentation,
294,Commentary on Reinhold Haux: Aims and tasks of medical informatics,International Journal of Medical Informatics,1997,,,
295,Accelerating geostatistical simulations using graphics processing units (GPU),Computers & Geosciences,2012,"Geostatistical simulations have become a widely used tool for modeling of oil and gas reservoirs and the assessment of uncertainty. One important current issue is the development of high-resolution models in a reasonable computational time. A possible solution is based on taking advantage of parallel computational strategies. In this paper we present a new methodology that exploits the benefits of graphics processing units (GPUs) along with the master–slave architecture for geostatistical simulations that are based on random paths. The methodology is a hybrid method in which different levels of master and slave processors are used to distribute the computational grid points and to maximize the use of multiple processors utilized in GPU. It avoids conflicts between concurrently simulated grid points, an important issue in high-resolution and efficient simulations. For the sake of comparison, two distinct parallelization methods are implemented, one of which is specific to pattern-based simulations. To illustrate the efficiency of the method, the algorithm for the simulation of pattern is adapted with the GPU. Performance tests are carried out with three large grid sizes. The results are compared with those obtained based on simulations with central processing units (CPU). The comparison indicates that the use of GPUs reduces the computation time by a factor of 26–85.",Graphics processing units (GPU) Geostatistical simulation Random paths Parallelization Compute unified device architecture (CUDA),
296,Radar-based road-traffic monitoring in urban environments,Digital Signal Processing,2013,"This work presents a novel approach to object detection and tracking in urban environments using images obtained from a radar network, deployed in an urban environment. The proposed system detects, tracks and computes the speed of vehicles and generates alerts when vehicles exceed the predefined road speed limit. The available radar model is a low-cost device oriented to marine environments rather than terrestrial applications. For this reason, we emphasize in the development of a realistic, robust, efficient and effective algorithm which deals with the hardware limitations to provide a suitable overall performance. To reach this objective, we propose dual background subtraction model to detect objects and a tracking method based on the particle filter algorithm. Furthermore, to ensure real time restriction even in HD imagery, our method takes advantage in a natural way of multicore systems and exploits advanced SIMD capabilities available in last multicore processors families. Experimental results demonstrate that the proposed system is able to detect and track multiple objects and to provide speeding alarms when needed. It is also capable to handle target occlusions and disappearances derived from the radar limitations and the noisy urban environment.",Radar processing Real time performance Visual tracking Particle filters,
297,Image Noise Removal on Heterogeneous CPU-GPU Configurations,Procedia Computer Science,2014," A parallel algorithm to remove impulsive noise in digital images using heterogeneous CPU/GPU computing is proposed. The parallel denoising algorithm is based on the peer group concept and uses an Euclidean metric. In order to identify the amount of pixels to be allocated in multi-core and GPUs, a performance analysis using large images is presented. A comparison of the parallel implementation in multi-core, GPUs and a combination of both is performed. Performance has been evaluated in terms of execution time and Megapixels/second. We present several optimization strategies especially effective for the multi-core environment, and demonstrate significant performance improvements. The main advantage of the proposed noise removal methodology is its computational speed, which enables efficient filtering of color images in real-time applications.",parallel computing noise removal in images GPU CUDA multi-core OpenMP,
298,A Parallel Method for Impulsive Image Noise Removal on Hybrid CPU/GPU Systems,Procedia Computer Science,2013," A parallel algorithm for image noise removal is proposed. The algorithm is based on peer group concept and uses a fuzzy metric. An optimization study on the use of the CUDA platform to remove impulsive noise using this algorithm is presented. Moreover, an implementation of the algorithm on multi-core platforms using OpenMP is presented. Performance is evaluated in terms of execution time and a comparison of the implementation parallelised in multi-core, GPUs and the combination of both is conducted. A performance analysis with large images is conducted in order to identify the amount of pixels to allocate in the CPU and GPU. The observed time shows that both devices must have work to do, leaving the most to the GPU. Results show that parallel implementations of denoising filters on GPUs and multi-cores are very advisable, and they open the door to use such algorithms for real-time processing.",Parallel computing Noise removal in images GPU CUDA Multi-core OpenMP,
299,Unstructured mesh generation from the Virtual Family models for whole body biomedical simulations,Procedia Computer Science,2010,"Physiological systems are inherently complex, involving multi-physics phenomena at a multitude of spatial and temporal scales. To realistically simulate their functions, detailed high quality multi-resolution often patient specific human models are required. Mesh generation has remained a central topic in finite element analysis (FEA) for a few decades now. Recent developments in high performance computing (HPC) driven by the need for multi-physics multiscale simulations of physiological systems define new challenges in this area. Even though many algorithms have been developed over years and are offered as commercial packages, they are often limited to mechanical engineering applications only. Mesh generation for human anatomical domains requires more effective and flexible techniques to tackle their greater geometrical and topological complexities. We present, evaluate and discuss several methods to generate unstructured body fitted multi-domain adaptive meshes with geometrically and topologically compatible interfaces from the segmented cross-sections of the Virtual Family models for the purpose of large scale whole body simulations. We found that an automated solution is difficult to achieve with real-image qualities, but if optimal methods are selected, good results can be achieved with minimal user-interactions. Therefore we believe that our observations can serve as guidance when choosing an optimal method for a specific application.",Biomedical simulation Whole body anatomical model Unstructured mesh generation FEA FEM Compatible triangulation Multi-label medical image data Multi-physics multi-scale simulation,
300,Linear-time connected-component labeling based on sequential local operations,Computer Vision and Image Understanding,2003,"This paper presents a fast algorithm for labeling connected components in binary images based on sequential local operations. A one-dimensional table, which memorizes label equivalences, is used for uniting equivalent labels successively during the operations in forward and backward raster directions. The proposed algorithm has a desirable characteristic: the execution time is directly proportional to the number of pixels in connected components in an image. By comparative evaluations, it has been shown that the efficiency of the proposed algorithm is superior to those of the conventional algorithms.",Labeling Connected component Linear time Sequential local operation One-dimensional table Raster scan order,
301,On the use of subword parallelism in medical image processing,Parallel Computing,1998,"Parallel implementations of algorithms for medical image processing mostly focus on the use of multiprocessor parallelism. Modern processor architectures however, provide several additional forms of parallelism at the processor level: subword parallelism, speculative execution, superscalar pipelining, very long instruction word, etc. In this article, we show that well-known parallelization techniques for multiprocessor systems can be used to exploit subword parallelism. Loop unrolling, loop fusion and if-hoisting prove to be valuable to achieve this goal. To illustrate this, we transformed the inner loops of a positron emission tomography image reconstruction algorithm. We achieved a speed-up of 45% on Sun's UltraSPARC processor.",Subword parallelism Loop transformations Positron emission tomography,
302,An fMRI framework for identifying statistical differences in blood oxygenated level dependent response levels: A brain injury demonstration,Artificial Intelligence in Medicine,2007,"SummaryObjective The general concept surrounding fMRI data analysis for decision support is leveraging previously hidden knowledge from publicly available metadata sources with a high degree of precision. Methods and materials Normalized fMRI scans are used to calculate cumulative voxel intensity curves for every subject in the dataset that fits chosen demographic criteria. The voxel intensity curve has a direct linear relationship to the subject's neuronal activity. In the case of head trauma, a subject's voxel intensity curve would be statistically compared to the weighted average curve for every subject in dataset that is demographically similar. If the new subject's neuronal activity falls below the threshold for their demographic group, the brain injury detection (BID) system would then pinpoint the areas of deficiency based on Broadmann's cortical areas. Analysis The analysis presented in this paper indicates that statistical differences among demographic groups exist in BOLD fMRI responses. Conclusion Useful knowledge can in fact be leveraged from mining stockpiled fMRI data without the need for unique human identifiers. The BID system offers the radiologist a statistically based decision support for brain injury.",Data mining fMRI Decision support Bioinformatics Medical imaging,
303,GPU-accelerated MRF segmentation algorithm for SAR images,Computers & Geosciences,2012,"Markov Random Field (MRF) approaches have been widely studied for Synthetic Aperture Radar (SAR) image segmentation, but they have a large computational cost and hence are not widely used in practice. Fortunately parallel algorithms have been documented to enjoy signiﬁcant speedups when ported to run on a graphics processing units (GPUs) instead of a standard CPU. Presented here is an implementation of graphics processing units in General Purpose Computation (GPGPU) for SAR image segmentation based on the MRF method, using the C-oriented Compute Unified Device Architecture (CUDA) developed by NVIDIA. This experiment with GPGPU shows that the speed of segmentation can be increased by a factor of 10 for large images.",GPU GPGPU CUDA SAR image segmentation MRF,
304,Towards a grid system for medical image reconstruction,,2007,"This paper presents an experimental grid system-the MIRGrid-which enables transparent usage of highperformance computers for medical image reconstruction. MIR-Grid provides a comfortable way to perform time-consuming iterative image reconstructions on interconnected high-performance computers; it covers the whole imaging workflow: from reading the raw data acquired by the scanner, over user-transparent parallel reconstruction to visualization. The system is able to perform and monitor parallel reconstructions on different kinds of parallel architectures with shared and distributed memory, as well as combinations of both. In order to optimally distribute the reconstructions among the high-performance computers, we estimate parallel reconstruction time by using a previously developed performance model. Additionally to traditional 3D imaging, the system seamlessly integrates dynamic (4D) and gated studies. The system optimizes parallel 4D reconstruction time by partitioning dynamic and gated reconstructions into independent 3D sub-reconstructions that are computed simultaneously on several highperformance computers.",,
305,Fluid flow simulation on the Cell Broadband Engine using the lattice Boltzmann method,Computers & Mathematics with Applications,2009,"In this paper we present a fast lattice Boltzmann fluid solver that has been performance optimized and tailored for the Cell Broadband Engine Architecture. Many design decisions were motivated by the long range objective to simulate blood flow in human blood vessels, especially in aneurysms, but have proven to be much more generally applicable. After explaining implementation details and how they were influenced by the target platform, the performance and memory requirements of this prototype solver are evaluated.",Cell processor CBEA Blood flow Lattice Boltzmann Hemodynamics Aneurysm High performance computing,
306,Highly interactive distributed visualization,Future Generation Computer Systems,2006,"We report on our iGrid2005 demonstration, called the “Dead Cat Demo”; an example of a highly interactive augmented reality application consisting of software services distributed over a wide-area, high-speed network. We describe our design decisions, analyse the implications of the design on application performance and show performance measurements.",Highly interactive visualization Distributed visualization Augmented Reality,
307,A parts-and-geometry initialiser for 3D non-rigid registration using features derived from spin images,Neurocomputing,2013," Non-rigid registration is an important precursor to statistical analysis and machine learning in medical image analysis. It is commonly used to find correspondences between images which is a necessary first step for further processing. However, registering images which have large pose differences and/or are composed of substructures of similar appearance requires that registration be initialised carefully for the results to be valid. This work addresses both problems in the context of 3D volumetric images. We use parts-and-geometry models to automatically align images before registration proceeds. An important component of the parts are orientation-invariant descriptors computed using spin images. In the following we describe the construction of the parts-and-geometry models and how they can be incorporated into non-rigid registration. We use 3D CT images of the wrist and knee to demonstrate the effectiveness of the models at locating substructures with similar appearance, and show both qualitatively and quantitatively that initialisation with parts-and-geometry models improve the accuracy of registration.",Spin images MRF Parts-and-geometry Registration,
308,A scalable pipelined architecture for real-time computation of MLP-BP neural networks,Microprocessors and Microsystems,2012,"In this paper a novel architecture for implementing multi-layer perceptron (MLP) neural networks on field programmable gate arrays (FPGA) is presented. The architecture presents a new scalable design that allows variable degrees of parallelism in order to achieve the best balance between performance and FPGA resources usage. Performance is enhanced using a highly efficient pipelined design. Extensive analysis and simulations have been conducted on four standard benchmark problems. Results show that a minimum performance boost of three orders of magnitude (O3) over software implementation is regularly achieved. We report performance of 2–67 GCUPS for these simple problems, and performance reaching over 1 TCUPS for larger networks and different single FPGA chips. To our knowledge, this is the highest speed reported to date for any MLP network implementation on FPGAs.",Field programmable gate arrays Parallel computing Artificial Neural Networks Multi-layer perceptron Scalability Hardware accelerators On-line learning,
309,Performance evaluation of a parallel sparse lattice Boltzmann solver,Journal of Computational Physics,2008,"We develop a performance prediction model for a parallelized sparse lattice Boltzmann solver and present performance results for simulations of flow in a variety of complex geometries. A special focus is on partitioning and memory/load balancing strategy for geometries with a high solid fraction and/or complex topology such as porous media, fissured rocks and geometries from medical applications. The topology of the lattice nodes representing the fluid fraction of the computational domain is mapped on a graph. Graph decomposition is performed with both multilevel recursive-bisection and multilevel k-way schemes based on modified Kernighan–Lin and Fiduccia–Mattheyses partitioning algorithms. Performance results and optimization strategies are presented for a variety of platforms, showing a parallel efficiency of almost 80% for the largest problem size. A good agreement between the performance model and experimental results is demonstrated.",Sparse lattice Boltzmann Partitioning METIS MPI performance measurements Optimization Performance prediction,
310,Dynamic replication in a data grid using a Modified BHR Region Based Algorithm,Future Generation Computer Systems,2011,"Grid computing is emerging as a key part of the infrastructure for a wide range of disciplines in science and engineering, including astronomy, high energy physics, molecular biology and earth sciences. These applications handle large data sets that need to be transferred and replicated among different grid sites. A data grid deals with data intensive applications in scientific and enterprise computing. Data grid technology is developed to permit data sharing across many organizations in geographically disperse locations. Replication of data to different sites will help researchers around the world analyse and initiate future experiments. The general idea of replication is to store copies of data in different locations so that data can be easily recovered if a copy at one location is lost or unavailable. In a large-scale data grid, replication provides a suitable solution for managing data files, which enhances data reliability and availability. In this paper, a Modified BHR algorithm is proposed to overcome the limitations of the standard BHR algorithm. The algorithm is simulated using a data grid simulator, OptorSim, developed by European Data Grid projects. The performance of the proposed algorithm is improved by minimizing the data access time and avoiding unnecessary replication.",Grid computing Data grid Static replication Dynamic replication,
311,Accelerated hardware video object segmentation: From foreground detection to connected components labelling,Computer Vision and Image Understanding,2010,"This paper demonstrates the use of a single-chip FPGA for the segmentation of moving objects in a video sequence. The system maintains highly accurate background models, and integrates the detection of foreground pixels with the labelling of objects using a connected components algorithm. The background models are based on 24-bit RGB values and 8-bit gray scale intensity values. A multimodal background differencing algorithm is presented, using a single FPGA chip and four blocks of RAM. The real-time connected component labelling algorithm, also designed for FPGA implementation, run-length encodes the output of the background subtraction, and performs connected component analysis on this representation. The run-length encoding, together with other parts of the algorithm, is performed in parallel; sequential operations are minimized as the number of run-lengths are typically less than the number of pixels. The two algorithms are pipelined together for maximum efficiency.",Background differencing Image segmentation Connected component labelling Object extraction FPGA,
312,Combining high-performance computing and networking for advanced 3-D cardiac imaging,Ieee Transactions on Information Technology in Biomedicine,2000,"This paper deals with the integration of a powerful parallel computer-based image analysis and visualization system for cardiology into a hospital information system. Further services are remote access to the hospital Web server through an internet network. The visualization system includes dynamic three-dimensional representation of two types of medical images (e.g,, magnetic resonance and nuclear medicine) as well as two images in the same modality (e.g., basal versus stress images). A series of software tools for quantitative image analysis developed for supporting diagnosis of cardiac disease are also available, including automated image segmentation and quantitative time evaluation of left ventricular volumes and related indices during cardiac cycle, myocardial mass, and myocardial perfusion indices. The system has been tested both at a specialized cardiologic center and for remote consultation in diagnosis of cardiac disease by using anatomical and perfusion magnetic resonance images.",,
313,"Algorithm, software, and hardware optimizations for Delaunay mesh generation on simultaneous multithreaded architectures",Journal of Parallel and Distributed Computing,2009,"This article focuses on the optimization of PCDM, a parallel, two-dimensional (2D) Delaunay mesh generation application, and its interaction with parallel architectures based on simultaneous multithreading (SMT) processors. We first present the step-by-step effect of a series of optimizations on performance. These optimizations improve the performance of PCDM by up to a factor of six. They target issues that very often limit the performance of scientific computing codes. We then evaluate the interaction of PCDM with a real SMT-based SMP system, using both high-level metrics, such as execution time, and low-level information from hardware performance counters.",Parallel Mesh Generation SMT Optimizations Finite element,
314,Comments on parallel algorithms for the knapsack problem,Parallel Computing,2002,"Chang et al. [Parallel Comput. (1994) 233] introduced a parallel algorithm based on a shared memory SIMD architecture for the generation phase of the classic Horowitz and Sahni [J. ACM 21(2) (1974) 277] two-list serial algorithm for the knapsack problem. They claimed that their parallel generation phase could be accomplished in time O((n/8)2) and in space O(2n/4) with O(2n/8) processors. We prove that their results are not correct, i.e., that the suggested scheme time and space complexity should be bounded, instead, by O(n2n/2) and O(2n/2), respectively. These results also invalidate the performance analysis of the more recent Lou and Chang [Parallel Comput. (1997) 1985] algorithm. 2002 Elsevier Science B.V. All rights reserved.",Parallel algorithms Data storage equipment Parallel processing systems Performance Program processors,
315,A multigrain Delaunay mesh generation method for multicore SMT-based architectures,Journal of Parallel and Distributed Computing,2009,"Given the proliferation of layered, multicore- and SMT-based architectures, it is imperative to deploy and evaluate important, multi-level, scientific computing codes, such as meshing algorithms, on these systems. We focus on Parallel Constrained Delaunay Mesh (PCDM) generation. We exploit coarse-grain parallelism at the subdomain level, medium-grain at the cavity level and fine-grain at the element level. This multi-grain data parallel approach targets clusters built from commercially available SMTs and multicore processors. The exploitation of the coarser degree of granularity facilitates scalability both in terms of execution time and problem size on loosely-coupled clusters. The exploitation of medium-grain parallelism allows performance improvement at the single node level. Our experimental evaluation shows that the first generation of SMT cores is not capable of taking advantage of fine-grain parallelism in PCDM. Many of our experimental findings with PCDM extend to other adaptive and irregular multigrain parallel algorithms as well.",Parallel Mesh Generation Delaunay Multigrain Multicore SMT,
316,High performance computing for deformable image registration: Towards a new paradigm in adaptive radiotherapy,Medical Physics,2008,"The advent of readily available temporal imaging or time series volumetric (4D) imaging has become an indispensable component of treatment planning and adaptive radiotherapy (ART) at many radiotherapy centers. Deformable image registration (DIR) is also used in other areas of medical imaging, including motion corrected image reconstruction. Due to long computation time, clinical applications of DIR in radiation therapy and elsewhere have been limited and consequently relegated to offline analysis. With the recent advances in hardware and software, graphics processing unit (GPU) based computing is an emerging technology for general purpose computation, including DIR, and is suitable for highly parallelized computing. However, traditional general purpose computation on the GPU is limited because the constraints of the available programming platforms. As well, compared to CPU programming, the GPU currently has reduced dedicated processor memory, which can limit the useful working data set for parallelized processing. We present an implementation of the demons algorithm using the NVIDIA 8800 GTX GPU and the new CUDA programming language. The GPU performance will be compared with single threading and multithreading CPU implementations on an Intel dual core 2.4 GHz CPU using the C programming language. CUDA provides a C-like language programming interface, and allows for direct access to the highly parallel compute units in the GPU. Comparisons for volumetric clinical lung images acquired using 4DCT were carried out. Computation time for 100 iterations in the range of 1.8-13.5 s was observed for the GPU with image size ranging from 2.0x10(6) to 14.2x10(6) pixels. The GPU registration was 55-61 times faster than the CPU for the single threading implementation, and 34-39 times faster for the multithreading implementation. For CPU based computing, the computational time generally has a linear dependence on image size for medical imaging data. Computational efficiency is characterized in terms of time per megapixels per iteration (TPMI) with units of seconds per megapixels per iteration (or spmi). For the demons algorithm, our CPU implementation yielded largely invariant values of TPMI. The mean TPMIs were 0.527 spmi and 0.335 spmi for the single threading and multithreading cases, respectively, with < 2% variation over the considered image data range. For GPU computing, we achieved TPMI=0.00916 spmi with 3.7% variation, indicating optimized memory handling under CUDA. The paradigm of GPU based real-time DIR opens up a host of clinical applications for medical imaging. (c) 2008 American Association of Physicists in Medicine.",,
317,A two-phase decision support framework for the automatic screening of digital fundus images,Journal of Computational Science,2012,"In this paper we give a brief review on the present status of automated detection systems describe for the screening of diabetic retinopathy. We further detail an enhanced detection procedure that consists of two steps. First, a pre-screening algorithm is considered to classify the input digital fundus images based on the severity of abnormalities. If an image is found to be seriously abnormal, it will not be analysed further with robust lesion detector algorithms. As a further improvement, we introduce a novel feature extraction approach based on clinical observations. The second step of the proposed method detects regions of interest with possible lesions on the images that previously passed the pre-screening step. These regions will serve as input to the specific lesion detectors for detailed analysis. This procedure can increase the computational performance of a screening system. Experimental results show that both two steps of the proposed approach are capable to efficiently exclude a large amount of data from further processing, thus, to decrease the computational burden of the automatic screening system.",Biomedical image processing Medical decision-making Medical expert systems,
318,An intelligent system for the diagnosis of complex images,Artificial Intelligence in Medicine,1996,"An intelligent system suitable to perform a computer aided diagnosis of complex images should have a knowledge base containing all information related both to the images to be interpreted and to their symbolic description. In this paper, a system able to classify unknown medical digital images into four classes is proposed (searched pathology recognized, searched pathology absent, different pathology from the searched one recognized, unknown pathology). A main component of this system is a knowledge base that, starting from information deduced from sample images, can be processed to create synthetic reference models that, in turn, permit the interpretation of real scenes. The system has been tested on digitized plain film of thorax, in order to perform a computer-aided diagnosis of pneumothorax cases.",Image interpretation Diagnostic system Knowledge acquisition Knowledge processing Intelligent systems Pneumothorax,
319,Discrete wavelet transform based image fusion and de-noising in FPGA,Journal of Electrical Systems and Information Technology,2014," Image fusion is an extensively discussed topic for improving the information content of images. The main objective of image fusion algorithm is to combine information from multiple images of a scene. The result of image fusion is a new image which is more feasible for human and machine perception for further image processing operations such as segmentation, feature extraction and object recognition. This paper explores the possibility of using the specialized wavelet approach in image fusion and de-noising. These algorithms are compared on digital microscope images. The approach uses an affine transform based image registration followed by wavelet fusion. Then the least squares support vector machine based frequency band selection for image denoising can be incorporated to reduce the artifacts. The indentations are to maximize resolution, decrease artifacts and blurring in the final super image. To accelerate the entire operations, it is proposed to offload the image processing algorithms to a hardware platform thereby the performance can be improved. FPGAs provide an excellent platform in implementing real time image processing applications, since inherent parallelism of the architecture can be exploited explicitly. Image processing tasks executed on FPGAs can be up to 2 orders of magnitude faster than the equivalent application on a general purpose computer.",Discrete wavelet transform Image fusion Image registration Image denoising FPGA,
320,A massively parallel approach to deformable matching of 3D medical images via stochastic differential equations,Parallel Computing,2005,"The deformable matching of 3D medical images remains a difficult problem due to the high dimension of both geometric transformations and data. The matching problem is usually expressed as the minimization of a highly non-linear energy (objective) function, yielding a hard, computationally intensive, optimization problem. This paper presents a comprehensive parallel approach that yields computation times compatible with clinical routine. The image matching is based on the simulation of stochastic differential equations, enabling the optimization of the global objective function, through an annealing process. The resulting algorithm allows a fully parallel sampling of the parameters to be optimized. Due to the large number of parameters involved in deformable matching, this approach is naturally suited to massively parallel implementations. We present implementation issues and timing analysis on an MIMD parallel processing computer (SGI Origin 2000). The performances of the approach are assessed on real data, using 3D brain MR images from different individuals. Beside yielding accurate registrations, the parallel algorithm exhibits excellent relative speedups.",Deformable image matching Global optimization Stochastic differential equations Massive parallelism,
321,50 Years of object recognition: Directions forward,Computer Vision and Image Understanding,2013," Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems. Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems, assembly-line industrial inspection systems, as well as chip defect identification systems. It has also led to significant advances in medical imaging, defence and biometrics. In this paper we discuss the evolution of computer-based object recognition systems over the last fifty years, and overview the successes and failures of proposed solutions to the problem. We survey the breadth of approaches adopted over the years in attempting to solve the problem, and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations, while simultaneously leading to efficient learning and inference algorithms. From the earliest systems which dealt with the character recognition problem, to modern visually-guided agents that can purposively search entire rooms for objects, we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can. At the same time, however, we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system. We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations, as well as the learning and inference algorithms used.",Active vision Object recognition Object representations Object learning Dynamic vision Cognitive vision systems,
322,Connected operators,"Signal Processing Magazine, IEEE",2009,"Connected operators are filtering tools that act by merging elementary regions called flat zones. Connecting operators cannot create new contours nor modify their position. Therefore, they have very good contour-preservation properties and are capable of both low-level filtering and higher-level object recognition. This article gives an overview on connected operators and their application to image and video filtering. There are two popular techniques used to create connected operators. The first one relies on a reconstruction process. The operator involves first a simplification step based on a classical"" filter and then a reconstruction process. In fact", i.e.,
323,Content-adaptive reliable robust lossless data embedding,Neurocomputing,2012,"It is well known that robust lossless data embedding (RLDE) methods can be used to protect copyright of digital images when the intactness of host images is highly demanded and the unintentional attacks may be encountered in data communication. However, the existing RLDE methods cannot be applied satisfactorily to the practical scenarios due to different drawbacks, e.g., serious “salt-and-pepper” noise, low capacity and unreliable reversibility. In this paper, we propose an effective solution to RLDE by improving the histogram rotation (HR)-based embedding model. The proposed method is a content-adaptive reliable RLDE or CAR for short. It eliminates the “salt-and-pepper” noise in HR by the pixel adjustment mechanism. Therefore, reliable regions for embedding can be well constructed. Furthermore, we basically expect the watermark strengths to be adaptive to different image contents, and thus we have a chance to make an effective tradeoff between invisibility and robustness. The luminance masking together with the threshold strategy is duly adopted in the proposed RLDE method, so the just noticeable distortion thresholds of different local regions can be well utilized to control the watermark strengths. Experimental evidence on 300 test images including natural, medical and synthetic aperture radar (SAR) images demonstrates the effectiveness of the proposed data embedding method.",Histogram rotation Just noticeable distortion Robust lossless data embedding,
324,Robust lossless data hiding using clustering and statistical quantity histogram,Neurocomputing,2012,"Lossless data hiding methods usually fail to recover the hidden messages completely when the watermarked images are attacked. Therefore, the robust lossless data hiding (RLDH), or the robust reversible watermarking technique, is urgently needed to effectively improve the recovery performance. To date a couple of methods have been developed; however, they have such drawbacks as poor visual quality and low capacity. To solve this problem, we develop a novel statistical quantity histogram shifting and clustering-based RLDH method or SQH-SC for short. The benefits of SQH-SC in comparison with existing typical methods include: (1) strong robustness against lossy compression and random noise due to the usage of k-means clustering; (2) good imperceptibility and reasonable performance tradeoff due to the consideration of the just noticeable distortion of images; (3) high capacity due to the flexible adjustment of the threshold; and (4) wide adaptability and good stability to different kinds of images. Extensive experimental studies based on natural images, medical images, and synthetic aperture radar (SAR) images demonstrate the effectiveness of the proposed SQH-SC.",Just noticeable distortion k-Means clustering Robust lossless data hiding Statistical quantity histogram,
325,Towards clinical applicability of the diffusion-based DT-MRI visualization algorithm,Journal of Visual Communication and Image Representation,2012,"For the purpose of DT-MRI data visualization, an algorithm based on a numerical model of texture diffusion is proposed. As a prerequisite of entering clinical use, its parameters need to be adjusted properly so that the procedure gives satisfactory results with limited computational resources and time available. This contribution introduces the principles of the method and reports on the results of extensive computational studies aimed at finding optimal settings of the numerical scheme and model parameters with respect to visualization purposes. Total variation is used as a measure of visual quality of the produced images. Further, we provide evidence that using the algorithm is fully feasible on state of the art hardware. Finally, high resolution visualizations based on real data are demonstrated.",Biomedical magnetic resonance imaging Diffusion equations Computational study Parallel processing Scientific visualization Diffusion tensor Numerical solution Total variation,
326,Accelerating advanced MRI reconstructions on GPUs,Journal of Parallel and Distributed Computing,2008,"Computational acceleration on graphics processing units (GPUs) can make advanced magnetic resonance imaging (MRI) reconstruction algorithms attractive in clinical settings, thereby improving the quality of MR images across a broad spectrum of applications. This paper describes the acceleration of such an algorithm on NVIDIA’s Quadro FX 5600. The reconstruction of a 3D image with 1283 voxels achieves up to 180 GFLOPS and requires just over one minute on the Quadro, while reconstruction on a quad-core CPU is twenty-one times slower. Furthermore, for the data set studied in this article, the percent error exhibited by the advanced reconstruction is roughly three times lower than the percent error incurred by conventional reconstruction techniques.",GPU computing MRI Reconstruction CUDA,
327,High-level Programming for Medical Imaging on Multi-GPU Systems Using the SkelCL Library,Procedia Computer Science,2013," Application development for modern high-performance systems with Graphics Processing Units (GPUs) relies on low-level programming approaches like CUDA and OpenCL, which leads to complex, lengthy and error-prone programs. In this paper, we present SkelCL – a high-level programming model for systems with multiple GPUs and its implementa- tion as a library on top of OpenCL. SkelCL provides three main enhancements to the OpenCL standard: 1) computations are conveniently expressed using parallel patterns (skeletons); 2) memory management is simplified using parallel container data types; 3) an automatic data (re)distribution mechanism allows for scalability when using multi-GPU systems. We use a real-world example from the field of medical imaging to motivate the design of our programming model and we show how application development using SkelCL is simplified without sacrificing performance: we were able to reduce the code size in our imaging example application by 50% while introducing only a moderate runtime overhead of less than 5%.",SkelCL Multi-GPU Computing Algorithmic Skeletons LM OSEM Algorithm Image Reconstruction,
328,Grid powered nonlinear image registration with locally adaptive regularization,Medical Image Analysis,2004,"Multi-subject non-rigid registration algorithms using dense deformation fields often encounter cases where the transformation to be estimated has a large spatial variability. In these cases, linear stationary regularization methods are not sufficient. In this paper, we present an algorithm that uses a priori information about the nature of imaged objects in order to adapt the regularization of the deformations. We also present a robustness improvement that gives higher weight to those points in images that contain more information. Finally, a fast parallel implementation using networked personal computers is presented. In order to improve the usability of the parallel software by a clinical user, we have implemented it as a grid service that can be controlled by a graphics workstation embedded in the clinical environment. Results on inter-subject pairs of images show that our method can take into account the large variability of most brain structures. The registration time for images of size 256 × 256 × 124 is 5 min on 15 standard PCs. A comparison of our non-stationary visco-elastic smoothing versus solely elastic or fluid regularizations shows that our algorithm converges faster towards a more optimal solution in terms of accuracy and transformation regularity.",Image registration Non-rigid transformation Nonlinear diffusion Adaptive regularization Parallel computing Grid computing Brain atlas Multi-subject image fusion,
329,Algorithm for hyperfast cone-beam spiral backprojection,Computer Methods and Programs in Biomedicine,2010,"Cone-beam spiral backprojection is computationally highly demanding. At first sight, the backprojection requirements are similar to those of cone-beam backprojection from circular scans such as it is performed in the widely used Feldkamp algorithm. However, there is an additional complication: the illumination of each voxel, i.e. the range of angles the voxel is seen by the X-ray cone is a complex function of the voxel position. The weight function has no analytically closed form and must be numerically determined. Storage of the weights is prohibitive since the amount of memory required equals the number of voxels per spiral rotation times the number of projections a voxel receives contributions and therefore is in the order of 1 0 9 to 1 0 11 floating point values for typical spiral scans. We propose a new algorithm that combines the spiral symmetry with the ability of today’s 64 bit CPUs to store large amounts of precomputed weights. Using the spiral symmetry in this way allows to exploit data-level parallelism and thereby to achieve a very high level of vectorization. An additional postprocessing step rotates these slices back to normal images. Our new backprojection algorithm achieves up to 24.6 Giga voxel updates per second (GUPS) on our systems that are equipped with two standard Intel X5570 quad core CPUs (Intel Xeon 5500 platform, 2.93 GHz, Intel Corporation). This equals the reconstruction of 410 images per second assuming each slice consists of 512 × 512 pixels, receiving contributions from 512 projections.",CT Image reconstruction Backprojection Spiral CT High performance computing,
330,High performance computing in biomedical imaging research,Parallel Computing,1998,"The Mayo Biomedical Imaging Resource (BIR) conducts research into and development of image analysis, visualization, and measurement capabilities and software tools for biomedical imaging applications. The design goal for these tools includes full interactivity, yet some tools are both compute bound and time sensitive. Therefore, effective use of these capabilities requires that they be executed on high performance computers. This paper provides an overview of the high performance computing activities in the BIR, including resources, algorithms and applications.",Biomedical imaging Image processing Image visualization Image registration Image segmentation,
331,Extraction of loess shoulder-line based on the parallel GVF snake model in the loess hilly area of China,Computers & Geosciences,2013,"Loess shoulder-lines are the most critical terrain feature in representing and modeling the landforms of the Loess Plateau of China. Existing algorithms usually fail in obtaining a continuous shoulder-line for complicated surface, DEM quality and algorithm limitation. This paper proposes a new method, by which gradient vector flow (GVF) snake model is employed to generate an integrated contour which could connect the discontinuous fragments of shoulder-line. Moreover, a new criterion for the selection of initial seeds is created for the snake model, which takes the value of median smoothing of the local neighborhood regions. By doing this, we can  the adjacent boundary of loess positive–negative terrains from the shoulder-line zones, which build a basis to found the real shoulder-lines by the gradient vector flow. However, the computational burden of this method remains heavy for large DEM dataset. In this study, a parallel computing scheme of the cluster for automatic shoulder-line extraction is proposed and implemented with a parallel GVF snake model. After analyzing the principle of the method, the paper develops an effective parallel algorithm integrating both single program multiple data (SPMD) and master/slave (M/S) programming modes. Based on domain decomposition of DEM data, each partition is decomposed regularly and calculated simultaneously. The experimental results on different DEM datasets indicate that parallel programming can achieve the main objective of distinctly reducing execution time without losing accuracy compared with the sequential model. The hybrid algorithm in this study achieves a mean shoulder-line offset of 15.8 m, a quite satisfied result in both accuracy and efficiency compared with published extraction methods.",GVF snake Parallel computing Shoulder-line Digital elevation model Loess terrain,
332,Enhancing clustering quality of geo-demographic analysis using context fuzzy clustering type-2 and particle swarm optimization,Applied Soft Computing,2014," Geo-Demographic Analysis, which is one of the most interesting inter-disciplinary research topics between Geographic Information Systems and Data Mining, plays a very important role in policies decision, population migration and services distribution. Among some soft computing methods used for this problem, clustering is the most popular one because it has many advantages in comparison with the rests such as the fast processing time, the quality of results and the used memory space. Nonetheless, the state-of-the-art clustering algorithm namely FGWC has low clustering quality since it was constructed on the basis of traditional fuzzy sets. In this paper, we will present a novel interval type-2 fuzzy clustering algorithm deployed in an extension of the traditional fuzzy sets namely Interval Type-2 Fuzzy Sets to enhance the clustering quality of FGWC. Some additional techniques such as the interval context variable, Particle Swarm Optimization and the parallel computing are attached to speed up the algorithm. The experimental evaluation through various case studies shows that the proposed method obtains better clustering quality than some best-known ones.",Context clustering Fuzzy clustering type-2 Geo-demographic analysis Heuristic algorithms Particle swarm optimization,
333,An information-theoretic approach to estimating ultrasound backscatter characteristics,Computers in Biology and Medicine,2004,"Analysis of backscatter in the ultrasound echo envelope, in conjunction with ultrasound B-scans, can provide important information for tissue characterization and pathology diagnosis. Statistical models have often proven useful in modeling backscatter. In this paper, an innovative approach to backscatter analysis based on generalized entropies and neural function approximation is presented. Entropy measures are shown to provide accurate estimates of scatterer density, regularity, and SNR of the amplitude distribution. Specific scattering distributions need not be assumed. Experimental results on ground truth envelopes show that generalized entropies can be used to accurately estimate backscatter properties.",Ultrasound Backscatter analysis Speckle Renyi entropy Tsallis entropy,
334,Image compression by texture modeling in the wavelet domain,"Image Processing, IEEE Transactions on",1996,"High-quality image compression algorithms are capable of achieving transmission or storage rates of 0.3 to 0.5 b/pixel with low degradation in image quality. In order to obtain even lower bit rates, we relax the usual RMS error definition of image quality and allow certain âless criticalâ portions of the image to be transmitted as texture models. These regions are then reconstructed at the receiver with statistical fidelity in the mid- to high-range spatial frequencies and absolute fidelity in the lowpass frequency range. This hybrid spectral texture modeling technique takes place in the discrete wavelet transform domain. In this way, we obtain natural spectral texture models and avoid the boundary blending problems usually associated with polygonal modeling. This paper describes the complete hybrid compression system with emphasis on the texture modeling issues",data compression image coding image reconstruction image segmentation image texture spectral analysis transform coding wavelet transforms RMS error absolute fidelity discrete wavelet transform domain hybrid compression system hybrid spectral texture modeling image compression algorithms image quality image reconstruction image segmentation lowpass frequency range spatial frequencies spectral texture models statistical fidelity storage rates transmission rates Bit rate Degradation Discrete wavelet transforms Frequency Image coding Image quality Image reconstruction Image storage Pixel Wavelet domain,
335,Comments on broadcast algorithms for two-dimensional grids,Parallel Computing,1991,"In a recent paper, Saad and Schultz (Data communication in parallel architectures, Parallel Comput. vol.11, p.131-50, (1989)) discussed several algorithms for broadcasting data in loosely coupled two-dimensional processor grids. This note disputes a claim made about the performance of one of their algorithms. An alternative algorithm is proposed.",parallel architectures,
336,UJA-3DFD: A program to compute the 3D fractal dimension from MRI data,Computer Methods and Programs in Biomedicine,2011,"This work presents a computer program for computing the 3D fractal dimension (3DFD) from magnetic-resonance images of the brain. The program is based on an algorithm that calculates the 3D box counting of the entire volume of the brain, and also of its 3D skeletonization. The validity and accuracy of the software has been confirmed using solids with well-known 3DFD values. The usefulness of the program developed is demonstrated by its successful characterization of several neurodegenerative diseases.",Fractal dimension 3D box counting Magnetic-resonance imaging,
337,Special section: Life science grids for biomedicine and bioinformatics,Future Generation Computer Systems,2007,,,
338,Benign and malignant breast tumors classification based on region growing and CNN segmentation,Expert Systems with Applications,2015," Breast cancer is regarded as one of the most frequent mortality causes among women. As early detection of breast cancer increases the survival chance, creation of a system to diagnose suspicious masses in mammograms is important. In this paper, two automated methods are presented to diagnose mass types of benign and malignant in mammograms. In the first proposed method, segmentation is done using an automated region growing whose threshold is obtained by a trained artificial neural network (ANN). In the second proposed method, segmentation is performed by a cellular neural network (CNN) whose parameters are determined by a genetic algorithm (GA). Intensity, textural, and shape features are extracted from segmented tumors. GA is used to select appropriate features from the set of extracted features. In the next stage, ANNs are used to classify the mammograms as benign or malignant. To evaluate the performance of the proposed methods different classifiers (such as random forest, naïve Bayes, SVM, and KNN) are used. Results of the proposed techniques performed on MIAS and DDSM databases are promising. The obtained sensitivity, specificity, and accuracy rates are 96.87%, 95.94%, and 96.47%, respectively.",Breast cancer Segmentation Cellular neural network Region growing Genetic algorithm Artificial neural network,
339,Securing skeletal systems with limited performance penalty: The muskel experience,Journal of Systems Architecture,2008,"Algorithmic skeletons have been exploited to implement several parallel programming environments, targeting workstation clusters as well as workstation networks and computational grids. When targeting non-dedicated clusters, workstation networks and grids, security has to be taken adequately into account in order to guarantee both code and data confidentiality and integrity. However, introducing security is usually an expensive activity, both in terms of the effort required to managed security mechanisms and in terms of the time spent performing security related activities at run time. We discuss the cost of security introduction as well as how some features typical of skeleton technology can be exploited to improve the efficiency code and data securing in a typical skeleton based parallel programming environment and we evaluate the performance cost of security mechanisms implemented exploiting state of the art tools. In particular, we take into account the cost of security introduction in muskel, a Java based skeletal system exploiting macro data flow implementation technology. We consider the adoption of mechanisms that allow securing all the communications involving remote, unreliable nodes and we evaluate the cost of such mechanisms. Also, we consider the implications on the computational grains needed to scale secure and insecure skeletal computations.",Skeletons Parallelism Security Scalability,
340,Training cellular automata for image processing,"Image Processing, IEEE Transactions on",2006,"Experiments were carried out to investigate the possibility of training cellular automata (CA) to perform several image processing tasks. Even if only binary images are considered, the space of all possible rule sets is still very large, and so the training process is the main bottleneck of such an approach. In this paper, the sequential floating forward search method for feature selection was used to select good rule sets for a range of tasks, namely noise filtering (also applied to grayscale images using threshold decomposition), thinning, and convex hulls. Various objective functions for driving the search were considered. Several modifications to the standard CA formulation were made (the B-rule and two-cycle CAs), which were found, in some cases, to improve performance.",cellular automata feature extraction image denoising image processing search problems B-rule CA CA formulation binary images cellular automata training convex hulls feature selection grayscale images image processing noise filtering objective functions sequential floating forward search method threshold decomposition two-cycle CA Automata Biological system modeling Content addressable storage Filling Filtering Gray-scale Hardware Image processing Object recognition Search methods Cellular automata image denoising image processing rule selection Algorithms Artificial Intelligence Biomimetics Cell Physiology Computer Graphics Image Enhancement Image Interpretation Computer-Assisted Information Storage and Retrieval Numerical Analysis Computer-Assisted Pattern Recognition Automated Signal Processing Computer-Assisted User-Computer Interface,
341,An advanced environment supporting structured parallel programming in Java,Future Generation Computer Systems,2003,"In this work we present Lithium, a pure Java structured parallel programming environment based on skeletons (common, reusable and efficient parallelism exploitation patterns). Lithium is implemented as a Java package and represents both the first skeleton based programming environment in Java and the first complete skeleton based Java environment exploiting macro-data flow implementation techniques. Lithium supports a set of user code optimizations which are based on skeleton rewriting techniques. These optimizations improve both absolute performance and resource usage with respect to original user code. Parallel programs developed using the library run on any network of workstations provided the workstations support plain JRE. The paper describes the library implementation, outlines the optimization techniques used and eventually presents the performance results obtained on both synthetic and real applications.",Java Parallel programming Skeletons Macro-data flow Optimizations,
342,Picture processing: 1985,"Computer Vision, Graphics, and Image Processing",1986,"This paper presents a bibliography of nearly 1100 references related to the computer processing of pictorial information, arranged by subject matter. Coverage is restricted, for the most part, to a selected set of U.S. journals and proceedings of specialized meetings. The topics covered include digitization, approximation, and compression; transforms, filtering, enhancement, restoration, and reconstruction; architectures, systems, software, and techniques; pictorial pattern recognition; feature detection, segmentation, and image analysis; matching and time-varying imagery; shape and pattern; geometry; texture; and three-dimensional scene analysis. No attempt is made to evaluate or summarize the items cited; the purpose is simply to provide a convenient compendium of references.",,
343,A data locality methodology for matrix-matrix multiplication algorithm,Journal of Supercomputing,2012,"Matrix-Matrix Multiplication (MMM) is a highly important kernel in linear algebra algorithms and the performance of its implementations depends on the memory utilization and data locality. There are MMM algorithms, such as standard, Strassen-Winograd variant, and many recursive array layouts, such as Z-Morton or U-Morton. However, their data locality is lower than that of the proposed methodology. Moreover, several SOA (state of the art) self-tuning libraries exist, such as ATLAS for MMM algorithm, which tests many MMM implementations. During the installation of ATLAS, on the one hand an extremely complex empirical tuning step is required, and on the other hand a large number of compiler options are used, both of which are not included in the scope of this paper. In this paper, a new methodology using the standard MMM algorithm is presented, achieving improved performance by focusing on data locality (both temporal and spatial). This methodology finds the scheduling which conforms with the optimum memory management. Compared with (Chatterjee et al. in IEEE Trans. Parallel Distrib. Syst. 13:1105, 2002; Li and Garzaran in Proc. of Lang. Compil. Parallel Comput., 2005; Bilmes et al. in Proc. of the 11th ACM Int. Conf. Super-comput., 1997; Aberdeen and Baxter in Concurr. Comput. Pract. Exp. 13:103, 2001), the proposed methodology has two major advantages. Firstly, the scheduling used for the tile level is different from the element level's one, having better data locality, suited to the sizes of memory hierarchy. Secondly, its exploration time is short, because it searches only for the number of the level of tiling used, and between (1, 2) (Sect. 4) for finding the best tile size for each cache level. A software tool (C-code) implementing the above methodology was developed, having the hardware model and the matrix sizes as input. This methodology has better performance against others at a wide range of architectures. Compared with the best existing related work, which we implemented, better performance up to 55% than the Standard MMM algorithm and up to 35% than Strassen's is observed, both under recursive data array layouts. 2010 Springer Science+Business Media, LLC.",Matrix algebra Algorithms C (programming language) Program compilers Scheduling Standards Storage allocation (computer),
344,Accelerating 3D nonrigid registration using the Cell Broadband Engine processor,Ibm Journal of Research and Development,2009,"Registration or alignment of medical images in clinical applications requires cost-effective high-performance computing. In this paper, we present a parallel design and implementation of a mutualinformation- based multiresolution nonrigid registration algorithm that takes advantage of the Cell Broadband Enginet (Cell/B.E.) Architecture by exploiting the different levels of parallelism and optimization strategies. The new method was tested with a dual-processor Cell/B.E. processor-based system. The experiments show an average performance of 1.09 ls per voxel and excellent scalability, demonstrating real-time or near-real-time performance for the computationally demanding task of nonrigid image registration.",,
345,A data locality methodology for matrix-matrix multiplication algorithm,Journal of Supercomputing,2012,"Matrix-Matrix Multiplication (MMM) is a highly important kernel in linear algebra algorithms and the performance of its implementations depends on the memory utilization and data locality. There are MMM algorithms, such as standard, Strassen-Winograd variant, and many recursive array layouts, such as Z-Morton or U-Morton. However, their data locality is lower than that of the proposed methodology. Moreover, several SOA (state of the art) self-tuning libraries exist, such as ATLAS for MMM algorithm, which tests many MMM implementations. During the installation of ATLAS, on the one hand an extremely complex empirical tuning step is required, and on the other hand a large number of compiler options are used, both of which are not included in the scope of this paper. In this paper, a new methodology using the standard MMM algorithm is presented, achieving improved performance by focusing on data locality (both temporal and spatial). This methodology finds the scheduling which conforms with the optimum memory management. Compared with (Chatterjee et al. in IEEE Trans. Parallel Distrib. Syst. 13:1105, 2002; Li and Garzaran in Proc. of Lang. Compil. Parallel Comput., 2005; Bilmes et al. in Proc. of the 11th ACM Int. Conf. Super-comput., 1997; Aberdeen and Baxter in Concurr. Comput. Pract. Exp. 13:103, 2001), the proposed methodology has two major advantages. Firstly, the scheduling used for the tile level is different from the element level's one, having better data locality, suited to the sizes of memory hierarchy. Secondly, its exploration time is short, because it searches only for the number of the level of tiling used, and between (1, 2) (Sect. 4) for finding the best tile size for each cache level. A software tool (C-code) implementing the above methodology was developed, having the hardware model and the matrix sizes as input. This methodology has better performance against others at a wide range of architectures. Compared with the best existing related work, which we implemented, better performance up to 55% than the Standard MMM algorithm and up to 35% than Strassen's is observed, both under recursive data array layouts.",algorithm theory linear algebra storage management,
346,"Nonrigid image registration in shared-memory multiprocessor environments with application to brains, breasts, and bees","Information Technology in Biomedicine, IEEE Transactions on",2003,"One major problem with nonrigid image registration techniques is their high computational cost. Because of this, these methods have found limited application to clinical situations where fast execution is required, e.g., intraoperative imaging. This paper presents a parallel implementation of a nonrigid image registration algorithm. It takes advantage of shared-memory multiprocessor computer architectures using multithreaded programming by partitioning of data and partitioning of tasks, depending on the computational subproblem. For three different biomedical applications (intraoperative brain deformation, contrast-enhanced MR mammography, intersubject brain registration), the scaling behavior of the algorithm is quantitatively analyzed. The method is demonstrated to perform the computation of intra-operative brain deformation in less than a minute using 64 CPUs on a 128-CPU shared-memory supercomputer (SGI Origin 3800). It is shown that its serial component is no more than 2% of the total computation time, allowing a speedup of at least a factor of 50. In most cases, the theoretical limit of the speedup is substantially higher (up to 132-fold in the application examples presented in this paper). The parallel implementation of our algorithm is, therefore, capable of solving nonrigid registration problems with short execution time requirements and may be considered an important step in the application of such techniques to clinically important problems such as the computation of brain deformation during cranial image-guided surgery.",brain image registration mammography medical image processing multi-threading parallel algorithms shared memory systems SGI Origin 3800 biomedical applications computational cost computational subproblem contrast-enhanced MR mammography cranial image-guided surgery data partitioning execution time requirements intersubject brain registration intraoperative brain deformation intraoperative imaging multithreaded programming nonrigid image registration techniques parallel algorithm scaling behavior serial component shared-memory multiprocessor computer architectures speedup task partitioning Algorithm design and analysis Application software Biomedical computing Biomedical imaging Breast Computational efficiency Computer architecture Image registration Mammography Partitioning algorithms Algorithms Animals Bees Brain Breast Humans Magnetic Resonance Imaging,
347,Collaboration of reconfigurable processors in grid computing: Theory and application,Future Generation Computer Systems,2011,"Traditional grid networks employ General Purpose Processors (GPPs) as their main processing elements. Incorporating reconfigurable processing elements in such networks can be a promising technology to increase their performance. In this paper, we propose and simulate collaboration of reconfigurable processors in grid computing. Collaborative Reconfigurable Grid Computing (CRGC) employs the availability of any reconfigurable processor to accelerate compute-intensive applications such as multimedia kernels. We explore the mapping of some compute-intensive multimedia kernels such as the 2D DWT and the co-occurrence matrix in the CRGC. These multimedia kernels are simulated as an independent set of gridlets submitted to a software simulator called CRGridSim. In addition, we analyze the lower and upper bounds of performance for CRGC. Our experimental results show that the CRGC approach improves performance up to 7.2 × and 2.5 × compared to a single GPP and the collaboration of GPPs, respectively, when assuming a speedup of 10 of the reconfigurable processors in a grid with 4 nodes.",Reconfigurable architectures Grid computing Multimedia kernels High-performance computing,
348,Robust and efficient overset grid assembly for partitioned unstructured meshes,Journal of Computational Physics,2014," This paper presents a method to perform efficient and automated Overset Grid Assembly (OGA) on a system of overlapping unstructured meshes in a parallel computing environment where all meshes are partitioned into multiple mesh-blocks and processed on multiple cores. The main task of the overset grid assembler is to identify, in parallel, among all points in the overlapping mesh system, at which points the flow solution should be computed (field points), interpolated (receptor points), or ignored (hole points). Point containment search or donor search, an algorithm to efficiently determine the cell that contains a given point, is the core procedure necessary for accomplishing this task. Donor search is particularly challenging for partitioned unstructured meshes because of the complex irregular boundaries that are often created during partitioning. Another challenge arises because of the large variation in the type of mesh-block overlap and the resulting large load imbalance on multiple processors. Desirable traits for the grid assembly method are efficiency (requiring only a small fraction of the solver time), robustness (correct identification of all point types), and full automation (no user input required other than the mesh system). Additionally, the method should be scalable, which is an important challenge due to the inherent load imbalance. This paper describes a fully-automated grid assembly method, which can use two different donor search algorithms. One is based on the use of auxiliary grids and Exact Inverse Maps (EIM), and the other is based on the use of Alternating Digital Trees (ADT). The EIM method is demonstrated to be more efficient than the ADT method, while retaining robustness. An adaptive load re-balance algorithm is also designed and implemented, which considerably improves the scalability of the method.",Numerical algorithms Overset methods Computational Fluid Dynamics,
349,Capacity planning and scheduling in Grid computing environments,Future Generation Computer Systems,2008,"Grid computing infrastructures embody a cost-effective computing paradigm that virtualises heterogeneous system resources to meet the dynamic needs of critical business and scientific applications. These applications range from batch processes and long-running tasks to real-time and even transactional applications. Grid computing environments are inherently dynamic and unpredictable environments sharing services amongst many different users. Grid schedulers aim to make the most efficient use of Grid resources (high utilisation) while providing the best possible performance to the Grid applications (reducing makespan) and satisfying the associated performance and Quality of Service (QoS) constraints. Additionally, in commercial Grid settings where economic considerations are an increasingly important part of Grid scheduling, it is necessary to minimise the cost of application execution on the behalf of the Grid users while ensuring that the applications meet their QoS constraints. Furthermore, efficient resource allocation may allow a resource broker to maximise their profit by minimising the quantity of resource procurement. Scheduling in such a large-scale, dynamic and distributed environment is a complex undertaking. In this paper, we propose an approach to Grid scheduling which  over the details of individual applications, focusing instead on the global cost optimisation problem while taking into account the entire workload, dynamically adjusting to the varying service demands. Our model places particular emphasis on the stochastic and unpredictable nature of the Grid, leading to a more accurate reflection of the state of the Grid and hence more efficient and accurate scheduling decisions.",Grid computing Scheduling Brokering Advance reservations Performance Quality-of-service Queueing Optimisation,
350,Wall distance search algorithm using voxelized marching spheres,Journal of Computational Physics,2013,"Minimum distance to a solid wall is a commonly used parameter in turbulence closure formulations associated with the Reynolds Averaged form of the Navier Stokes Equations (RANS). This paper presents a new approach to efficiently compute the minimum distance between a set of points and a surface. The method is based on sphere voxelization, and uses fast integer arithmetic algorithms from the field of computer graphics. Using a simple test case where the number of points ( N p ) and surface elements ( N b ) can be independently specified, the present method is empirically estimated to be O ( N p 0.8 N b 0.5 ) . An unstructured grid around an aircraft configuration (DLR-F6) is chosen as the test case for demonstration and validation. Multi-processor computations (up to 256 processors) are conducted to study efficiency and scalability. Encouraging results are obtained, with the sphere voxelization algorithm demonstrated to be more efficient than all of the alternate methods for computing minimum distances. However, a load imbalance does exist, which negatively impacts the scalability for large number of cores. A simple method for load re-balancing is formulated and tested, which results in significant improvements in both efficiency and scalability.",Numerical algorithms Computational fluid dynamics Turbulence modeling Computer graphics Minimum wall distance Voxel-based methods,
351,Electronics and health care revisited: thirty-eight years later,Proceedings of the IEEE,2000,"The holy grail of modern medicine is the quest to achieve an all-inclusive lifelong health record. There has been long discussion as to what belongs in such a record. Advances in data storage devices have made such discussion irrelevant, The answer is: store everything"". The more timely and difficult question is the efficient and specific retrieval of the needed information from the stored data. This leads to questions of indexing schemes at storage time or natural language processing at retrieval time. One must remember that the lifelong record is not made up of just text. It is a multimedia record containing pictures (X-rays", pathology),
352,Visualization in biomedical computing,Parallel Computing,1999,"Visualizable objects in biology and medicine extend across a vast range of scale, from individual molecules and cells, to the varieties of tissue and interstitial interfaces, to complete organs, organ systems and body parts, and include functional attributes of these systems, such as biophysical, biomechanical and physiological properties. Medical applications include accurate anatomy and function mapping, enhanced diagnosis, accurate treatment planning and rehearsal, and education/training. However, the greatest potential for revolutionary innovation in the practice of medicine lies in direct, fully immersive, real-time multisensory fusion of real and virtual information data streams into online, real-time visualizations available during an actual clinical procedure. Current high-performance computers and advanced image processing capabilities have facilitated major progress toward realization of this goal. With these advances in hand, there are several important applications possible to be delivered soon that will have a significant impact on the practice of medicine and on biological research.",Visualization 3-D imaging 3-D display Volume rendering Volume modeling,
353,The Visible Human Project,Proceedings of the IEEE,1998,"The Visible Human Project data sets are designed to serve as a common reference point for the study of human anatomy, as a set of common public-domain data for testing medical imaging algorithms, and as a testbed and model for the construction of image libraries that can be accessed through networks. The data sets are being applied to a wide range of educational, diagnostic, treatment planning, virtual reality, artistic, mathematical, and industrial uses by more than 800 licensees in 27 countries. But key issues remain in the development of methods to link such image data to text-based data. Standards do not currently exist for such linkages. Basic research is needed in the description and representation of image-based structures and in the connection of image-based structural-anatomical data to text-based functional-physiological data. This is the larger, long-term goal of the Visible Human Project: to link the print library of functional-physiological knowledge with the image library of structural-anatomical knowledge transparently into one unified resource of health information",biomedical education information networks libraries medical image processing patient treatment physiology planning virtual reality visual databases Visible Human Project data sets artistic use common public-domain data diagnostic use educational use health information resource human anatomy image data image libraries image-based structural-anatomical data industrial use mathematical use medical imaging algorithm testing networks text-based functional-physiological data treatment planning virtual reality Algorithm design and analysis Biomedical imaging Cognitive science Computer displays Human anatomy Image storage Libraries Medical tests Rendering (computer graphics) Two dimensional displays,
354,A distributed spectral-screening PCT algorithm,Journal of Parallel and Distributed Computing,2003,"This paper describes a novel distributed algorithm for use in remote-sensing, medical image analysis, and surveillance applications. The algorithm combines spectral-screening classification with the principal component transform, and human-centered mapping. It fuses a multi- or hyper-spectral image set into a single color-composite image that maximizes the impact of spectral variation on the human visual system. The algorithm operates on distributed collections of shared-memory multiprocessors that are connected through high-performance networking. Scenes taken from a standard 210 frame remote-sensing data set, collected with the hyper-spectral digital imagery collection experiment airborne imaging spectrometer, are used to assess the algorithms image quality, performance, and scaling. The algorithm is supported with a predictive analytical model that allows its performance to be assessed for a wide variety of typical variations in use. For example, changes to the number of spectra, image resolution, processor speed, memory size, network bandwidth/latency, and granularity of decomposition. The motivation in building a performance model is to assess the impact of changes in technology and problem size associated with different applications, allowing cost–performance tradeoffs to be assessed.",Principal component transform Spectral angle classification Distributed algorithm Performance prediction,
355,The biomedical imaging resource at Mayo Clinic,"Medical Imaging, IEEE Transactions on",2001,"This editorial reviews the history and summarizes achievements, describes current activities, indicates future directions, and finally suggests some keys to success of the Biomedical Imaging Resource (BTR) at Mayo Clinic/Foundation. The origin and history section provides a chronological description, with associated references, emphasizing progress and milestones over the past three decades. Several figures are included to illustrate and highlight some particularly unique achievements.",biomedical imaging history reviews Mayo Clinic biomedical imaging resource chronological description historical review Biomedical imaging Computed tomography Engineering profession Evolution (biology) Heart History Image reconstruction Laboratories Lungs Three dimensional displays Animals Diagnostic Imaging History 20th Century Hospitals Group Practice Humans Minnesota Technology Radiologic,
356,Virtual endoscopy: development and evaluation using the Visible Human Datasets,Computerized Medical Imaging and Graphics,2000,"Virtual endoscopy (VE) is a new method of diagnosis using computer processing of 3D image datasets (such as CT or MRI scans) to provide simulated visualizations of patient specific organs similar or equivalent to those produced by standard endoscopic procedures. Conventional endoscopy is invasive and often uncomfortable for patients. It sometimes has serious side effects such as perforation, infection and hemorrhage. VE visualization avoids these risks and can minimize difficulties and decrease morbidity when used before actual endoscopic procedures. In addition, there are many body regions not compatible with real endoscopy that can be explored with VE. Eventually, VE may replace many forms of real endoscopy. There remains a critical need to refine and validate VE visualizations for routine clinical use. We have used the Visible Human Dataset from the National Library of Medicine to develop and test these procedures and to evaluate their use in a variety of clinical applications. We have developed specific clinical protocols to compare virtual endoscopy with real endoscopy. We have developed informative and dynamic on-screen navigation guides to help the surgeon or physician interactively determine body orientation and precise anatomical localization while performing the VE procedures. Additionally, the adjunctive value of full 3D imaging (e.g. looking “outside” of the normal field of view) during the VE exam is being evaluated. Quantitative analyses of local geometric and densitometric properties obtained from the virtual procedures (“virtual biopsy”) are being developed and compared with other direct measures. Preliminary results suggest that these virtual procedures can provide accurate, reproducible and clinically useful visualizations and measurements. These studies will help drive improvements in and lend credibility to VE procedures and simulations as routine clinical tools. VE holds significant promise for optimizing endoscopic diagnostic procedures, minimizing patient risk and morbidity, and reducing health care costs.",Virtual endoscopy Visible humans Anatomic modeling Volume rendering,
357,A parallel algorithm for 3D reconstruction of angiographic images,Future Generation Computer Systems,2000,"Accurate diagnosis and therapeutic evaluation of coronary dysfunction is possible by tri-dimensional (3D) visualization of Coronary arteries. Reconstruction based on bi-dimensional (2D) images can be presented as a discrete optimization problem. A blind search cannot be applied, instead a Branch-and-Bound algorithm is used to explore the state space and give an intermediate result. The heuristic information used is based on 2D and 3D a priori knowledge. A sequential algorithm using suitable filters leads to implementations where the execution time is measured in days. In order to minimize the execution time we propose to apply parallel computing techniques. The critical issue in parallel search algorithms is the distribution of the search space among the processors. We propose a technique to compute the total amount of work units among the processors. The technique is based on the enlargement of segments (unitary threads) representing pieces of arteries. We achieve a good load balancing and the speedup obtained is nearly optimum.",Angiographic images reconstruction Parallel application Load balancing,
358,CAIMAN: An online algorithm repository for Cancer Image Analysis,Computer Methods and Programs in Biomedicine,2011,"CAIMAN (CAncer IMage ANalysis: http://www.caiman.org.uk) is an online algorithm repository that provides specifically designed algorithms to analyse the images produced by experiments relevant to Cancer Research and Life Sciences, especially vascular biology. CAIMAN is accessed through a user-friendly website where researchers can upload their images and the results are returned by email. CAIMAN does not intend to replace more sophisticated software solutions such as ImageJ, Matlab, or commercial packages, but it will provide a first stop where any researcher can upload images and can obtain quantitative results without having to do any programming at all.",Online image analysis Distributed processing Imaging in cancer research,
359,MESA: Complete approach for design and evaluation of segmentation methods using real and simulated tomographic images,Biocybernetics and Biomedical Engineering,2014," In this paper we present MESA: a platform for design and evaluation of medical image segmentation methods. The platform offers a complete approach for the method creation and validation using simulated and real tomographic images. The system consists of several modules that provide a comprehensive workflow for generation of test data, segmentation method development as well as experiment planning and execution. The test data can be created as a virtual scene that provides an ideal reference segmentation and is also used to simulate the input images by a virtual magnetic resonance imaging (MRI) scanner. Both ideal reference segmentation and simulated images could be utilized during the evaluation of the segmentation methods. The platform offers various experimental capabilities to measure and compare the performance of the methods on various data sets, parameters and initializations. The segmentation framework, currently based on deformable models, uses a template solution for dynamical composition and creation of two- and three-dimensional methods. The platform is based on a client–server architecture, with computational and data storage modules deployed on the server and with browser-based client applications. We demonstrate the platform capabilities during the design of segmentation methods with the use of simulated and actual tomographic images.",Image segmentation Magnetic resonance imaging Deformable models Segmentation evaluation,
360,3D nonrigid registration via optimal mass transport on the GPU,Medical Image Analysis,2009,"In this paper, we present a new computationally efficient numerical scheme for the minimizing flow approach for optimal mass transport (OMT) with applications to non-rigid 3D image registration. The approach utilizes all of the gray-scale data in both images, and the optimal mapping from image A to image B is the inverse of the optimal mapping from B to A . Further, no landmarks need to be specified, and the minimizer of the distance functional involved is unique. Our implementation also employs multigrid, and parallel methodologies on a consumer graphics processing unit (GPU) for fast computation. Although computing the optimal map has been shown to be computationally expensive in the past, we show that our approach is orders of magnitude faster then previous work and is capable of finding transport maps with optimality measures (mean curl) previously unattainable by other works (which directly influences the accuracy of registration). We give results where the algorithm was used to compute non-rigid registrations of 3D synthetic data as well as intra-patient pre-operative and post-operative 3D brain MRI datasets.",Non-rigid registration Optimal mass transport Monge–Kantorovich Multigrid Variational methods GPU,
361,The dawn of terascale computing,"Solid-State Circuits Magazine, IEEE",2009,"The digital revolution, far from abating, continues with even greater intensity in new applications in health, media, social networking, and many other areas of our lives. These applications will require revolutionary improvements in speed and capacity in future microprocessors so that they can process terabytes of information with teraflops of terascale computing power. Tera is not an exaggeration: trillions of hertz and trillions of bytes will be needed. In a terascale world, there will be new processing capabilities for mining and interpreting the world's growing mountain of data, and for doing so with even greater efficiency. Examples of applications are artificial intelligence in smart cars and appliances and virtual reality for modeling, visualization, physics simulation, and medical training. Many other applications are still on the edge of science fiction. In these applications, massive amounts of data must be processed. Three-dimensional (3-D) images in connected visual computing applications like virtual worlds can include hundreds of hours of video, thousands of documents, and tens of thousands of digital photos that require indexing and searching. Terascale computing refers to this massive processing capability with the right mix of memory and input/output (I/O) capabilities for use in everyday devices, from servers to desktops to laptops.",artificial intelligence microprocessor chips multiprocessing systems artificial intelligence massive processing capability terascale computing Artificial intelligence Biomedical imaging Computational modeling Data visualization Home appliances Medical simulation Microprocessors Physics Social network services Virtual reality,
362,Multi-shell diffusion signal recovery from sparse measurements,Medical Image Analysis,2014," For accurate estimation of the ensemble average diffusion propagator (EAP), traditional multi-shell diffusion imaging (MSDI) approaches require acquisition of diffusion signals for a range of b-values. However, this makes the acquisition time too long for several types of patients, making it difficult to use in a clinical setting. In this work, we propose a new method for the reconstruction of diffusion signals in the entire q-space from highly undersampled sets of MSDI data, thus reducing the scan time significantly. In particular, to sparsely represent the diffusion signal over multiple q-shells, we propose a novel extension to the framework of spherical ridgelets by accurately modeling the monotonically decreasing radial component of the diffusion signal. Further, we enforce the reconstructed signal to have smooth spatial regularity in the brain, by minimizing the total variation (TV) norm. We combine these requirements into a novel cost function and derive an optimal solution using the Alternating Directions Method of Multipliers (ADMM) algorithm. We use a physical phantom data set with known fiber crossing angle of 45° to determine the optimal number of measurements (gradient directions and b-values) needed for accurate signal recovery. We compare our technique with a state-of-the-art sparse reconstruction method (i.e., the SHORE method of Cheng et al. (2010)) in terms of angular error in estimating the crossing angle, incorrect number of peaks detected, normalized mean squared error in signal recovery as well as error in estimating the return-to-origin probability (RTOP). Finally, we also demonstrate the behavior of the proposed technique on human in vivo data sets. Based on these experiments, we conclude that using the proposed algorithm, at least 60 measurements (spread over three b-value shells) are needed for proper recovery of MSDI data in the entire q-space.",Diffusion MRI Compressed sensing Diffusion spectrum imaging Diffusion propagator Kurtosis,
363,Speeding-up codon analysis on the cloud with local MapReduce aggregation,Information Sciences,2014," A notable obstacle to higher performance of data-intensive Hadoop MapReduce (MR) bioinformatics algorithms is the large volume of intermediate data that need to be sorted, shuffled, and transmitted between mapper and reducer tasks. This difficulty manifests itself quite clearly in MR codon analysis which is known to generate voluminous intermediate data that create a bottleneck in basic MR codon analysis algorithms. Our proposed approach to handle the intermediate data bottleneck is local in-mapper aggregation (or simply local aggregation), a technique that helps reduce the intermediate data volume between mapper and reducer tasks in MR. We experimentally evaluate the performance of local aggregation (i) by developing codon analysis MR algorithms with and without local aggregation and (ii) by experimentally measuring their performance on Amazon Web Services (AWS), the Amazon cloud platform. Codon analysis with local aggregation maintains consistently high performance with the growth of larger datasets while basic codon analysis, without local aggregation becomes impractically slow even for smaller datasets. Our results can be beneficial (i) to members of the bioinformatics community who need to perform fast and cost-effective nucleotide MR analysis on the cloud and (ii) to computer scientists who strive to increase the performance of MR algorithms.",Codon analysis Hadoop MapReduce Local aggregation Cloud computing,
364,An integrated service architecture for managing capital market systems,"Network, IEEE",2002,"This article studies current developments and trends in the area of capital market systems. In particular, it defines the trading lifecycle and the activities associated with it. The article then investigates opportunities for the integration of legacy systems and existing communication protocols through distributed integrated services that correspond to established business processes. These integrated services link to basic services such as an exchange, a settlement, or a registry service. Examples of such integrated services include pre-trade services (e.g., analytics) or post-trade services (e.g., surveillance). The article then presents the various levels of integration in capital market systems and discusses the standards in place. It establishes that most interactions occur at a low level of  such as the network (e.g., TCP/IP), data format (e.g., FIX, XML), and middleware levels (e.g., CORBA). Finally, the article discusses a software development methodology based on the use of design patterns. These design patterns address the essential aspects of managing integrated services in a technology-independent fashion. These aspects are service wrapping, service composition, service contracting, service discovery, and service execution. The objective of the methodology is to facilitate the rapid development of new integrated services that correspond to emerging business opportunities",Internet business communication commerce computer network management distributed object management distributed processing hypermedia markup languages software architecture stock markets telecommunication standards transport protocols CORBA FIX Internet TCP/IP Web XML business processes capital market systems management communication protocols data format design patterns distributed integrated services exchange integrated service architecture legacy systems middleware post-trade services pre-trade services registry service service composition service contracting service discovery service execution service wrapping settlement software architecture software development standards surveillance trading lifecycle Business communication Intserv networks Middleware Programming Protocols Surveillance TCPIP Technology management Wrapping XML,
365,Efficient 2D and 3D watershed on graphics processing unit: block-asynchronous approaches based on cellular automata,Computers & Electrical Engineering,2013," The watershed transform is a method for non-supervised image segmentation. In this paper we show that a watershed algorithm based on a cellular automaton is a good choice for the recent GPU architectures, especially when the synchronization rules are relaxed. In particular, we propose a block-asynchronous computation strategy that maps the cellular automaton on the thread blocks of the GPU. This method reduces the number of points of global synchronization allowing efficient exploitation of the memory hierarchy of the GPU. We also avoid the artifacts produced in the watershed lines by the block-asynchronous updating scheme by correcting the data propagation speed among the blocks. The proposals are compared to an OpenMP multithreaded code. The high speedups indicate the potential of this kind of algorithm for new architectures based on hundreds of cores. The method is tuned to be applied to 3D volumes obtaining similar results.",,
366,Geometric modeling and motion analysis of the epicardial surface of the heart,Mathematics and Computers in Simulation,2010,"Pathological processes cause abnormal regional motions of the heart. Regional wall motion analyses are important to evaluate the success of therapy, especially of cell therapy, since the recovery of the heart in cell therapy proceeds slowly and results in only small changes of ventricular wall motility. The usual ultrasound imaging of heart motion is too inaccurate to be considered as an appropriate method. MRI studies are more accurate, but insufficient to reliably detect small changes in regional ventricular wall motility. We thus aim at a more accurate method of motion analysis. Our approach is based on two imaging modalities, viz. cardiac CT and biplane cineangiography. The epicardial surface represented in the CT data set at the end of the diastole is registered to the three-dimensionally reconstructed epicardial artery tree from the angiograms in end-diastolic position. The motion tracking procedures are carried out by applying thin-plate spline transformations between the epicardial artery trees belonging to consecutive frames of our cineangiographic imagery.",Cardiac motion tracking Surface mesh generation Registration Thin-plate spline transformation Radial basis function,
367,Dual optimization based prostate zonal segmentation in 3D MR images,Medical Image Analysis,2014," Efficient and accurate segmentation of the prostate and two of its clinically meaningful sub-regions: the central gland (CG) and peripheral zone (PZ), from 3D MR images, is of great interest in image-guided prostate interventions and diagnosis of prostate cancer. In this work, a novel multi-region segmentation approach is proposed to simultaneously segment the prostate and its two major sub-regions from only a single 3D T2-weighted (T2w) MR image, which makes use of the prior spatial region consistency and incorporates a customized prostate appearance model into the segmentation task. The formulated challenging combinatorial optimization problem is solved by means of convex relaxation, for which a novel spatially continuous max-flow model is introduced as the dual optimization formulation to the studied convex relaxed optimization problem with region consistency constraints. The proposed continuous max-flow model derives an efficient duality-based algorithm that enjoys numerical advantages and can be easily implemented on GPUs. The proposed approach was validated using 18 3D prostate T2w MR images with a body-coil and 25 images with an endo-rectal coil. Experimental results demonstrate that the proposed method is capable of efficiently and accurately extracting both the prostate zones: CG and PZ, and the whole prostate gland from the input 3D prostate MR images, with a mean Dice similarity coefficient (DSC) of 89.3 ± 3.2 % for the whole gland (WG), 82.2 ± 3.0 % for the CG, and 69.1 ± 6.9 % for the PZ in 3D body-coil MR images; 89.2 ± 3.3 % for the WG, 83.0 ± 2.4 % for the CG, and 70.0 ± 6.5 % for the PZ in 3D endo-rectal coil MR images. In addition, the experiments of intra- and inter-observer variability introduced by user initialization indicate a good reproducibility of the proposed approach in terms of volume difference (VD) and coefficient-of-variation (CV) of DSC.",3D prostate MRI Zonal segmentation Convex optimization Multi-region segmentation,
368,A framework using cluster-based hybrid network architecture for collaborative virtual surgery,Computer Methods and Programs in Biomedicine,2009,"Research on collaborative virtual environments (CVEs) opens the opportunity for simulating the cooperative work in surgical operations. It is however a challenging task to implement a high performance collaborative surgical simulation system because of the difficulty in maintaining state consistency with minimum network latencies, especially when sophisticated deformable models and haptics are involved. In this paper, an integrated framework using cluster-based hybrid network architecture is proposed to support collaborative virtual surgery. Multicast transmission is employed to transmit updated information among participants in order to reduce network latencies, while system consistency is maintained by an administrative server. Reliable multicast is implemented using distributed message acknowledgment based on cluster cooperation and sliding window technique. The robustness of the framework is guaranteed by the failure detection chain which enables smooth transition when participants join and leave the collaboration, including normal and involuntary leaving. Communication overhead is further reduced by implementing a number of management approaches such as computational policies and collaborative mechanisms. The feasibility of the proposed framework is demonstrated by successfully extending an existing standalone orthopedic surgery trainer into a collaborative simulation system. A series of experiments have been conducted to evaluate the system performance. The results demonstrate that the proposed framework is capable of supporting collaborative surgical simulation.",Surgical simulation Collaborative virtual environments Cluster-based network architecture Reliable multicast Efficient collaboration,
369,SEGEDMA: Sensor grid enhancement data management system for Health Care computing,Expert Systems with Applications,2011,"Wireless sensor network (WSN) can be deployed to monitor the health of patients suffering from critical diseases. Also a wireless network consisting of biomedical sensors can be implanted into the patient’s body and can monitor the patients’ conditions. These sensor devices apart from having an enormous capability of collecting data from their physical surroundings are also resource constraint in nature with a limited processing and communication ability and hence we have to integrate the Grid technology to do the processing and storage of the data collected by the sensor nodes. In this paper, we proposed the sensor grid enhancement data management system, called SEGEDMA ensuring the integration of different network technologies and the continuous data access to system users. The main contribution of this work is to achieve the interoperability of both technologies through a novel network architecture and according to the results SEGEDMA can be applied successfully.",Wireless sensor network Grid computing Sensor grid Grid storage Data management system HealthGrid Health Care E-Health,
370,Pattern recognition in highly-integrated circuits,Pattern Recognition,1985,"Possibilities for the implementation of pattern recognition in highly-integrated circuits are explored. As an example, a specific system for the recognition of handwritten letters is modeled. This model employs the basic operations of a cellular array of coupled devices for feature extraction, i.e. tesselation or the shifting of information between neighboring cells.",Visual pattern recognition VLSI implementation Handwritten letters Perceptron-like Simulation,
371,Creating a high-resolution spatial/symbolic model of the inner organs based on the Visible Human,Medical Image Analysis,2001,"Computerized three-dimensional models of the human body, based on the Visible Human Project of the National Library of Medicine, so far do not reflect the rich anatomical detail of the original cross-sectional images. In this paper, a spatial/symbolic model of the inner organs is developed, which is based on more than 1000 cryosections and congruent fresh and frozen CT images of the male Visible Human. The spatial description is created using color-space segmentation, graphic modeling, and a matched volume visualization with subvoxel resolution. It is linked to a symbolic knowledge base, providing an ontology of anatomical terms. With over 650 three-dimensional anatomical constituents, this model offers an unsurpassed photorealistic presentation and level of detail. A three-dimensional atlas of anatomy and radiology based on this model is available as a PC-based program.",Visible Human Three-dimensional body model Anatomical atlas Color-space segmentation Volume visualization,
372,Applying spatial distribution analysis techniques to classification of 3D medical images,Artificial Intelligence in Medicine,2005,"SummaryObjective: The objective of this paper is to classify 3D medical images by analyzing spatial distributions to model and characterize the arrangement of the regions of interest (ROIs) in 3D space. Methods and material: Two methods are proposed for facilitating such classification. The first method uses measures of similarity, such as the Mahalanobis distance and the Kullback–Leibler (KL) divergence, to compute the difference between spatial probability distributions of ROIs in an image of a new subject and each of the considered classes represented by historical data (e.g., normal versus disease class). A new subject is predicted to belong to the class corresponding to the most similar dataset. The second method employs the maximum likelihood (ML) principle to predict the class that most likely produced the dataset of the new subject. Results: The proposed methods have been experimentally evaluated on three datasets: synthetic data (mixtures of Gaussian distributions), realistic lesion-deficit data (generated by a simulator conforming to a clinical study), and functional MRI activation data obtained from a study designed to explore neuroanatomical correlates of semantic processing in Alzheimer's disease (AD). Conclusion: Performed experiments demonstrated that the approaches based on the KL divergence and the ML method provide superior accuracy compared to the Mahalanobis distance. The later technique could still be a method of choice when the distributions differ significantly, since it is faster and less complex. The obtained classification accuracy with errors smaller than 1% supports that useful diagnosis assistance could be achieved assuming sufficiently informative historic data and sufficient information on the new subject.",Classification Medical images Regions of interest Similarity measures Probability distributions Spatial data mining,
373,Multiscale FE method for analysis of bone micro-structures,Journal of the Mechanical Behavior of Biomedical Materials,2011,"Bones are composed of hierarchical bio-composite materials characterized by complex multiscale structural geometry and behavior. The architecture and the mechanical properties of bone tissue differ at each level of hierarchy. Thus, a multiscale approach for mechanical analysis of bone is imperative. This paper proposes a new approach for 3D multiscale finite element analysis of trabecular bone that can offer physicians a “digital magnifying glass” to facilitate continuous transition between macro- and micro-scales. The approach imitates the human ability to perceive details. That is, zooming-out from an object causes fewer details to be visible. As a result, the material appears to be smoother and more homogeneous. Zooming-in, in contrast, reveals additional details and material heterogeneity. Realization of the proposed approach requires synergy between a hierarchical geometric model for representing intermediate scales and a mechanical model for local material properties of bone tissue for each scale. The geometric model facilitates seamless and continuous bi-directional transition between macro- and micro-scales, while the mechanical model preserves the effective material properties. A 2D model of a simplified trabecular structure was implemented and analyzed in order to assess the feasibility of the proposed multiscale approach. The successful results of this model led to extending the method into 3D and analyzing real trabecular structures.",Bone micro-structure μ CT / μ MRI images Multiscale Diagnostic system,
374,Utilizing Hierarchical Multiprocessing for Medical Image Registration,"Signal Processing Magazine, IEEE",2010,"This work discusses an approach to utilize hierarchical multiprocessing in the context of medical image registration. By first organizing application parallelism into a domain-specific taxonomy, an algorithm is structured to target a set of multicore platforms.The approach on a cluster of graphics processing units (GPUs) requiring the use of two parallel programming environments to achieve fast execution times is demonstrated.There is negligible loss in accuracy for rigid registration when employing GPU acceleration, but it does adversely effect our nonrigid registration implementation due to our usage of a gradient descent approach.",image registration medical image processing parallel programming GPU acceleration application parallelism domain-specific taxonomy gradient descent approach graphics processing unit hierarchical multiprocessing medical image registration multicore platform set parallel programming Acceleration Biomedical imaging Image registration Magnetic resonance imaging Medical diagnostic imaging Multicore processing Parallel processing Robustness Signal processing algorithms Ultrasonic imaging,
375,Generalizing the data management of three community grids,Future Generation Computer Systems,2009,"Implementing efficient data management is a key challenge of grid computing. Due to seemingly different domain specific requirements, data management solutions have been developed separately for each community grid using a selection of low-level tools and APIs. This has led to unnecessarily complex and overspecialized systems. We describe three D-Grid community grid projects, AstroGrid-D, C3Grid and MediGRID, and analyze to what degree they share the same data management requirements. As a result, we derive the viewpoint that data management systems should provide applications with data access based on declarative and logical addressing, while ensuring the required quality of service (QoS). As a possible approach for this, we describe a conceptual data management system architecture that separates application, community, and resource concerns, using three layers of addressing, thus providing a highly adaptable architecture for different community grids. Additionally, we discuss approaches for the integration of legacy applications and grid scheduling with the proposed architecture.",Data management Grid computing System architecture Community grids,
376,Distributed computing methodology for training neural networks in an image-guided diagnostic application,Computer Methods and Programs in Biomedicine,2006,"Distributed computing is a process through which a set of computers connected by a network is used collectively to solve a single problem. In this paper, we propose a distributed computing methodology for training neural networks for the detection of lesions in colonoscopy. Our approach is based on partitioning the training set across multiple processors using a parallel virtual machine. In this way, interconnected computers of varied architectures can be used for the distributed evaluation of the error function and gradient values, and, thus, training neural networks utilizing various learning methods. The proposed methodology has large granularity and low synchronization, and has been implemented and tested. Our results indicate that the parallel virtual machine implementation of the training algorithms developed leads to considerable speedup, especially when large network architectures and training sets are used.",Distributed computing Parallel implementations Parallel virtual machine—PVM Backpropagation training Image-guided diagnosis and surgery,
377,Three-dimensional reconstruction algorithm for electrical resistance tomography,"Science, Measurement and Technology, IEE Proceedings -",1998,"Electrical resistance tomography (ERT) is an imaging tool for process and clinical applications in which maps of the electric conductivity distribution of a body are formed from the current-to-voltage map of the body's surface. ERT is inherently a three-dimensional problem. The transposition of two-dimensional ERT to three-dimensional (3D) ERT is a challenging task and imposes significant increase in computational power demands and storage requirements. It is demonstrated that 3D ERT is a viable offline technique for use today and can be implemented using a low-cost Pentium PC. The 3D image reconstruction algorithm implemented is based on Newton's method in which optimal experiments are used for the reconstruction process. More importantly, the algorithm incorporates 3D forward modelling, 3D data collection and reconstruction. A `fast' reconstruction algorithm is also implemented and yields a reduced set of equations which has the advantage of no longer being ill-conditioned at the expense of a loss in image resolution",biomedical measurement chemical technology computational complexity computerised tomography electric impedance imaging finite element analysis image reconstruction medical image processing mixing stereo image processing voltage measurement 3D ERT 3D data collection 3D forward modelling Newton's method Pentium PC clinical applications computational power current-to-voltage map electric conductivity distribution electrical resistance tomography equations image reconstruction algorithm offline technique optimal experiments process applications storage requirements three-dimensional reconstruction algorithm,
378,Personalized identification of abdominal wall hernia meshes on computed tomography,Computer Methods and Programs in Biomedicine,2014," An abdominal wall hernia is a protrusion of the intestine through an opening or area of weakness in the abdominal wall. Correct pre-operative identification of abdominal wall hernia meshes could help surgeons adjust the surgical plan to meet the expected difficulty and morbidity of operating through or removing the previous mesh. First, we present herein for the first time the application of image analysis for automated identification of hernia meshes. Second, we discuss the novel development of a new entropy-based image texture feature using geostatistics and indicator kriging. Third, we seek to enhance the hernia mesh identification by combining the new texture feature with the gray-level co-occurrence matrix feature of the image. The two features can characterize complementary information of anatomic details of the abdominal hernia wall and its mesh on computed tomography. Experimental results have demonstrated the effectiveness of the proposed study. The new computational tool has potential for personalized mesh identification which can assist surgeons in the diagnosis and repair of complex abdominal wall hernias.",Abdominal wall hernia mesh Computed tomography Pattern classification Geostatistical entropy Co-occurrence matrix Information fusion,
379,Parametric-based brain Magnetic Resonance Elastography using a Rayleigh damping material model,Computer Methods and Programs in Biomedicine,2014," The three-parameter Rayleigh damping (RD) model applied to time-harmonic Magnetic Resonance Elastography (MRE) has potential to better characterise fluid-saturated tissue systems. However, it is not uniquely identifiable at a single frequency. One solution to this problem involves simultaneous inverse problem solution of multiple input frequencies over a broad range. As data is often limited, an alternative elegant solution is a parametric RD reconstruction, where one of the RD parameters (μI or ρI) is globally constrained allowing accurate identification of the remaining two RD parameters. This research examines this parametric inversion approach as applied to in vivo brain imaging. Overall, success was achieved in reconstruction of the real shear modulus (μR) that showed good correlation with brain anatomical structures. The mean and standard deviation shear stiffness values of the white and gray matter were found to be 3 ± 0.11 kPa and 2.2 ± 0.11 kPa, respectively, which are in good agreement with values established in the literature or measured by mechanical testing. Parametric results with globally constrained μI indicate that selecting a reasonable value for the μI distribution has a major effect on the reconstructed ρI image and concomitant damping ratio (ξd). More specifically, the reconstructed ρI image using a realistic μI = 333 Pa value representative of a greater portion of the brain tissue showed more accurate differentiation of the ventricles within the intracranial matter compared to μI = 1000 Pa, and ξd reconstruction with μI = 333 Pa accurately captured the higher damping levels expected within the vicinity of the ventricles. Parametric RD reconstruction shows potential for accurate recovery of the stiffness characteristics and overall damping profile of the in vivo living brain despite its underlying limitations. Hence, a parametric approach could be valuable with RD models for diagnostic MRE imaging with single frequency data.",Magnetic Resonance Elastography Inverse problem methods Brain Tissue characterisation Mechanical properties Medical imaging,
380,Multi-frequency inversion in Rayleigh damped Magnetic Resonance Elastography,Biomedical Signal Processing and Control,2014," Magnetic Resonance Elastography (MRE) is able to identify mechanical properties of biological tissues in vivo based on underlying assumptions of the model used for inversion. Models, such as the linearly elastic or viscoelastic (VE), can be used with a single input frequency data and can produce a reasonable estimate of identified parameters associated with mechanical properties. However, more complex models, such as the Rayleigh damping (RD) model, are not identifiable given single frequency data without significant a priori information under certain conditions, thus limiting diagnostic potential. To overcome this limitation, two approaches have been postulated: simultaneous inversion across multiple input frequencies and a parametric approach, when only single frequency data is available. This research compares simultaneous multi-frequency (MF) RD reconstructions using both zero-order and power-law (PL) models with parametric reconstructions for a series of tissue-simulating phantoms, made of tofu and gelatine materials, tested at 4 frequencies (50 Hz, 75 Hz, 100 Hz and 125 Hz) that are commonly applied in clinical MRE examinations. Results indicate that accurate delineation of RD based properties and concomitant damping ratio (ξd) using MF inversion is still a challenging task. Specific results showed that the real shear modulus (μR) can be reconstructed well, while imaginary components representing attenuation (μI and ρI ) had much lower quality. However, overall trends correlate well with the expected higher damping levels within the saturated tofu material compared to stiff gelatine in both phantoms. Depending on the phantom configuration, measured μR values within the tofu and gelatine materials ranged from 4.77 to 7 kPa and 15.5 to 16.3 kPa, respectively, while damping levels were 11–19% and 3.1–4.3%, as expected. Correlation of the μR and ξd values with previously reported result measured by independent mechanical testing and VE based MRE is acceptable, ranging from 48 to 60%. Both PL and zero-order models produced similar qualitative and quantitate results, thus no significant advantage of the PL model was noted to account for dispersion characteristics of these types of materials. The relatively narrow range of frequencies used in this study limited practical identifiability and can thus produce a potentially false assurance of identifiability of the model parameters. We conclude that application of multiple input frequencies over a wide range, as well as selection of an appropriate model that can accurately account for dispersion characteristics of given materials are required for achieving robust practical identifiability of the RD model in time-harmonic MRE.",Magnetic Resonance Elastography Rayleigh damping Multi-frequency inversion Parametric inversion Model identifiability Mechanical properties,
381,Digital Image Elasto-Tomography: Combinatorial and Hybrid Optimization Algorithms for Shape-Based Elastic Property Reconstruction,"Biomedical Engineering, IEEE Transactions on",2008,"Results from the application of three nonlinear stiffness reconstruction algorithms to two simple cylindrical geometries are presented in this paper. Finite-element simulated harmonic motion data with added noise were initially used to represent a measured surface displacement dataset for each geometry. This motion was used as input to gradient-descent, combinatorial optimization, and hybrid reconstruction algorithms that aimed to reconstruct two shape-based parameters describing the internal stiffness of the geometry. Both the combinatorial optimization and hybrid algorithms showed significant advantages in reconstructed parameter accuracy when compared with the traditional gradient-descent approach, with success metrics improving by 13-28%. Results from the hybrid algorithm applied to silicone phantom displacements demonstrated for the first time the ability of this type of algorithm to reconstruct internal stiffness using only experimentally measured surface motion data. Improvements in the sophistication of the hybrid approach should lead to improved accuracy in reconstructed solutions, as well as enabling reconstructions where the geometry is less straightforward.",biological tissues biomechanics cancer combinatorial mathematics finite element analysis image reconstruction medical image processing optical tomography optimisation cancerous breast tissue combinatorial optimization algorithms digital image elasto-tomography elastic properties finite-element simulated harmonic motion human tissue hybrid optimization algorithms nonlinear stiffness reconstruction algorithms shape-based elastic property reconstruction silicone phantom displacements surface displacement dataset Digital images Displacement measurement Finite element methods Geometry Image reconstruction Motion measurement Noise shaping Reconstruction algorithms Solid modeling Surface reconstruction Biomedical imaging combinatorial mathematics finite-element methods inverse problems Algorithms Elasticity Elasticity Imaging Techniques Finite Element Analysis Humans Image Enhancement Image Processing Computer-Assisted Phantoms Imaging Silicones Tomography,
382,Modeling of Soft Poroelastic Tissue in Time-Harmonic MR Elastography,"Biomedical Engineering, IEEE Transactions on",2009,"Elastography is an emerging imaging technique that focuses on assessing the resistance to deformation of soft biological tissues in vivo. Magnetic resonance elastography (MRE) uses measured displacement fields resulting from low-amplitude, low-frequency (10 Hz-1 kHz) time-harmonic vibration to recover images of the elastic property distribution of tissues including breast, liver, muscle, prostate, and brain. While many soft tissues display complex time-dependent behavior not described by linear elasticity, the models most commonly employed in MRE parameter reconstructions are based on elastic assumptions. Further, elasticity models fail to include the interstitial fluid phase present in vivo. Alternative continuum models, such as consolidation theory, are able to represent tissue and other materials comprising two distinct phases, generally consisting of a porous elastic solid and penetrating fluid. MRE reconstructions of simulated elastic and poroelastic phantoms were performed to investigate the limitations of current-elasticity-based methods in producing accurate elastic parameter estimates in poroelastic media. The results indicate that linearly elastic reconstructions of fluid-saturated porous media at amplitudes and frequencies relevant to steady-state MRE can yield misleading effective property distributions resulting from the complex interaction between their solid and fluid phases.",biological tissues biomechanics biomedical MRI biomedical measurement elasticity image reconstruction image representation medical image processing parameter estimation phantoms MRE reconstruction current-elasticity-based method displacement field measurement elastic parameter estimation elastic phantom fluid-saturated porous media frequency 10 Hz to 1 kHz image representation in vivo imaging magnetic resonance elastography poroelastic phantom soft poroelastic tissue modeling solid-fluid phase interaction steady-state MRE time-harmonic MRE imaging time-harmonic vibration Biological system modeling Biological tissues Elasticity Image reconstruction Immune system In vivo Magnetic resonance Magnetic resonance imaging Solids Vibration measurement Finite-element method (FEM) magnetic resonance elastography (MRE) poroelasticity reconstructive imaging Algorithms Computer Simulation Elastic Modulus Elasticity Imaging Techniques Finite Element Analysis Humans Image Processing Computer-Assisted Models Biological Phantoms Imaging Porosity Soy Foods,
383,The Telescience Portal for advanced tomography applications,Journal of Parallel and Distributed Computing,2003,"Electron tomography is a powerful tool for deriving three-dimensional (3D) structural information about biological systems within the spatial scale spanning 1 nm3 and 10 μm3. With this technique, it is possible to derive detailed models of subcellular components such as organelles and synaptic complexes and to resolve the 3D distribution of their protein constituents in situ. While there continues to be progress towards the integration of high-performance computing technologies with traditional electron tomography processes, there is a significant need for more transparent integration with applications and to minimize the administrative overhead and complexity (resource administration, authentication, scheduling, data delivery) passed on to the non-computer scientist end user. Here we present the “Telescience Portal” (https://gridport.npaci.edu/Telescience) as an example of a fully integrated, web-based solution for performing end-to-end electron tomography. More than just a collection of individual applications, the Portal provides a transparent workflow, where simple intuitive interfaces for grid-enabled parallel computation, resource scheduling, remote instrumentation, advanced image processing and visualization, access to distributed/federated databases, and network-enabled data management and archival are tightly coupled within a secure environment which promotes increased collaboration between researchers. This tightly integrated Telescience system is a test-bed application for using grid resources to accelerate the throughput of data acquisition and processing, increase access to scarce and/or expensive instrumentation, and improve the accuracy of derived data products.",,
384,Elastic registration of brain images on large PC-Clusters,Future Generation Computer Systems,2001,"The aim of the human neuroscanning project (HNSP) is to build an atlas of a human brain at cellular level. The database was obtained by a variety of image modalities and in particular histological sections of a prepared brain. As the preparation leads to linear and non-linear deformations of the tissue, reconstructing the essential information out of deformed images is a key problem within the HNSP. Our approach of correcting these deformations is based on an elastic matching of the images. Therefore, a parallel implementation was used, since the problem in general is computational expensive and for very large scale digital images a huge amount of data has to be processed. As these requirements are in the range of today’s grand challenges, a large PC-Cluster was used to provide the performance demands. The measurements and results presented here were obtained on a cluster of 48 Dual SMP platforms connected via a Myrinet network.",Elastic matching Image registration Human brain Cluster-computing,
385,Swine Influenza Models Based Optimization (SIMBO),Applied Soft Computing,2013,"This paper introduces a new optimization technique known as Swine Influenza Model based Optimization (SIMBO). It is mimicked from Susceptible–Infectious–Recovered (SIR) models of swine flu. The development of SIMBO follows through treatment (SIMBO-T), vaccination (SIMBO-V) and quarantine (SIMBO-Q) based on probability. The SIMBO variants can be used to optimize complex multimodal functions with improved convergence and accuracy. Firstly, swine flue test based on the dynamic threshold identifies a confirmed case of swine flue. After a confirmed case of swine flue in the community, the susceptible are advised to go for the swine flue vaccination to acquire immunity. The confirmed case of swine flue is quarantined from the population. The suspected cases are treated with antiviral. The amount of antiviral drugs given to individual is dependent on patients with or without complications as well as current health of individual. In SIMBO-V and SIMBO-Q, state of the individual is updated directly through vaccination/quarantine and indirectly through treatment. The nonlinear momentum factors restrict the individuals’ treatment and state inside the defined limits without checking the health every day. SIMBO variants can easily be implemented on parallel computer architecture without having over burden or modifications. The SIMBO-T, SIMBO-V and SIMBO-Q are tested with thirteen standard benchmark functions and results are compared with other optimization techniques. The results validate that, the SIMBO variants perform comparably better. The performance of SIMBO variants are evaluated in terms of quality of optima, number of times heating stopping criteria, convergence, Fitness Evaluations (FEs), t-test, statistical parameters and analysis of variance test (ANOVA). A real time application in video motion estimation is also considered by authors to test the efficiency of the SIMBO variants. The results of motion estimation using proposed variants seems to be faster than the published methods by maintaining similar peak signal to noise ratio.",Swine Influenza Model Based Optimization (SIMBO) Vaccination Quarantine Treatment Motion estimation Peak signal to noise ratio Computational time,
386,A parallel Attainable Region construction method suitable for implementation on a graphics processing unit (GPU),Computers & Chemical Engineering,2014, A method for computing candidate Attainable Regions (ARs) suitable for implementation on a graphics processing unit (GPU) is discussed. This allows for both fast computation and complex candidate Attainable Regions to be considered. The method is tested on several problems of varying dimension with kinetics that contain multiple steady-states and temperature dependent kinetics. The computation of unbounded regions is also studied. The constructions obtained appear to approximate the AR boundary well and exhibit good convergence in a small number of iteration steps. A brief analysis of the convergence characteristics of the method is also provided and compared to the same algorithm implemented on a conventional multicore central processing unit (CPU).,Attainable Regions Reactor networks GPGPU CUDA Chemical reactors Parallelization,
387,3-D maximum a posteriori estimation for single photon emission computed tomography on massively-parallel computers,"Medical Imaging, IEEE Transactions on",1993,"A fully three-dimensional (3-D) implementation of the maximum a posteriori (MAP) method for single photon emission computed tomography (SPECT) is demonstrated. The 3-D reconstruction exhibits a major increase in resolution when compared to the generation of the series of separate 2-D slice reconstructions. As has been noted, the iterative EM algorithm for 2-D reconstruction is highly computational; the 3-D algorithm is far worse. To accommodate the computational complexity, previous work in the 2-D arena is extended, and an implementation on the class of massively parallel processors of the 3-D algorithm is demonstrated. Using a 16000- (4000-) processor MasPar/DECmpp-Sx machine, the algorithm is demonstrated to execute at 2.5 (7.8) s/EM-iteration for the entire 64Ã64Ã64 cube of 96 planar measurements obtained from the Siemens Orbiter rotating camera operating in the high-resolution mode",computerised tomography radioisotope scanning and imaging 2D slice reconstructions 3D maximum a posteriori estimation MasPar/DECmpp-Sx machine SPECT Siemens Orbiter rotating camera high-resolution mode iterative EM algorithm massively-parallel computers medical diagnostic imaging nuclear medicine planar measurements single photon emission computed tomography Attenuation Detectors Image reconstruction Iterative algorithms Maximum a posteriori estimation Maximum likelihood detection Maximum likelihood estimation Optical collimators Optical computing Single photon emission computed tomography,
388,Parallel processing for image and video processing: Issues and challenges,Parallel Computing,2008,"Some meaningful hints about parallelization problems in image processing and analysis are discussed. The issues of the operation of various architectures used to solve vision problems, from the pipeline of dedicated operators to general purpose MIMD machines, passing through specialized SIMD machines and processors with extended instruction sets, and parallelization tools, from parallel library to parallel programming languages, are reviewed. In this context, a discussion of open issues and directions for future research is provided.",Image analysis Computer vision Parallel computing Distributed computing,
389,Acceleration method of 3D medical images registration based on compute unified device architecture,Bio-medical materials and engineering,2014,"Compute Unified Device Architecture (CUDA) is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increase in computing performance via the power of the graphics processing unit (GPU). In medical image analysis, 3D image registration generally takes relatively long time, which is not feasible for clinical applications. To solve this problem, this paper proposed a high performance computational method based on CUDA, which took full advantage of GPU parallel computing under CUDA architecture combined with image multiple scale and maximum mutual information. Experiments showed that this algorithm can not only maintain the registration accuracy but also greatly increase the speed of registration process and meet the real-time requirement of clinical application.",,
390,3D Medical Images Registration Based on GPU Parallel Computing,,2013,"Real time 3D medical image registration method is key technology of medical image processing, especially in surgical operation navigation. However, current 3D medical image registration methods are time-consuming, which can't meet the real time requirement of clinical application. To solve this problem, this paper presented a high performance computational method based on CUDA ( Compute Unified Device Architecture), which took full advantage of GPU parallel computing under CUDA architecture combined with image multiple scale and maximum mutual information to make fast registration of three dimensional medical image. Experiments showed that this algorithm can greatly accelerate the computational speed of registration of three dimensional medical image, and meet the real time requirement of clinical application.",,
391,Towards a performance-portable description of geometric multigrid algorithms using a domain-specific language,Journal of Parallel and Distributed Computing,2014," High Performance Computing (HPC) systems are nowadays more and more heterogeneous. Different processor types can be found on a single node including accelerators such as Graphics Processing Units (GPUs). To cope with the challenge of programming such complex systems, this work presents a domain-specific approach to automatically generate code tailored to different processor types. Low-level CUDA and OpenCL code is generated from a high-level description of an algorithm specified in a Domain-Specific Language (DSL) instead of writing hand-tuned code for GPU accelerators. The DSL is part of the Heterogeneous Image Processing Acceleration (HIPAcc) framework and was extended in this work to handle grid hierarchies in order to model different cycle types. Language constructs are introduced to process and represent data at different resolutions. This allows to describe image processing algorithms that work on image pyramids as well as multigrid methods in the stencil domain. By decoupling the algorithm from its schedule, the proposed approach allows to generate efficient stencil code implementations. Our results show that similar performance compared to hand-tuned codes can be achieved.",Multigrid Multiresolution Image pyramid Domain-specific language Stencil codes Code generation GPU CUDA OpenCL,
392,"A metacomputing environment for demanding applications: design, implementation, experiments and business benefit",Future Generation Computer Systems,1999,"The paper describes the design, implementation, and use of a commercial metacomputing environment for computationally intensive loosely-coupled parallel applications. Much weight has been laid on practical and commercialisation aspects, and on business benefit. This distinguishes this work from many other metacomputing activities in a positive way. It demonstrates how a metacomputing environment can be used to improve a company’s position in the market. A cluster of networked geographically dispersed computing nodes is considered as physical layer. The proposed distribution of work over the nodes of the execution network is proven optimal, in terms of minimizing the execution time, with respect to the availability of resources. We also present our experience on testing the environment for computing-intensive 3D-rendering jobs derived from the ESPRIT project EROPPA and demonstrate that the new environment can change dramatically the character of the post production business.",Distributed high-performance computing Metacomputing Cluster computing Post production Remote rendering Distributed rendering CODINE Job management Turnaround time Loosely-coupled parallelism EROPPA,
393,Intelligent grid enabled services for neuroimaging analysis,Neurocomputing,2013," This paper reports our work in the context of the neuGRID project in the development of intelligent services for a robust and efficient Neuroimaging analysis environment. neuGRID is an EC-funded project driven by the needs of the Alzheimer's disease research community that aims to facilitate the collection and archiving of large amounts of imaging data coupled with a set of services and algorithms. By taking Alzheimer's disease as an exemplar, the neuGRID project has developed a set of intelligent services and a Grid infrastructure to enable the European neuroscience community to carry out research required for the study of degenerative brain diseases. We have investigated the use of machine learning approaches, especially evolutionary multi-objective meta-heuristics for optimising scientific analysis on distributed infrastructures. The salient features of the services and the functionality of a planning and execution architecture based on an evolutionary multi-objective meta-heuristics to achieve analysis efficiency are presented. We also describe implementation details of the services that will form an intelligent analysis environment and present results on the optimisation that has been achieved as a result of this investigation.",Intelligent services Machine learning and genetic algorithms Grid enabled planning and execution Service oriented architecture Neuroimaging analysis,
394,High performance medical image processing in client/server-environments,Computer Methods and Programs in Biomedicine,1999,"As 3D scanning devices like computer tomography (CT) or magnetic resonance imaging (MRI) become more widespread, there is also an increasing need for powerful computers that can handle the enormous amounts of data with acceptable response times. We describe an approach to parallelize some of the more frequently used image processing operators on distributed memory architectures. It is desirable to make such specialized machines accessible on a network, in order to save costs by sharing resources. We present a client/server approach that is specifically tailored to the interactive work with volume data. Our image processing server implements a volume visualization method that allows the user to assess the segmentation of anatomical structures. We can enhance the presentation by combining the volume visualizations on a viewing station with additional graphical elements, which can be manipulated in real-time. The methods presented were verified on two applications for different domains.",Client/server environments Image processing Volume visualization Hybrid visualization Parallel processing,
395,A Medical Informatics View of Quantum Computation,Neuroquantology,2011,"Many medical centers lack access to the parallel-computing capability that is now needed for large-scale, genetic diagnostic information. Such enhanced computer resources are also needed for recent diagnostic physiological model simulations, and for enhancements in medical imaging. Increases in classical parallel processing capability are incremental and expensive. Quantum computing is the only new computing method that promises a very-large increase in parallel-processing capability. The reason is that the classical-computing unit of information, the 0-bit, or the 1-bit, is replaced by the quantum qubit unit which can be superposed as multiple strings of 0's and 1's. Medical-center research units may also need enhanced parallel computing ability for genome research, protein structure prediction, and the use of new simulations of physiological models. It is suggested that health-research funding and collaboration needs to be directed towards the final development of large-scale quantum computing.",,
396,Calibration Using Matrix Completion With Application to Ultrasound Tomography,"Signal Processing, IEEE Transactions on",2013,"We study the application of matrix completion in the process of calibrating physical devices. In particular we propose an algorithm together with reconstruction bounds for calibrating circular ultrasound tomography devices. We use the time-of-flight (ToF) measurements between sensor pairs in a homogeneous medium to calibrate the system. The calibration process consists of a low-rank matrix completion algorithm to de-noise and estimate random and structured missing ToFs, and the classic multi-dimensional scaling method to estimate the sensor positions from the ToF measurements. We provide theoretical bounds on the calibration error. Several simulations are conducted to evaluate the theoretical results presented in this paper.",acoustic tomography calibration matrix algebra sensors ultrasonic transducers ToF measurements calibration error circular ultrasound tomography devices denoising low-rank matrix completion algorithm matrix completion physical device calibration sensor time-of-flight measurements Calibration Instruments Inverse problems Receivers Tomography Transmitters Ultrasonic imaging Calibration matrix completion multidimensional scaling sensor localization ultrasound tomography,
397,A new era in scientific computing: Domain decomposition methods in hybrid CPU–GPU architectures,Computer Methods in Applied Mechanics and Engineering,2011,"Recent advances in graphics processing units (GPUs) technology open a new era in high performance computing. Applications of GPUs to scientific computations are attracting a lot of attention due to their low cost in conjunction with their inherently remarkable performance features and the recently enhanced computational precision and improved programming tools. Domain decomposition methods (DDM) constitute today an important category of methods for the solution of highly demanding problems in simulation-based applied science and engineering. Among them, dual domain decomposition methods have been successfully applied in a variety of problems in both sequential as well as in parallel/distributed processing systems. In this work, we demonstrate the implementation of the FETI method to a hybrid CPU–GPU computing environment. Parametric tests on implicit finite element structural mechanics benchmark problems revealed the tremendous potential of this type of hybrid computing environment as a result of the full exploitation of multi-core CPU hardware resources and the intrinsic software and hardware features of the GPUs as well as the numerical properties of the solution method.",Hybrid computing Multi-core processing Many-core processing Graphics processing units Domain decomposition methods FETI method,
398,Interactive image segmentation based on synthetic graph coordinates,Pattern Recognition,2013," In this paper, we propose a framework for interactive image segmentation. The goal of interactive image segmentation is to classify the image pixels into foreground and background classes, when some foreground and background markers are given. The proposed method minimizes a min–max Bayesian criterion that has been successfully used on image segmentation problem and it consists of several steps in order to take into account visual information as well as the given markers, without any requirement of training. First, we partition the image into contiguous and perceptually similar regions (superpixels). Then, we construct a weighted graph that represents the superpixels and the connections between them. An efficient algorithm for graph clustering based on synthetic coordinates is used yielding an initial map of classified pixels. This method reduces the problem of graph clustering to the simpler problem of point clustering, instead of solving the problem on the graph data structure, as most of the known algorithms from literature do. Finally, having available the data modeling and the initial map of classified pixels, we use a Markov Random Field (MRF) model or a flooding algorithm to get the image segmentation by minimizing a min–max Bayesian criterion. Experimental results and comparisons with other methods from the literature are presented on LHI, Gulshan and Zhao datasets, demonstrating the high performance and accuracy of the proposed scheme.",Image segmentation Interactive image segmentation Network coordinates Community detection Markov Random Field,
399,Markov surfaces: A probabilistic framework for user-assisted three-dimensional image segmentation,Computer Vision and Image Understanding,2011,"This paper presents Markov surfaces, a probabilistic algorithm for user-assisted segmentation of elongated structures in 3D images. The 3D segmentation problem is formulated as a path-finding problem, where path probabilities are described by Markov chains. Users define points, curves, or regions on 2D image slices, and the algorithm connects these user-defined features in a way that respects the underlying elongated structure in data. Transition probabilities in the Markov model are derived from intensity matches and interslice correspondences, which are generated from a slice-to-slice registration algorithm. Bézier interpolations between paths are applied to generate smooth surfaces. Subgrid accuracy is achieved by linear interpolations of image intensities and the interslice correspondences. Experimental results on synthetic and real data demonstrate that Markov surfaces can segment regions that are defined by texture, nearby context, and motion. A parallel implementation on a streaming parallel computer architecture, a graphics processor, makes the method interactive for 3D data.",Image segmentation Probabilistic framework Markov chain GPU,
400,Parallel Multiscale Feature Extraction and Region Growing: Application in Retinal Blood Vessel Detection,"Information Technology in Biomedicine, IEEE Transactions on",2010,"This paper presents a parallel implementation based on insight segmentation and registration toolkit for a multiscale feature extraction and region growing algorithm, applied to retinal blood vessels segmentation. This implementation is capable of achieving an accuracy (Ac) comparable to its serial counterpart (about 92%), but 8 to 10 times faster. In this paper, the Ac of this parallel implementation is evaluated by comparison with expert manual segmentation (obtained from public databases). On the other hand, its performance is compared with previous published serial implementations. Both these characteristics make this parallel implementation feasible for the analysis of a larger amount of high-resolution retinal images, achieving a faster and high-quality segmentation of retinal blood vessels.",biomedical optical imaging blood vessels eye feature extraction image registration image segmentation medical image processing parallel processing expert manual segmentation comparison feature extraction algorithm high resolution retinal images image registration image segmentation parallel multiscale feature extraction parallel multiscale region growing region growing algorithm retinal blood vessel detection retinal blood vessels segmentation Data processing distributed algorithms image analysis image processing image segmentation parallel programming Algorithms Databases Factual Diagnostic Techniques Ophthalmological Humans Image Processing Computer-Assisted Reproducibility of Results Retina Retinal Vessels,
401,Micro-CT image reconstruction based on alternating direction augmented Lagrangian method and total variation,Computerized Medical Imaging and Graphics,2013," Micro-computed tomography (micro-CT) plays an important role in pre-clinical imaging. The radiation from micro-CT can result in excess radiation exposure to the specimen under test, hence the reduction of radiation from micro-CT is essential. The proposed research focused on analyzing and testing an alternating direction augmented Lagrangian (ADAL) algorithm to recover images from random projections using total variation (TV) regularization. The use of TV regularization in compressed sensing problems makes the recovered image quality sharper by preserving the edges or boundaries more accurately. In this work TV regularization problem is addressed by ADAL which is a variant of the classic augmented Lagrangian method for structured optimization. The per-iteration computational complexity of the algorithm is two fast Fourier transforms, two matrix vector multiplications and a linear time shrinkage operation. Comparison of experimental results indicate that the proposed algorithm is stable, efficient and competitive with the existing algorithms for solving TV regularization problems.",Image reconstruction Total variation Compressed sensing Alternating direction augmented Lagrangian,
402,Partition-induced connections and operators for pattern analysis,Pattern Recognition,2010,"In this paper we present a generalization on the notion of image connectivity similar to that modeled by second-generation connections. The connected operators based on this new type of connection make use of image partitions aided by mask images to  path-wise connected regions that were previously treated as sets of singletons. This leads to a redistribution of image power which affects texture descriptors. These operators find applications in problems involving contraction-based connectivities, and we show how they can be used to counter the over-segmentation problem of connected filters. Despite restrictions which prevent extensions to gray-scale, we present a method for gray-scale spectral analysis of biomedical images characterized by filamentous details. Using connected pattern spectra as feature vectors to train a classifier we show that the new operators outperform the existing contraction-based ones and that the classification performance competes with, and in some cases outperforms methods based on the standard 4- or 8-connectivity. Finally, combining the two methods we enrich the texture description and increase the overall classification rate.",Image analysis Mathematical morphology Connected filters Connectivity classes Diatoms,
403,Hardware Acceleration of an Efficient and Accurate Proton Therapy Monte Carlo,Procedia Computer Science,2013," Proton radiation therapy is one of the more effective forms of cancer treatment because of the high degree of selectivity afforded by the behavior of energetic protons in matter. But because radiation does not distinguish between tumor cells and healthy body tissue, it is important to insure that the radiation energy is deposited in the appropriate locations within a patient. This is even more important for proton beams because of the concentrated nature of the radiation energy dose they leave in a body. Predicting such dose distributions can be accurately done via complex and slow Monte Carlo based simulation (using tools such as Geant), but such simulators are too slow for use in interactive situations where a doctor is trying to determine the best beams to use for a particular patient. In this paper we report on an accurate but extremely fast Monte Carlo based proton dose distribution simulator code named Jack. The simulator uses the same physics as more complex tools, but leverages massive parallelization and a streamlined code architecture. The paper describes the state of Jack and shows runtime results for it with and without various hardware acceleration techniques. We benchmark Jack against Geant4.9.4.p01, a well established particle transport code, on a water phantom. Future plans are presented at the end for further speed enhancement and model development.",proton therapy cancer massively parallel systems GPU POWER7 Monte Carlo,
404,Cybernetics of Vision Systems: Toward an Understanding of Putative Functions of the Outer Retina,"Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on",2011,"The retina still poses many structural and computational questions. Structurally, for example, it is not yet clear how many distinct horizontal cell (HC) types the primate retina contains and what the exact patterns of connections between photoreceptors (PRs) and HCs consist of. Computationally, it is not yet clear, for instance, what functions are present and how they are being implemented. This paper proposes a model (a linear recurrent neural network defined by 31 parameters) of the outer retina and an optimization methodology that hopes to shed some light on these questions. This paper shows that a simplified model of the outer retina can implement several low-level visual functions involving the modulation of noise, brightness, contrast, saturation, and even color. The results demonstrate that contrast control functions can be implemented with a minimum of two HC types and that spectral specificity between PRs and HCs is a common and important feature. It is also shown that several different spectrally specific patterns can emerge in order to implement the same function. One interesting microcircuit that naturally emerged from our experiments involves nonblurry denoising via interchromatic gap junctions and compensatory resaturation via HC circuits, a strategy that we hypothesize to exist in some biological retinae.",computer vision optimisation recurrent neural nets compensatory resaturation contrast control functions cybernetics horizontal cell types interchromatic gap junctions linear recurrent neural network nonblurry denoising optimization methodology outer retina photoreceptors primate retina putative functions vision systems visual functions Biological system modeling Computational modeling Image color analysis Optimization Retina Visualization Cones contrast enhancement denoising horizontal cells (HCs) image processing outer retina,
405,Improving MRI segmentation with probabilistic GHSOM and multiobjective optimization,Neurocomputing,2013,"In the last years, the improvements in Magnetic Resonance Imaging systems (MRI) provide new and additional ways to diagnose some brain disorders such as schizophrenia or the Alzheimer disease. One way to figure out these disorders from a MRI is through image segmentation. Image segmentation consist in partitioning an image into different regions. These regions determine different tissues present on the image. This results in a very interesting tool for neuroanatomical analyses. In this paper we present a segmentation method based on the Growing Hierarchical Self-Organizing Map and multiobjective-based feature selection to optimize the performance of the segmentation process. Since the features extracted from the image result crucial for the final performance of the segmentation process, optimized features are computed to maximize the performance of the segmentation process on each plane. The experiments performed on this paper use real brain scans from the Internet Brain Segmentation Repository (IBSR) and the Alzheimer Disease Neuroimaging Initiative (ADNI). Moreover, a comparison with other methods using the IBSR database shows that our method outperforms other algorithms.",MRI Image segmentation Multiobjective optimization Self-Organizing Maps,
406,Toward the Multi-scale Simulation for a Human Body Using the Next-generation Supercomputer,Procedia IUTAM,2014," We have developed novel numerical methods for fluid-structure and fluid-membrane interaction problems. The basic equation set is formulated in a full Eulerian framework. The method is based on the finite difference volume-of-fluid scheme with fractional step algorithm. It is validated through a numerical solution to a deformable vesicle problem, and applied to blood flows including red blood cells (RBCs) and platelets. Further, to gain insight into the mechanism of thrombus formation, a stochastic Monte Carlo model to describe the platelet-vessel wall interaction is incorporated into the Eulerian method. The effect of the RBCs on the platelet motion is discussed.",Eulerian method fluid-structure interaction blood flow stochastic Monte Carlo method,
407,Design and performance of a pixel-level pipelined-parallel architecture for high speed wavelet-based image compression,Computers & Electrical Engineering,2005,"Wavelets have widely been used in many signal and image processing applications. In this paper, a new serial-parallel architecture for wavelet-based image compression is introduced. It is based on a 4-tap wavelet transform, which is realised using some FIFO memory modules implementing a pixel-level pipeline architecture to compress and decompress images. The real filter calculation over 4 × 4 window blocks is done using a tree of carry save adders to ensure the high speed processing required for many applications. The details of implementing both compressor and decompressor sub-systems are given. The primarily analysis reveals that the proposed architecture, implemented using current VLSI technologies, can process a video stream in real time.",Image processing Compression Decompression Wavelet transform Serial-parallel architecture Pipelining FPGA implementation Performance,
408,Multi-sensor fusion: an Evolutionary algorithm approach,Information Fusion,2006,"Modern decision-making processes rely on data coming from different sources. Intelligent integration and fusion of information from distributed multi-source, multi-sensor network requires an optimization-centered approach. Traditional optimization techniques often fail to meet the demands and challenges of highly dynamic and volatile information flow. New methods are required, which are capable of fully automated adjustment and self-adaptation to fluctuating inputs and tasks. One such method is Evolutionary algorithms (EA), a generic, flexible, and versatile framework for solving complex problems of global optimization and search in real world applications. The evolutionary approach provides a valuable alternative to traditional methods used in information fusion, due to its inherent parallel nature and its ability to deal with difficult problems. However, the application of the algorithm to a particular problem is often more an art than science. Choosing the right model and parameters requires an in-depth understanding of the morphological development of the algorithm, as well as its recent advances and trends. This paper attempts to give a compact overview of both basic and advanced concepts, models, and variants of Evolutionary algorithms in various implementations and applications particularly those in information fusion. We have brought together material scattered throughout numerous books, journal papers, and conference proceedings. Strong emphasis is made on the practical aspects of the EA implementation, including specific and detailed recommendations drawn from these various sources. However, the practical aspects are discussed from the standpoint of concepts and models, rather than from applications in specific problem domains, which emphasize the generality of the provided recommendations across different applications including information fusion.",Information fusion Global optimization Heuristic methods Evolutionary algorithms Genetic algorithm Evolution strategies Evolutionary programming Genetic programming,
409,Parallel linear congruential generators with Sophie-Germain moduli,Parallel Computing,2004,"Monte Carlo simulations are thought to be very easy to parallelize; however, the quality of these parallel Monte Carlo computations depends greatly on the quality of the parallel random number generators used. Linear congruential generators (LCGs), the most common number-theoretic pseudorandom number generators, with both power-of-two and prime moduli are used in many popular implementations of pseudorandom number generators. Recently, one of the authors of this paper [M. Mascagni, Parallel linear congruential generators with prime moduli, Parallel Comput. 24 (1998) 923-936] developed an explicit parameterization of prime modulus LCGs for use in parallel computations. This approach was based on an explicit enumeration of all the primitive roots modulo the prime modulus for use as unique multipliers in each parallel LCG. In that paper, only Mersenne prime moduli were considered because of the existence of a fast modular multiplication algorithm for primes close to powers-of-two. In the current paper, we investigate the nature of the trade-off implicitly made in the choice of Mersenne primes by comparing them to parameterized Sophie-Germain prime modulus LCGs. While the choice of Mersenne primes trades off initialization time for generation time, the choice of Sophie-Germain primes not only largely reduces initialization time but also provides competitive generation time when an appropriately chosen Sophie-Germain primes are used. The resulting Sophie-Germain prime modulus LCGs have been tested, and incorporated into the Scalable Parallel Random Number Generators SPRNG library [SPRNG. Scalable parallel random number generators, http://sprng.fsu.edu], a widely used random number generation suite for parallel, distributed, and grid-based Monte Carlo computations [M. Mascagni, A. Srinivasan, Computational infrastructure for parallel, distributed, and grid-based Monte Carlo computations, Lect. Notes Comput. Sci. 2907 (2004) 39-52]. [All rights reserved Elsevier].",grid computing Monte Carlo methods parallel algorithms random number generation simulation,
410,Cross-Approximate Entropy parallel computation on GPUs for biomedical signal analysis. Application to MEG recordings,Computer Methods and Programs in Biomedicine,2013," Cross-Approximate Entropy (Cross-ApEn) is a useful measure to quantify the statistical dissimilarity of two time series. In spite of the advantage of Cross-ApEn over its one-dimensional counterpart (Approximate Entropy), only a few studies have applied it to biomedical signals, mainly due to its high computational cost. In this paper, we propose a fast GPU-based implementation of the Cross-ApEn that makes feasible its use over a large amount of multidimensional data. The scheme followed is fully scalable, thus maximizes the use of the GPU despite of the number of neural signals being processed. The approach consists in processing many trials or epochs simultaneously, with independence of its origin. In the case of MEG data, these trials can proceed from different input channels or subjects. The proposed implementation achieves an average speedup greater than 250× against a CPU parallel version running on a processor containing six cores. A dataset of 30 subjects containing 148 MEG channels (49 epochs of 1024 samples per channel) can be analyzed using our development in about 30 min. The same processing takes 5 days on six cores and 15 days when running on a single core. The speedup is much larger if compared to a basic sequential Matlab® implementation, that would need 58 days per subject. To our knowledge, this is the first contribution of Cross-ApEn measure computation using GPUs. This study demonstrates that this hardware is, to the day, the best option for the signal processing of biomedical data with Cross-ApEn.",Cross Approximate Entropy CUDA GPGPU Magnetoencephalography Neural signal analysis,
411,Automatic detection of Parkinsonism using significance measures and component analysis in DaTSCAN imaging,Neurocomputing,2014," The study of neurodegenerative diseases has been based for some time on visual and semi-quantitative analysis of medical imaging. This is the case of Parkinsonian Syndrome (PS) or Parkinsonism, which is the second most common neurodegenerative disorder, where 123I-ioflupane (better known by its tradename, DaTSCAN) images have been of great help. Recently, new developments in machine learning methods and statistics have been applied to the analysis of medical images, yielding to a more operator-independent, objective analysis of them, and thus, setting the Computer Aided Diagnosis (CAD) paradigm. In this work, a new CAD system based on preprocessing, voxel selection, feature extraction and classification of the images is proposed. After preprocessing the images, voxels are ranked by means of their significance in class discrimination, and the first N are selected. Then, these voxels are modelled using Independent Component Analysis (ICA), obtaining a few components that represent each image, which will be used later to train a classifier. The proposed system has been tested on two databases: a 208-DaTSCAN image database from the “Virgen de la Victoria” Hospital in Málaga (VV), Spain and a 289-DaTSCAN image database from the Parkinson Progression Markers Initiative (PPMI). Values of accuracy up to 94.7% and 91.3% for VV and PPMI databases are achieved by the proposed system, which has proved its robustness in PS pattern detection, and significantly improves the baseline Voxels-as-Features (VAF) approach, used as an approximation of the visual analysis.",Parkinsonian Syndrome (PS) Computer Aided Diagnosis (CAD) Statistical significance Independent Component Analysis (ICA) Support Vector Machines (SVM) DaTSCAN,
412,Tracking by means of geodesic region models applied to multidimensional and complex medical images,Computer Vision and Image Understanding,2011,"From surgery to radiotherapy treatment planning, tracking organs or tissues is a fundamental task. The techniques used to achieve this tracking can be classified as: extrinsic and intrinsic. Intrinsic techniques only use image processing methods applied to medical images or sequences, as dealt with in this paper. To accurately perform this organ tracking it is necessary to find tracking models that can be applied to various image modalities involved in medical procedures (CT, MRI, etc.). Moreover these models must handle several image dimensions (2D, 3D, and 4D) that are common in many medical tasks. Among the several alternatives for tracking the organs of interest, a model based on a geodesic one combined with regional features is proposed. This model has been tested on CT images from the pelvic, cardiac and thoracic area. A novel model for the segmentation of organs composed of more than one region is proposed.",Tracking organs Geodesic models Medical images Multiple region organs 3D and 4D images,
413,Efficient data partitioning for the GPU computation of moment functions,Journal of Parallel and Distributed Computing,2014," In our previous work, we have provided tools for an efficient characterization of biomedical images using Legendre and Zernike moments, showing their relevance as biomarkers for classifying image tiles coming from bone tissue regeneration studies (Ujaldón, 2009) [24]. As part of our research quest for efficiency, we developed methods for accelerating those computations on GPUs (Martín-Requena and Ujaldón, 2011) [10,9]. This new stage of our work focuses on the efficient data partitioning to optimize the execution on many-cores and clusters of GPUs to attain gains up to three orders of magnitude when compared to the execution on multi-core CPUs of similar age and cost using 1 Mpixel images. We deploy a successive and successful chain of optimizations which exploit symmetries in trigonometric functions and access patterns to image pixels which are effectively combined with massive data parallelism on GPUs to enable (1) real-time processing for our set of input biomedical images, and (2) the use of high-resolution images in clinical practice.",Zernike moments Image features High performance computing GPU Data partitioning,
414,The establishment of a pilot telemedical information society,Future Generation Computer Systems,1999,"National and international telecommunication infrastructures have been set up through Europe to facilitate the movement of information. One major benefactor of the improved communication infrastructures is the health care community. The accessibility and interoperability of medical information systems is one of the grand challenges for the 21st century. Within Europe current developments in the application of telemedicine are being defined in separate initiatives. There are a number of pilot actions concentrating on various aspects of telemedicine. These actions involving the introduction of new technology or working practices rarely fail for technology related problems. However, in order to fully assess the likely take-up of telemedical technologies it is vital that all the aspects including non-technical are also addressed. This paper describes how a complete pilot telemedical information society will be set up which facilitates to support secure and standardised remote diagnosis, teleconsultations and advanced medical facilities in a number of sectors covering a crucial spectrum of those required to support a complete telemedical information society. This pilot testbed will then be assessed in the context of a European environment identifying a business plan for its extension to other member states therefore promoting a truly international telemedical information society for the 21st century.",Telemedicine Information society World Wide Web,
415,Computer Tomography 3D Edge Detection Comparative for Metrology Applications,Procedia Engineering,2013," The CT process for metrology applications is very complex because has many factors that influence the loss of accuracy during CT measurements. One of the most critical is the edge detection also called surface extraction or image segmentation, which is the process of surface formation from the CT‘s volume data that represents a grey value corresponding to the mass attenuation coefficient of the object material. This paper presents different edge detection methods commonly used in areas like machine and computer vision and they are analyzed as an alternative to the common methods used in CT for metrology applications. Each method is described and analyzed separately in order to highlight its advantages and disadvantages from a metrological point of view. An experimental comparative between two of them is also shown.",Edge detection metrology computed metrology.,
416,Maximum-likelihood reconstruction of transmission images in emission computed tomography via the EM algorithm,"Medical Imaging, IEEE Transactions on",1994,"The expectation-maximization (EM) algorithm for computing maximum-likelihood estimates of transmission images in positron-emission tomography (PET) (see K. Lange and R. Carson, J. Comput. Assist. Tomogr., vol.8, no.2, p.306-16, 1984) is extended to include measurement error, accidental coincidences and Compton scatter. A method for accomplishing the maximization step using one step of Newton's method is proposed. The algorithm is regularized with the method of sieves. Evaluations using both Monte Carlo simulations and phantom studies on the Siemens 953B scanner suggest that the algorithm yields unbiased images with significantly lower variances than filtered-backprojection when the images are reconstructed to the intrinsic resolution. Large features in the images converge in under 200 iterations while the smallest features required up to 2,000 iterations. All but the smallest features in typical transmission scans converge in approximately 250 iterations. The initial implementation of the algorithm requires 50 sec per iteration on a DECStation 5000",computerised tomography image reconstruction medical image processing radioisotope scanning and imaging 50 s Compton scatter DECStation 5000 EM algorithm Monte Carlo simulations Newton's method Siemens 953B scanner accidental coincidences algorithm regularization emission computed tomography filtered-backprojection lower variance images maximum-likelihood reconstruction measurement error medical diagnostic imaging method of sieves phantom studies positron-emission tomography transmission images unbiased images Additive noise Attenuation Computed tomography Electrical capacitance tomography Image converters Image reconstruction Maximum likelihood estimation Positron emission tomography Statistics X-ray imaging,
417,Towards applying content-based image retrieval in the clinical routine,Future Generation Computer Systems,2007,"Content-based image retrieval (CBIR) has been one the most vivid research areas in the field of computer vision, and substantial progress has been made over the last years. As such, many have argued for the use of CBIR to support medical imaging diagnosis. However, the sheer volume of data produced in radiology centers has precluded the use of CBIR in the daily routine of hospitals and clinics. This paper aims to change this status quo. We here present a solution that applies Computational Grids to significantly speed up the CBIR procedure, while preserving the security of data in the clinical routine. This solution combines texture attributes and registration algorithms that together are capable of retrieving images with greater-than-90% precision, yet running in a few minutes over the Grid, making it usable in the clinical routine.",Content-based image retrieval Texture attributes Image registration Grid Computing,
418,Special section: Medical imaging on grids,Future Generation Computer Systems,2010,,,
419,High-performance cone beam reconstruction using CUDA compatible GPUs,Parallel Computing,2010,"Compute unified device architecture (CUDA) is a software development platform that allows us to run C-like programs on the nVIDIA graphics processing unit (GPU). This paper presents an acceleration method for cone beam reconstruction using CUDA compatible GPUs. The proposed method accelerates the Feldkamp, Davis, and Kress (FDK) algorithm using three techniques: (1) off-chip memory access reduction for saving the memory bandwidth; (2) loop unrolling for hiding the memory latency; and (3) multithreading for exploiting multiple GPUs. We describe how these techniques can be incorporated into the reconstruction code. We also show an analytical model to understand the reconstruction performance on multi-GPU environments. Experimental results show that the proposed method runs at 83% of the theoretical memory bandwidth, achieving a throughput of 64.3 projections per second (pps) for reconstruction of 5123-voxel volume from 360 5122-pixel projections. This performance is 41% higher than the previous CUDA-based method and is 24 times faster than a CPU-based method optimized by vector intrinsics. Some detailed analyses are also presented to understand how effectively the acceleration techniques increase the reconstruction performance of a naive method. We also demonstrate out-of-core reconstruction for large-scale datasets, up to 10243-voxel volume.",Cone beam reconstruction Acceleration GPU CUDA,
420,Interpolating an unorganized 2D point cloud with a single closed shape,Computer-Aided Design,2011,"Given an unorganized two-dimensional point cloud, we address the problem of efficiently constructing a single aesthetically pleasing closed interpolating shape, without requiring dense or uniform spacing. Using Gestalt’s laws of proximity, closure and good continuity as guidance for visual aesthetics, we require that our constructed shape be a minimal perimeter, non-self intersecting manifold. We find that this yields visually pleasing results. Our algorithm is distinct from earlier shape reconstruction approaches, in that it exploits the overlap between the desired shape and a related minimal graph, the Euclidean Minimum Spanning Tree ( E M S T ). Our algorithm segments the E M S T to retain as much of it as required and then locally partitions and solves the problem efficiently. Comparison with some of the best currently known solutions shows that our algorithm yields better results.",Computational geometry Reconstruction Construction Shape Curve Boundary Point cloud Point set EMST,
421,Roughness penalties on finite domains,"Image Processing, IEEE Transactions on",1995,"A class of penalty functions for use in estimation and image regularization is proposed. These penalty functions are defined for vectors whose indexes are locations in a finite lattice as the discrepancy between the vector and a shifted version of itself. After motivating this class of penalty functions, their relationship to Markov random field priors is explored. One of the penalty functions proposed, a divergence roughness penalty, is shown to be a discretization of a penalty proposed by Good and Gaskins (1971) for use in density estimation. One potential use in estimation problems is explored. An iterative algorithm that takes advantage of induced neighborhood structures is proposed and convergence of the algorithm is proven under specified conditions. Examples in emission tomographic imaging and radar imaging are given",Markov processes convergence of numerical methods discrete systems emission tomography estimation theory image processing iterative methods medical image processing radar imaging random processes Markov random field priors convergence density estimation discretization divergence roughness penalty emission tomographic imaging estimation problems finite domain finite lattice image regularization induced neighborhood structures iterative algorithm penalty functions radar imaging Convergence Gaussian processes Helium Image converters Iterative algorithms Lattices Markov random fields Radar imaging Senior members Tomography,
422,Visualization in scientific and engineering computation,Computer,1991,"Some applications of visualization, based on an information flow model, are discussed. The model, which is described in detail, is designed to help in grasping an overview of this subject. It serves as a framework for describing various topics related to visualization in scientific and engineering computation. It consists of mathematical modeling, numerical solution and scientific computing, graph evaluation by extracting geometry, and transforming and rendering. To indicate trends and directions in visualization, three rather broad representative issues are discussed. They are algorithms and techniques for multidimensional data, user interfaces, and workload distribution and computing/network requirements.<>",computer graphics data analysis graph evaluation information flow model mathematical modeling multidimensional data numerical solution rendering scientific computing user interfaces visualization workload distribution Computational geometry Computer interfaces Data mining Data visualization Grasping Mathematical model Multidimensional systems Rendering (computer graphics) Scientific computing User interfaces,
423,Automated brain tumor segmentation using spatial accuracy-weighted hidden Markov Random Field,Computerized Medical Imaging and Graphics,2009,"A variety of algorithms have been proposed for brain tumor segmentation from multi-channel sequences, however, most of them require isotropic or pseudo-isotropic resolution of the MR images. Although co-registration and interpolation of low-resolution sequences, such as T2-weighted images, onto the space of the high-resolution image, such as T1-weighted image, can be performed prior to the segmentation, the results are usually limited by partial volume effects due to interpolation of low-resolution images. To improve the quality of tumor segmentation in clinical applications where low-resolution sequences are commonly used together with high-resolution images, we propose the algorithm based on Spatial accuracy-weighted Hidden Markov random field and Expectation maximization (SHE) approach for both automated tumor and enhanced-tumor segmentation. SHE incorporates the spatial interpolation accuracy of low-resolution images into the optimization procedure of the Hidden Markov Random Field (HMRF) to segment tumor using multi-channel MR images with different resolutions, e.g., high-resolution T1-weighted and low-resolution T2-weighted images. In experiments, we evaluated this algorithm using a set of simulated multi-channel brain MR images with known ground-truth tissue segmentation and also applied it to a dataset of MR images obtained during clinical trials of brain tumor chemotherapy. The results show that more accurate tumor segmentation results can be obtained by comparing with conventional multi-channel segmentation algorithms.",MRI Segmentation Brain tumor,
424,The Creation of a global telemedical information society,International Journal of Medical Informatics,1998,"Healthcare is a major candidate for improvement in any vision of the kinds of ‘information highways’ and ‘information societies’ that are now being visualized. The medical information management market is one of the largest and fastest growing segments of the healthcare device industry. The expected revenue by the year 2000 is US$21 billion. Telemedicine currently accounts for only a small segment but is expanding rapidly. In the USA more than 60% of federal telemedicine projects were initiated in the last 2 years. The concept of telemedicine captures much of what is developing in terms of technology implementations, especially if it is combined with the growth of the Internet and World Wide Web (WWW). It is foreseen that the World Wide Web (WWW) will become the most important communication medium of any future information society. If the development of such a society is to be on a global scale it should not be allowed to develop in an ad hoc manner. For this reason, the Euromed Project has identified 20 building blocks resulting in 39 steps requiring multi-disciplinary collaborations. Since, the organization of information is therefore critical especially when concerning healthcare the Euromed Project has also introduced a new (global) standard called ‘Virtual Medical Worlds’ which provides the potential to organize existing medical information and provide the foundations for its integration into future forms of medical information systems. Virtual Medical Worlds, based on 3D reconstructed medical models, utilizes the WWW as a navigational medium to remotely access multi-media medical information systems. The visualization and manipulation of hyper-graphical 3D ‘body/organ’ templates and patient-specific 3D/4D/and VR models is an attempt to define an information infrastructure in an emerging WWW-based telemedical information society.",,
425,Connected components labeling for giga-cell multi-categorical rasters,Computers & Geosciences,2013," Labeling of connected components in an image or a raster of non-imagery data is a fundamental operation in fields of pattern recognition and machine intelligence. The bulk of effort devoted to designing efficient connected components labeling (CCL) algorithms concentrated on the domain of binary images where labeling is required for a computer to recognize objects. In contrast, in the Geographical Information Science (GIS) a CCL algorithm is mostly applied to multi-categorical rasters in order to either convert a raster to a shapefile, or for statistical characterization of individual clumps. Recently, it has become necessary to label connected components in very large, giga-cell size, multi-categorical rasters but performance of existing CCL algorithms lacks sufficient speed to accomplish such task. In this paper we present a modification to the popular two-scan CCL algorithm that enables labeling of giga-cell size, multi-categorical rasters. Our approach is to apply a divide-and-conquer technique coupled with parallel processing to a standard two-scan algorithm. For specificity, we have developed a variant of a standard CCL algorithm implemented as r.clump in GRASS GIS. We have established optimal values of data blocks (stemming from the divide-and-conquer technique) and optimal number of computational threads (stemming from parallel processing) for a new algorithm called r.clump3p. The performance of the new algorithm was tested on a series of rasters up to 160 Mcells in size; for largest size test raster a speed up over the original algorithm is 74 times. Finally, we have applied the new algorithm to the National Land Cover Dataset 2006 raster with 1.6 × 10 10 cells. Labeling this raster took 39 h using two-processors, 16 cores computer and resulted in 221,718,501 clumps. Estimated speed up over the original algorithm is 450 times. The r.clump3p works within the GRASS environment and is available in the public domain.",Connected components labeling Divide-and-conquer technique Parallel processing Land cover dataset,
426,VR in medicine: Virtual colonoscopy,Future Generation Computer Systems,1998,"Colon/rectal cancer is the second most common cause of death, yet among the most preventable when detected in its early stages. The traditional diagnostic procedures cause tremendous discomfort and are deeply invasive. The motivation for this work is firstly to develop an alternative technique to visualise the inner mucosal surface of the colonic wall. This technique will be based on three-dimensional (3D) visualisation and virtual reality to perform virtual endoscopy. However, there is a requirement of vast computational support. Therefore, secondly, this paper will discuss the possibilities adopting high performance computing and networking to support virtual reality medical applications.",Virtual reality WWW Telemedicine Virtual colonoscopy,
427,Visualization of 3D fields and medical data and using VRML,Future Generation Computer Systems,1998,"This paper describes the visualization of 3D medical images and 3D fields on inexpensive workstations or personal computers. VRML 2.0 and Java language are used through the Web interface for manipulating 3D models. 3D data are typically stored and processed on powerful servers accessible by using TCP/IP. Surface rendering of CT and MRI images is done by applying the marching cube algorithm, and then simplifying the obtained triangular meshes and transforming them into VRML format. Various virtual tools may be used for field visualization and object examination and manipulation. The developed concepts and software may also be implemented for visualization of spatial fields in aerodynamics, fluid dynamics simulation, thermodynamics, etc.",Medicine Medical imaging Field visualization Surface reconstruction Mesh simplification Mesh decimation Internet WWW Web VMRL Java,
428,Advances in design and implementation of optimization software,European Journal of Operational Research,2002,"Developing optimization software that is capable of solving large and complex real-life problems is a huge effort. It is based on a deep knowledge of four areas: theory of optimization algorithms, relevant results of computer science, principles of software engineering, and the state-of-the-art in computer technology. The paper highlights the diverse requirements of optimization software and discusses the ingredients needed to fulfil those requirements. In addition to reviewing the current hardware/software environment it hints at how recent hardware developments can influence future optimization software. After a survey of computationally successful techniques for continuous optimization, it also outlines the perspective offered by parallel computing, and stresses the importance of optimization modeling systems. Being a survey paper, it includes many references to both give due credit to results in the field of optimization software, and to help readers obtain more detailed information on issues of interest.",Large scale optimization Optimization software Implementation technology,
429,Combination of data replication and scheduling algorithm for improving data availability in Data Grids,Journal of Network and Computer Applications,2013,"Data Grid is a geographically distributed environment that deals with large-scale data-intensive applications. Effective scheduling in Grid can reduce the amount of data transferred among nodes by submitting a job to a node, where most of the requested data files are available. Data replication is another key optimization technique for reducing access latency and managing large data by storing data in a wisely manner. In this paper two algorithms are proposed, first a novel job scheduling algorithm called Combined Scheduling Strategy (CSS) that uses hierarchical scheduling to reduce the search time for an appropriate computing node. It considers the number of jobs waiting in queue, the location of required data for the job and the computing capacity of sites. Second a dynamic data replication strategy, called the Modified Dynamic Hierarchical Replication Algorithm (MDHRA) that improves file access time. This strategy is an enhanced version of Dynamic Hierarchical Replication (DHR) strategy. Data replication should be used wisely because the storage capacity of each Grid site is limited. Thus, it is important to design an effective strategy for the replication replacement. MDHRA replaces replicas based on the last time the replica was requested, number of access, and size of replica. It selects the best replica location from among the many replicas based on response time that can be determined by considering the data transfer time, the storage access latency, the replica requests that waiting in the storage queue and the distance between nodes. The simulation results demonstrate the proposed replication and scheduling strategies give better performance compared to the other algorithms.",Data Grid Data replication Job scheduling Simulation,
430,A diffraction tomography method for medical imaging implemented on high performance computing environment,,1999,"The efficient implementation of a diffraction tomography method for medical imaging is addressed within the framework of High Performance Computing (HPC) environment. A non-linear optimization method for the solution of the inverse scattering problem is implemented on a shared memory model computer. Linear speed-up and significant reduction in the total execution time is achieved when the program is executed in parallel, enabling the feasibility of the method for realistic medical imaging applications.",,
431,Web-based interactive 2D/3D medical image processing and visualization software,Computer Methods and Programs in Biomedicine,2010,"There are many medical image processing software tools available for research and diagnosis purposes. However, most of these tools are available only as local applications. This limits the accessibility of the software to a specific machine, and thus the data and processing power of that application are not available to other workstations. Further, there are operating system and processing power limitations which prevent such applications from running on every type of workstation. By developing web-based tools, it is possible for users to access the medical image processing functionalities wherever the internet is available. In this paper, we introduce a pure web-based, interactive, extendable, 2D and 3D medical image processing and visualization application that requires no client installation. Our software uses a four-layered design consisting of an algorithm layer, web-user-interface layer, server communication layer, and wrapper layer. To compete with extendibility of the current local medical image processing software, each layer is highly independent of other layers. A wide range of medical image preprocessing, registration, and segmentation methods are implemented using open source libraries. Desktop-like user interaction is provided by using AJAX technology in the web-user-interface. For the visualization functionality of the software, the VRML standard is used to provide 3D features over the web. Integration of these technologies has allowed implementation of our purely web-based software with high functionality without requiring powerful computational resources in the client side. The user-interface is designed such that the users can select appropriate parameters for practical research and clinical studies.",Web-based software tools 2D and 3D processing and visualization Medical imaging and analysis,
432,The Optical Chained-Cubic Tree interconnection network: Topological structure and properties,Computers & Electrical Engineering,2012,"Interconnection networks with optical communication links outperform others using electronic communication links when the distance is long in terms of speed and power consumption. However, for short distances, electronic network topologies are preferred due to lower material cost requirements. As a result, hybrid network topologies were constructed to combine the benefits of both types of network topologies, such as Optical Transpose Interconnection System (OTIS). This paper presents a new hybrid interconnection network topology, which is constructed using both optical and electronic links, called the Optical Chained-Cubic Tree (OCCT). This new OCCT topology is based on the Chained-Cubic Tree (CCT) interconnection network and is designed to cope with both types of binary trees; full and complete. Also, the topological properties of OCCT in terms of diameter, connectivity, degree, bisection width, and cost are presented and compared with OTIS-Mesh and CCT interconnection networks.",,
433,Characterizing workflow-based activity on a production e-infrastructure using provenance data,Future Generation Computer Systems,2013," Grid computing and workflow management systems emerged as solutions to the challenges arising from the processing and storage of shear volumes of data generated by modern simulations and data acquisition devices. Workflow management systems usually document the process of the workflow execution either as structured provenance information or as log files. Provenance is recognized as an important feature in workflow management systems, however there are still few reports on its usage in practical cases. In this paper we present the provenance system implemented in our platform, and then use the information captured by this system during 8 months of platform operation to analyze the platform usage and to perform multilevel error pattern analysis. We make use of the large amount of structured data using the explanatory potential of statistical approaches to find properties of workflows, jobs and resources that are related to workflow failure. Such an analysis enables us to characterize workflow executions on the infrastructure and understand workflow failures. The approach is generic and applicable to other e-infrastructures to gain insight into operational incidents.",Grid computing Scientific workflow management systems Provenance Workflow failure analysis,
434,Business-driven short-term management of a hybrid IT infrastructure,Journal of Parallel and Distributed Computing,2012,"We consider the problem of managing a hybrid computing infrastructure whose processing elements are comprised of in-house dedicated machines, virtual machines acquired on-demand from a cloud computing provider through short-term reservation contracts, and virtual machines made available by the remote peers of a best-effort peer-to-peer (P2P) grid. Each of these resources has different cost basis and associated quality of service guarantees. The applications that run in this hybrid infrastructure are characterized by a utility function: the utility gained with the completion of an application depends on the time taken to execute it. We take a business-driven approach to manage this infrastructure, aiming at maximizing the profit yielded, that is, the utility produced as a result of the applications that are run minus the cost of the computing resources that are used to run them. We propose a heuristic to be used by a contract planner agent that establishes the contracts with the cloud computing provider to balance the cost of running an application and the utility that is obtained with its execution, with the goal of producing a high overall profit. Our analytical results show that the simple heuristic proposed achieves very high relative efficiency in the use of the hybrid infrastructure. We also demonstrate that the ability to estimate the grid behaviour is an important condition for making contracts that allow such relative efficiency values to be achieved. On the other hand, our simulation results with realistic error predictions show only a modest improvement in the profit achieved by the simple heuristic proposed, when compared to a heuristic that does not consider the grid when planning contracts, but uses it, and another that is completely oblivious to the existence of the grid. This calls for the development of more accurate predictors for the availability of P2P grids, and more elaborated heuristics that can better deal with the several sources of non-determinism present in this hybrid infrastructure.",Cloud computing Grid computing Peer-to-peer Business-driven IT management Short-term management Capacity planning,
435,Fast multi-scale edge-detection in medical ultrasound signals,Signal Processing,2012,"In this article we suggest a fast multi-scale edge-detection scheme for medical ultrasound signals. The edge-detector is based on well-known properties of the continuous wavelet transform. To achieve both good localization of edges and detect only significant edges, we study the maxima-lines of the wavelet transform. One can obtain the maxima-lines between two scales by computing the wavelet transform at several intermediate scales. To reduce computational effort and time we suggest a time-scale filtering procedure which uses only few scales to connect modulus-maxima across time-scale plane. The design of this procedure is based on a study of maxima-lines corresponding to edges typical for medical ultrasound signals. This study allows us to construct an algorithm for medical ultrasound signals which meets the demand for speed, but not on expense of reliability. The edge-detection algorithm has been applied to a large class of medical ultrasound signals including tumour-, liver- and artery-images. Our results show that the proposed algorithm effectively detects major features in such signals, including edges with low contrast.",Medical ultrasound signal Wavelet transform Maxima-line Time-scale filtering Space-scale filtering Multi-scale edge-detector,
436,Direct volume manipulation for visualizing intraoperative liver resection process,Computer Methods and Programs in Biomedicine,2014," This paper introduces a new design and application for direct volume manipulation for visualizing the intraoperative liver resection process. So far, interactive volume deformation and resection have been independently handled due to the difficulty of representing elastic behavior of volumetric objects. Our framework models global shape editing and discontinuous local deformation by merging proxy geometry encoding and displacement mapping. A local-frame-based elastic model is presented to allow stable editing of the liver shape including bending and twisting while preserving the volume. Several tests using clinical CT data have confirmed the developed software and interface can represent the intraoperative state of liver and produce local views of reference vascular structures, which provides a “road map of vessels” that are key features when approaching occluded tumors during surgery.",Volume manipulation Shape editing Liver resection Computer aided surgery,
437,Recovery of respiratory motion and deformation of the liver using laparoscopic freehand 3D ultrasound system,Medical Image Analysis,2007,"The present paper describes a method for intraoperative recovery of respiratory motion and deformation of the liver by using a laparoscopic freehand 3D ultrasound (US) system. The proposed method can extend 3D US data of the liver to 4D by acquiring additional several sequences of time-varying 2D US images during a couple of respiration cycles. 2D US images are acquired on several sagittal image planes and their time-varying 3D positions and orientations are measured using a miniature magnetic 3D position sensor attached to a laparoscopic US (LUS) probe. During the acquisition, the LUS probe is assumed to move together with hepatic surface. Respiratory phases and in-plane 2D deformation fields are estimated from time-varying 2D US images, and then time-varying 3D deformation fields on sagittal image planes are obtained by combining 3D positions and orientations of the image planes. Time-varying 3D deformation field of the volume, that is, 4D deformation field, is obtained by interpolating the 3D deformation fields estimated on several planes. In vivo experiments using a pig liver showed that the proposed method could perform accurate estimation of respiratory cycle and in-plane 2D deformation fields. Furthermore, evaluation for the effects of sagittal plane interval indicated that 4D deformation fields could be stably recovered.",Respiratory motion compensation Laparoscopic surgery Non-rigid registration Intraoperative image analysis 4D deformation field,
438,A decompression pipeline for accelerating out-of-core volume rendering of time-varying data,Computers & Graphics,2008,"This paper presents a decompression pipeline capable of accelerating out-of-core volume rendering of time-varying scalar data. Our pipeline is based on a two-stage compression method that cooperatively uses the CPU and the graphics processing unit (GPU) to transfer compressed data entirely from the storage device to the video memory. This method combines two different compression algorithms, namely packed volume texture compression (PVTC) and Lempel–Ziv–Oberhumer (LZO) compression, allowing us to exploit both temporal and spatial coherence in time-varying data. Furthermore, it achieves fast decompression by taking architectural advantages of each processing unit: a hardware component on the GPU and a large cache on the CPU, each suited to decompress PVTC and LZO encoded data, respectively. We also integrate the method with a thread-based pipeline mechanism to increase the data throughput by overlapping data loading, data decompression, and rendering stages. Our pipelined renderer runs on a quad-core PC and achieves a video rate of 41 frames per second (fps) in average for 258 × 258 × 208 voxel data with 150 time steps. It also demonstrates an almost interactive rate of 8 fps for 512 × 512 × 295 voxel data with 411 time steps.",Volume rendering Time-varying data Pipelined rendering Data compression GPU,
439,Fast ell _1 -SPIRiT Compressed Sensing Parallel Imaging MRI: Scalable Parallel Implementation and Clinically Feasible Runtime,"Medical Imaging, IEEE Transactions on",2012,"We present l1 -SPIRiT, a simple algorithm for auto calibrating parallel imaging (acPI) and compressed sensing (CS) that permits an efficient implementation with clinically-feasible runtimes. We propose a CS objective function that minimizes cross-channel joint sparsity in the wavelet domain. Our reconstruction minimizes this objective via iterative soft-thresholding, and integrates naturally with iterative self-consistent parallel imaging (SPIRiT). Like many iterative magnetic resonance imaging reconstructions, l1-SPIRiT's image quality comes at a high computational cost. Excessively long runtimes are a barrier to the clinical use of any reconstruction approach, and thus we discuss our approach to efficiently parallelizing l1 -SPIRiT and to achieving clinically-feasible runtimes. We present parallelizations of l1 -SPIRiT for both multi-GPU systems and multi-core CPUs, and discuss the software optimization and parallelization decisions made in our implementation. The performance of these alternatives depends on the processor architecture, the size of the image matrix, and the number of parallel imaging channels. Fundamentally, achieving fast runtime requires the correct trade-off between cache usage and parallelization overheads. We demonstrate image quality via a case from our clinical experimentation, using a custom 3DFT spoiled gradient echo (SPGR) sequence with up to 8Ã acceleration via Poisson-disc undersampling in the two phase-encoded directions.",biomedical MRI calibration compressed sensing image reconstruction image sequences iterative methods medical image processing optimisation wavelet transforms 3DFT spoiled gradient echo sequence MRI Poisson-disc undersampling autocalibrating parallel imaging compressed sensing cross-channel joint sparsity imaging reconstructions iterative soft-thresholding l1 -SPIRiT multiGPU systems multicore CPU parallelization self-consistent parallel imaging software optimization Acceleration Calibration Coils Compressed sensing Image reconstruction Imaging Runtime Autocalibrating parallel imaging compressed sensing general-purpose computing on graphics processing unit (GPGPU) parallel computing self-consistent parallel imaging (SPIRiT) Algorithms Data Compression Feasibility Studies Humans Image Enhancement Image Interpretation Computer-Assisted Magnetic Resonance Imaging Pattern Recognition Automated Reproducibility of Results Sensitivity and Specificity Wavelet Analysis,
440,JHelioviewer: Visualizing Large Sets of Solar Images Using JPEG 2000,Computing in Science Engineering,2009,"All disciplines that work with image data-from astrophysics to medical research and historic preservation-increasingly require efficient ways to browse and inspect large sets of high-resolution images. Based on the JPEG 2000 image-compression standard, the JHelioviewer solar image visualization tool lets users browse petabyte-scale image archives as well as locate and manipulate specific data sets.",astronomy computing data compression data visualisation image coding image resolution JPEG 2000 image-compression astrophysics high-resolution image historic preservation solar image visualization tool Data visualization Image coding Image resolution Image storage Motion pictures Observatories Sun Transform coding data visualization,
441,Automatic location of vertebrae in digitized videofluoroscopic images of the lumbar spine,Medical Engineering & Physics,1997,"Back pain is a widespread problem, and the disability it engenders continues to grow, despite efforts to contain it. A major problem in the diagnosis and management of back pain is the assessment of the degree to which mechanical factors play a part. Of considerable importance in understanding these mechanical factors is being able to quantify how the human spine actually moves in vivo. Digitized videofluoroscopy is currently the only practical method available for studying spinal motion in vivo at the segmental level. Low-dose, planar motion X-rays of the spine are captured on videotape and subsequently digitized for analysis. Until now, vertebrae in the digitized images were identified and marked manually as a basis for calculating intervertebral kinematics. This paper describes a procedure for automatically identifying the vertebrae in the motion sequences. The process increases objectivity and repeatability, and significantly reduces the manual effort required in locating the vertebrae prior to calculating the kinematics. The technique has been applied to images of a calibrated model and the results are promising. In-plane rotations may be calculated to an accuracy of at least 1°. Repeated analysis reveals standard deviations of less than 0.5° for intervertebral rotations and less than 0.25 mm for translations.",Back pain image processing videofluoroscopy lumbar spine cross-correlation polar coordinates intervertebral kinematics,
442,Real-time deformable registration of multi-modal whole slides for digital pathology,Computerized Medical Imaging and Graphics,2011,"Digital pathology provides new ways to visualize tissue slides and enables new workflows for analyzing these slides. Analogous to radiology, adjacent tissue sections prepared with different stains or biomarkers (e.g. H&amp;E, IHC, special stains, or ISH; chromogenic or fluorescent) may be seen as different modalities, each representing different structural and/or functional information. Today, the anatomic pathologist views multiple glass slides using an optical microscope and then combines the information in their head to reach a (diagnostic) opinion. Moreover, due to the nature of the slide preparation and digitization process, the tissue and its features do not have the exact same morphology, appearance, or spatial alignment, making it difficult to find the same region on adjacent slides. To address such concerns, this paper presents a method for the spatial alignment of multi-modal whole slide digital microscopy images. To remain practical, the described method employs a two-step registration strategy designed to reduce computation time: the first step computes a B-spline deformable transform on low-resolution images prior to visualization, the second step applies the precomputed transformation only to the high-resolution region currently being viewed. The proposed method is demonstrated using a number of cases comprising H&amp;E and IHC stained slides. These results indicate the feasibility of deformable registration for spatial alignment of multi-modal whole slide digital microscopy images within practical time constraints.",Digital pathology Deformable registration Whole slide imaging Immunohistochemistry Mutual information B-spline,
443,Processing moldable tasks on the grid: Late job binding with lightweight user-level overlay,Future Generation Computer Systems,2011,"Independent observations and everyday user experience indicate that performance and reliability of large grid infrastructures may suffer from large and unpredictable variations. In this paper we study the impact of the job queuing time on processing of moldable tasks which are commonly found in large-scale production grids. We use the mean value and variance of makespan as the quality of service indicators. We develop a general task processing model to provide a quantitative comparison between two models: early and late job binding in a user-level overlay applied to the EGEE Grid infrastructure. We find that the late-binding model effectively defines a transformation of the distribution of makespan according to the Central Limit Theorem. As demonstrated by Monte Carlo simulations using real job traces, this transformation allows to substantially reduce the mean value and variance of makespan. For certain classes of applications task granularity may be adjusted such that a speedup of an order of magnitude or more may be achieved. We use this result to propose a general strategy for managing access to resources and optimization of workload based on Ganga and DIANE user-level overlay tools. Key features of this approach include: a late-binding scheduler, an ability to interface to a wide range of distributed systems, an ability to extend and customize the system to cover application-specific scheduling and processing patterns and finally, ease of use and lightweight deployment in the user space. We discuss the impact of this approach for some practical applications where efficient processing of many tasks is required to solve scientific problems.",Grid computing Quality of service Moldable task processing Late binding Agent scheduling Pilot jobs Applications Interoperability,
444,Learning in the combinatorial neural model,"Neural Networks, IEEE Transactions on",1998,"The combinatorial neural model (CNM) is a type of fuzzy neural network for classification problems. Learning in CNM is a complex task spanning the learning of input-neuron membership functions, the network topology and connection weights. We deal with these various aspects of learning in CNM, most notably with the learning of connection weights, whose complexity comes from the existence of nondifferentiable, nonconvex error functions associated with the learning process. We introduce several algorithms for weight learning. All the algorithms are based on âlocalâ rules, and are therefore amenable to distributed/parallel implementations. Experimental results are provided on the large-scale problem of monitoring the deforestation of the Amazon region on satellite images. These results show that a hybrid CNM system outperforms previous results obtained with variations of error backpropagation techniques. In addition, this hybrid system has demonstrated robustness in the context under consideration",computational complexity forestry fuzzy neural nets fuzzy set theory image recognition learning systems network topology remote sensing Amazon region combinatorial neural model connection weights deforestation fuzzy neural network image analysis learning systems membership functions network topology pattern classification satellite images Computer networks Fuzzy neural networks Intelligent networks Large-scale systems Monitoring Multidimensional systems Network topology Neural networks Satellites Uncertainty,
445,Implementation and evaluation of the Level Set method: Towards efficient and accurate simulation of wet etching for microengineering applications,Computer Physics Communications,2013," The use of atomistic methods, such as the Continuous Cellular Automaton (CCA), is currently regarded as a computationally efficient and experimentally accurate approach for the simulation of anisotropic etching of various substrates in the manufacture of Micro-electro-mechanical Systems (MEMS). However, when the features of the chemical process are modified, a time-consuming calibration process needs to be used to transform the new macroscopic etch rates into a corresponding set of atomistic rates. Furthermore, changing the substrate requires a labor-intensive effort to reclassify most atomistic neighborhoods. In this context, the Level Set (LS) method provides an alternative approach where the macroscopic forces affecting the front evolution are directly applied at the discrete level, thus avoiding the need for reclassification and/or calibration. Correspondingly, we present a fully-operational Sparse Field Method (SFM) implementation of the LS approach, discussing in detail the algorithm and providing a thorough characterization of the computational cost and simulation accuracy, including a comparison to the performance by the most recent CCA model. We conclude that the SFM implementation achieves similar accuracy as the CCA method with less fluctuations in the etch front and requiring roughly 4 times less memory. Although SFM can be up to 2 times slower than CCA for the simulation of anisotropic etchants, it can also be up to 10 times faster than CCA for isotropic etchants. In addition, we present a parallel, GPU-based implementation (gSFM) and compare it to an optimized, multicore CPU version (cSFM), demonstrating that the SFM algorithm can be successfully parallelized and the simulation times consequently reduced, while keeping the accuracy of the simulations. Although modern multicore CPUs provide an acceptable option, the massively parallel architecture of modern GPUs is more suitable, as reflected by computational times for gSFM up to 7.4 times faster than for cSFM.",Level Set method Sparse Field Method Anisotropic wet chemical etching Microengineering Cellular Automata MEMS Parallel computing GPU,
446,A classification of file placement and replication methods on grids,Future Generation Computer Systems,2013,"This paper presents a classification of file placement and replication methods on grids. The study is motivated by file transfer issues encountered in the Virtual Imaging Platform deployed on the European Grid Infrastructure. Approaches proposed in the last 6 years are classified using taxonomies of replication process, replication optimization, file models, resource models and replication validation. Most existing approaches implement file replication as a middleware service, using dynamic strategies. Production approaches are slightly different than works evaluated in simulation or in controlled conditions which (i) mostly assumes simplistic file models (undistinguished read-only files), (ii) rely on elaborated access patterns, (iii) assume clairvoyance of the infrastructure parameters and (iv) study file availability less than other metrics but insist on cost.",File placement Replication Taxonomy Classification Grid,
447,Optoelectronic interconnection technology in the HOLMS system,"Selected Topics in Quantum Electronics, IEEE Journal of",2003,"The High-Speed Optoelectronic Memory Systems (HOLMS) project, sponsored by the European Union Information Society Technology program, aims to make the use of board level optical interconnection in information systems practical and economical by developing optoelectronic packaging technology compatible with standard electronic assembly processes. To demonstrate the potential of the technology, we develop a demonstrator system that addresses the most pressing problem of contemporary computer architecture, memory latency. This paper describes the key ideas and some preliminary results of the HOLMS projects focusing on electronic interconnection technology, in particular optoelectronic packaging issues.",integrated circuit interconnections integrated circuit packaging integrated optoelectronics memory architecture multichip modules optical computing optical interconnections printed circuit accessories HOLMS system High-Speed Optoelectronic Memory Systems project board level optical interconnection computer architecture demonstrator system electronic assembly process compatibility memory architecture memory latency optical computing optoelectronic interconnection technology optoelectronic multichip modules optoelectronic packaging technology Assembly systems Computer architecture Electronics packaging High speed optical techniques Integrated circuit interconnections Optical computing Optical interconnections Optimized production technology Paper technology Very large scale integration,
448,Tool environments in CORBA-based medical high-performance computing,Future Generation Computer Systems,2002,High-performance computing in medical science has led to important progress in the field of computer tomography. A fast calculation of various types of images is a precondition for statistical comparison of big sets of input data. With our current research we adapted parallel programs from PVM to CORBA. CORBA makes the integration into clinical environments much easier. In order to improve the efficiency and maintainability we added load balancing and graphical on-line tools to our CORBA-based application program.,Computer tomography Parallel computing Load balancing,
449,Parallel algorithms for finding polynomial roots on OTIS-torus,Journal of Supercomputing,2010,"We present two parallel algorithms for finding all the roots of an N-degree polynomial equation on an efficient model of Optoelectronic Transpose Interconnection System (OTIS), called OTIS-2D torus. The parallel algorithms are based on the iterative schemes of Durand-Kerner and Ehrlich methods. We show that the algorithm for the Durand-Kerner method requires (N0.75 + 0.5N0.25 - 1) electronic moves + 2(N0.5 -1) OTIS moves using N processors. The parallel algorithm for Ehrlich method is shown to run in (N0.75 + 0.5N0.25 - 1) electronic moves + 2(N 0.5 - 1) OTIS moves with the same number of processors. The algorithms have lower AT cost than the algorithms presented in Jana (Parallel Comput 32:301-312, 2006). The scalability of the algorithms is also discussed. 2009 Springer Science+Business Media, LLC.",Iterative methods Parallel algorithms Parallel architectures Parallel processing systems Polynomials,
450,Parallel algorithms for finding polynomial Roots on OTIS-torus,Journal of Supercomputing,2010,"We present two parallel algorithms for finding all the roots of an N-degree polynomial equation on an efficient model of Optoelectronic Transpose Interconnection System (OTIS), called OTIS-2D torus. The parallel algorithms are based on the iterative schemes of Durand-Kerner and Ehrlich methods. We show that the algorithm for the Durand-Kerner method requires (N0.75+0.5N0.25-1) electronic moves + 2(N0.5-1) OTIS moves using N processors. The parallel algorithm for Ehrlich method is shown to run in (N0.75+0.5N0.25-1) electronic moves + 2(N0.5-1) OTIS moves with the same number of processors. The algorithms have lower AT cost than the algorithms presented in Jana (Parallel Comput 32:301-312, 2006). The scalability of the algorithms is also discussed.",iterative methods parallel algorithms polynomials,
451,A non-rigid cardiac image registration method based on an optical flow model,Optik - International Journal for Light and Electron Optics,2013,"According to non-rigid medical image registration, new method of classification registration is proposed. First, Feature points are extracted based on SIFT (Scale Invariant Feature Transform) from reference images and floating images to match feature points. And the coarse registration is performed using the least square method. Then the precise registration is achieved using the optical flow model algorithm. SIFT algorithm is based on local image features that are with good scale, rotation and illumination invariance. Optical flow algorithm does not  features and use the image gray information directly, and its registration speed is faster. The both algorithms are complementary. SIFT algorithm is used for improving the convergence speed of optical flow algorithm, and optical flow algorithm makes the registration result more accurate. The experimental results prove that the algorithm can improve the accuracy of the non-rigid medical image registration and enhance the convergence speed. Therefore, the algorithm has some advantages in the image registration.",Non-rigid registration Medical image SIFT algorithm Optical flow model,
452,A non-rigid medical image registration method based on improved linear elastic model,Optik - International Journal for Light and Electron Optics,2012,"Non-rigid medical image registration is an important research project of medical image processing; it is the basis of medical image fusion. Relative to rigid image, the deformation of non-rigid image is more serious and more complicated. According to the characteristics of non-rigid medical image deformation, this paper proposes an adaptive non-rigid medical image registration algorithm. Firstly, it is based on global registration; secondly, it is about extracting feature points of global registration image and the reference image, and then generating irregular triangle grid according to extracted feature points. Finally, local accurate image registration is achieved using the minimum potential energy as a similar measure. Experimental results show that relative to the traditional non-rigid registration algorithm, this algorithm not only ensures the registration accuracy but also enhances the robustness and anti-noise of registration algorithm.",Non-rigid registration Medical image Feature points Adaptive triangle grid Total potentials,
453,Acceleration Method of 3D Medical Images Registration Based on Compute Unified Device Architecture,Bio-Medical Materials and Engineering,2014,"Compute Unified Device Architecture (CUDA) is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increase in computing performance via the power of the graphics processing unit (GPU). In medical image analysis, 3D image registration generally takes relatively long time, which is not feasible for clinical applications. To solve this problem, this paper proposed a high performance computational method based on CUDA, which took full advantage of GPU parallel computing under CUDA architecture combined with image multiple scale and maximum mutual information. Experiments showed that this algorithm can not only maintain the registration accuracy but also greatly increase the speed of registration process and meet the real-time requirement of clinical application.",,
454,Joint CT/CBCT deformable registration and CBCT enhancement for cancer radiotherapy,Medical Image Analysis,2013,"This paper details an algorithm to simultaneously perform registration of computed tomography (CT) and cone-beam computed (CBCT) images, and image enhancement of CBCT. The algorithm employs a viscous fluid model which naturally incorporates two components: a similarity measure for registration and an intensity correction term for image enhancement. Incorporating an intensity correction term improves the registration results. Furthermore, applying the image enhancement term to CBCT imagery leads to an intensity corrected CBCT with better image quality. To achieve minimal processing time, the algorithm is implemented on a graphic processing unit (GPU) platform. The advantage of the simultaneous optimization strategy is quantitatively validated and discussed using a synthetic example. The effectiveness of the proposed algorithm is then illustrated using six patient datasets, three head-and-neck datasets and three prostate datasets.",Deformable image registration Multimodal registration Mutual information Shading correction Scatter removal,
455,Computational fluid dynamics,"Engineering in Medicine and Biology Magazine, IEEE",2009,"In this article, we have described a computational framework for multiscale simulation of gas flow in subject-specific airway models of the human lung. The framework consists of five major components: accurate extraction of airway geometry from MDCT image data sets, geometrical modeling of airway trees, novel 3-D and 1-D coupled mesh generation, 3-D high-fidelity CFD techniques for turbulent and transitional flow, and CT-derived subject-specific physiological boundary conditions. This work demonstrates the importance of multi-scale simulation of pulmonary gas flow for accurate prediction of flow characteristics at large and small airways and their interactions. The multiscale simulation presented here can be further applied to other healthy and diseased human subjects for intra- and intersubject analyses to better understand the lung flow-structure relationship, the progression of lung diseases, and the correlation between inhaled pharmaceutical drug aerosols or air pollutants with airway structure.",Weibull distribution computational fluid dynamics computerised tomography diagnostic radiography diseases drugs image resolution image segmentation lung medical image processing mesh generation paediatrics physiological models pneumodynamics Weibel airway model X-ray computed tomography age 10 yr to 18 yr airborne pollutant vulnerability airway geometry breathing lung children lung development chronic environmental pollutant computational fluid dynamics computational power fixed mesh generation image segmentation imaging resolution pathologic development perfusion pulmonary air flow subject-specific model ventilation imaging xenon gas Boundary conditions Computational fluid dynamics Computational modeling Data mining Fluid flow Geometry Humans Lungs Mesh generation Solid modeling Biomedical Engineering Computer Simulation Humans Hydrodynamics Imaging Three-Dimensional Lung Models Anatomic Models Biological Pulmonary Alveoli Respiratory Mechanics Respiratory Physiological Phenomena Tomography X-Ray Computed,
456,Parallel Delaunay triangulation in three dimensions,Computer Methods in Applied Mechanics and Engineering,2012,"A generic parallel Delaunay triangulation scheme by means of zonal partition of points is proposed. For efficient Delaunay triangulation, points are first sorted into cells, each of which is allocated roughly equal number of points. The cells are naturally grouped into zones, in which Delaunay triangulation is constructed by simultaneous point insertion cell by cell within each zone. Tetrahedra at the boundary between zones are created in parallel by adding layers of cells at the boundary of each zone to ensure that circumspheres of boundary tetrahedra contain no points in their interior. Redundant tetrahedra at the boundary between zones can be easily eliminated by individual processors in a completely independent manner by means of the elegant minimum vertex allocation scheme, such that a simplex with k vertices from zones (z1, z2, … , zk) is allocated to zone z = min(z1, z2, … , zk). The parallel 3D Delaunay triangulation algorithm has been coded in Intel FORTRAN VS2010. The parallel zonal insertion on a PC can boost the speed of the single-processor insertion by 4.5 times for the insertion of 50 million randomly generated spatial points in 133 s. The scalability of the parallel zonal insertion algorithm has also been tested on a proper multi-core machine with 12 processors running on OpenMP parallel directives with shared memory. Provided the number of zones is an integral multiple of the number of processors used, almost 100% scalability at 90% efficiency was observed for parallel insertion using 2, 4, 6, 8 and 12 processors, and a 10.8 time speed up was recorded in a parallel insertion of 20 million points in 2 × 2 × 3 = 12 zones by 12 processors.",Parallel Delaunay triangulation Partition into cells and zones Point insertion Three dimensions,
457,A fast segmentation method based on constraint optimization and its applications: Intensity inhomogeneity and texture segmentation,Pattern Recognition,2011,"We propose a new constraint optimization energy and an iteration scheme for image segmentation which is connected to edge-weighted centroidal Voronoi tessellation (EWCVT). We show that the characteristic functions of the edge-weighted Voronoi regions are the minimizers (may not unique) of the proposed energy at each iteration. We propose a narrow banding algorithm to accelerate the implementation, which makes the proposed method very fast. We generalize the CVT segmentation to hand intensity inhomogeneous and texture segmentation by incorporating the global and local image information into the energy functional. Compared with other approaches such as level set method, the experimental results in this paper have shown that our approach greatly improves the calculation efficiency without losing segmentation accuracy.",Image segmentation Constraint optimization Centroidal Voronoi tessellation Texture segmentation Intensity inhomogeneous Fast algorithm,
458,An optical flow approach to tracking colonoscopy video,Computerized Medical Imaging and Graphics,2013,"We can supplement the clinical value of an optical colonoscopy procedure if we can continuously co-align corresponding virtual colonoscopy (from preoperative X-ray CT exam) and optical colonoscopy images. In this work, we demonstrate a computer vision algorithm based on optical flow to compute egomotion from live colonoscopy video, which is then used to navigate and visualize the corresponding patient anatomy from X-ray CT data. The key feature of the algorithm lies in the effective combination of sparse and dense optical flow fields to compute the focus of expansion (FOE); FOE permits independent computation of camera translational and rotational parameters, directly contributing to the algorithm's accuracy and robustness. We performed extensive evaluation via a colon phantom and clinical colonoscopy data. We constructed two colon like phantoms, a straight phantom and a curved phantom to measure actual colonoscopy motion; tracking accuracy was quantitatively evaluated by comparing estimated motion parameters (velocity and displacement) to ground truth. Thirty straight and curved phantom sequences were collected at 10, 15 and 20 mm/s (5 trials at each speed), to simulate typical velocities during colonoscopy procedures. The average error in velocity estimation was within 3 mm/s in both straight and curved phantoms. Displacement error was under 7 mm over a total distance of 287–288 mm in the straight and curved phantoms. Algorithm robustness was successfully demonstrated on 27 optical colonoscopy image sequences from 20 different patients, and spanning 5 different colon segments. Specific sequences among these were chosen to illustrate the algorithm's decreased sensitivity to (1) recording interruptions, (2) errors in colon segmentation, (3) illumination artifacts, (4) presence of fluid, and (5) changes in colon structure, such as deformation, polyp removal, and surgical tool movement during a procedure.",Colonoscopy Tracking Optical flow Egomotion,
459,Scheduling jobs on computational grids using a fuzzy particle swarm optimization algorithm,Future Generation Computer Systems,2010,Grid computing is a computational framework used to meet growing computational demands. This paper introduces a novel approach based on Particle Swarm Optimization (PSO) for scheduling jobs on computational grids. The representations of the position and velocity of the particles in conventional PSO is extended from the real vectors to fuzzy matrices. The proposed approach is to dynamically generate an optimal schedule so as to complete the tasks within a minimum period of time as well as utilizing the resources in an efficient way. We evaluate the performance of the proposed PSO algorithm with a Genetic Algorithm (GA) and Simulated Annealing (SA) approach. Empirical results illustrate that an important advantage of the PSO algorithm is its speed of convergence and the ability to obtain faster and feasible schedules.,Swarm intelligence Grid computing Particle swarm optimization Fuzzy system,
460,Improved C-arm cardiac cone beam CT based on alternate reconstruction and segmentation,Biomedical Signal Processing and Control,2014," Sparsity regularized iterative reconstruction is an important and promising method for ECG-gated tomographic reconstruction of coronary artery during intervention treatment of cardiovascular diseases. As the reconstruction suffers from the problems of background overlay and data truncation, the background of angiogram should be well suppressed to obtain high reconstruction quality. Considering the deficiency of the commonly applied background suppression methods, this work proposes a strategy of alternate reconstruction and segmentation. During reconstruction, while the image intensity is iteratively updated, a contour is also evolved to segment the reconstructed vascular tree based on level set segmentation method. When the structure of the vascular tree is completely detected, the segmented vascular tree is re-projected to generate projection mask which is used to further reduce the projection background. Several experiments were performed to quantitatively evaluate the proposed method and the method is also compared with a state-of-the-art method. Experimental results show that the proposed strategy could effectively improve the reconstruction quality.",C-arm cardiac CBCT ECG-gated reconstruction Background suppression Regularized iterative reconstruction Top-hat,
461,A simple method of rapid and automatic color image segmentation for serialized Visible Human slices,Computers & Electrical Engineering,2014," In this paper, a rapid and automatic color image segmentation method for the serialized slices of the Visible Human is proposed. The main strategy is based on region growing and pixel color difference. A rapid color similarity computing method is improved and applied for classifying different pixels. An algorithm based on corrosion from four directions is proposed to automatically  the seed points for the serialized slices. Utilizing this method, the color slice images of the Visible Human body can be segmented in series automatically. Also, the multithreading frame of parallel computing is introduced in the entire segmentation process. This method is simple but rapid and automatic. The primary organs of the Visible Human can be segmented clearly and accurately. The 3D models of these organs after 3D reconstruction are satisfactory. This novel method can provide support to the Visible Human research.",,
462,Parallel centerline extraction on the GPU,Computers & Graphics,2014," Centerline extraction is important in a variety of visualization applications including shape analysis, geometry processing, and virtual endoscopy. Centerlines allow accurate measurements of length along winding tubular structures, assist automatic virtual navigation, and provide a path-planning system to control the movement and orientation of a virtual camera. However, efficiently computing centerlines with the desired accuracy has been a major challenge. Existing centerline methods are either not fast enough or not accurate enough for interactive application to complex 3D shapes. Some methods based on distance mapping are accurate, but these are sequential algorithms which have limited performance when running on the CPU. To our knowledge, there is no accurate parallel centerline algorithm that can take advantage of modern many-core parallel computing resources, such as GPUs, to perform automatic centerline extraction from large data volumes at interactive speed and with high accuracy. In this paper, we present a new parallel centerline extraction algorithm suitable for implementation on a GPU to produce highly accurate, 26-connected, one-voxel-thick centerlines at interactive speed. The resulting centerlines are as accurate as those produced by a state-of-the-art sequential CPU method [40], while being computed hundreds of times faster. Applications to fly through path planning and virtual endoscopy are discussed. Experimental results demonstrating centeredness, robustness and efficiency are presented.",Centerline Parallel algorithm GPU techniques Virtual endoscopy,
463,Fluid mechanics based classification of the respiratory efficiency of several nasal cavities,Computers in Biology and Medicine,2013," The flow in the human nasal cavity is of great importance to understand rhinologic pathologies like impaired respiration or heating capabilities, a diminished sense of taste and smell, and the presence of dry mucous membranes. To numerically analyze this flow problem a highly efficient and scalable Thermal Lattice-BGK (TLBGK) solver is used, which is very well suited for flows in intricate geometries. The generation of the computational mesh is completely automatic and highly parallelized such that it can be executed efficiently on High Performance Computers (HPCs). An evaluation of the functionality of nasal cavities is based on an analysis of pressure drop, secondary flow structures, wall-shear stress distributions, and temperature variations from the nostrils to the pharynx. The results of the flow fields of three completely different nasal cavities allow their classification into ability groups and support the a priori decision process on surgical interventions.",Lattice Boltzmann Thermal Lattice Boltzmann Respiration capability Heating capability Nasal cavity flows,
464,"On mixed reality environments for minimally invasive therapy guidance: Systems architecture, successes and challenges in their implementation from laboratory to clinic",Computerized Medical Imaging and Graphics,2013,"Mixed reality environments for medical applications have been explored and developed over the past three decades in an effort to enhance the clinician's view of anatomy and facilitate the performance of minimally invasive procedures. These environments must faithfully represent the real surgical field and require seamless integration of pre- and intra-operative imaging, surgical instrument tracking, and display technology into a common framework centered around and registered to the patient. However, in spite of their reported benefits, few mixed reality environments have been successfully translated into clinical use. Several challenges that contribute to the difficulty in integrating such environments into clinical practice are presented here and discussed in terms of both technical and clinical limitations. This article should raise awareness among both developers and end-users toward facilitating a greater application of such environments in the surgical practice of the future.",Minimally invasive surgery and therapy Image-guided interventions Virtual Augmented and mixed reality environments Clinical translation,
465,Global information infrastructure,International Journal of Bio-Medical Computing,1994,"The High Performance Computing and Communications Program (HPCC) is a multiagency federal initiative under the leadership of the White House Office of Science and Technology Policy, established by the High Performance Computing Act of 1991. It has been assigned a critical role in supporting the international collaboration essential to science and to health care. Goals of the HPCC are to extend USA leadership in high performance computing and networking technologies; to improve technology transfer for economic competitiveness, education, and national security; and to provide a key part of the foundation for the National Information Infrastructure. The first component of the National Institutes of Health to participate in the HPCC, the National Library of Medicine (NLM), recently issued a solicitation for proposals to address a range of issues, from privacy to ‘testbed’ networks, ‘virtual reality,’ and more. These efforts will build upon the NLM's extensive outreach program and other initiatives, including the Unified Medical Language System (UMLS), MEDLARS, and Grateful Med. New Internet search tools are emerging, such as Gopher and ‘Knowbots’. Medicine will succeed in developing future intelligent agents to assist in utilizing computer networks. Our ability to serve patients is so often restricted by lack of information and knowledge at the time and place of medical decision-making. The new technologies, properly employed, will also greatly enhance our ability to serve the patient.",High Performance Computing and Communications Program (HPCC) National information infrastructure National Library of Medicine (NLM) Outreach projects Federal initiatives National Institutes of Health (NIH),
466,Implementation and evaluation of a multifunctional telemedicine system in NTUH,International Journal of Medical Informatics,2001,"In this article, we proposed a multifunctional telemedicine system supporting both telediagnosis and teleconsultation services. We attempted not only to insure that the implementation of this system satisfied most requirements, but also to evaluate the impact of the system. With regard to system architecture, we designed a unified multimedia database to store all types of data and used two kinds of network (ATM and ISDN) for different possible applications. As for data transmission, the REFRESH and PREFETCH mechanisms were implemented to enhance data transfer efficiency. A total of 1107 consultations employing the telemedicine system were performed during the past 3 years. This technology was used most frequently for radiology consultation (32.7%, n=362) and ultrasonic examination (19.5%, n=216). An evaluation of the impact on diagnosis (507 valid cases) indicated that the diagnosis in 80 cases (15.78%) were altered on the basis of second opinions from a medical center; and the number of patients transferred to the medical center was reduced from 24 (4.7%) to eight cases. Most of the rural-site physicians (97%) thought that they did benefit from specialists' experience and knowledge via the telemedicine system. Based on 431 valid questionnaires, the number of the patients with confidence in the telemedicine system at their local healthcare center increased from 72.6% to 87.5%. Overall, more than 90% of patients and physicians believed that the system was valuable and provided satisfactory services.",Telemedicine ATM ISDN,
467,Visualizing 3D atmospheric data with spherical volume texture on virtual globes,Computers & Geosciences,2014," Volumetric ray-casting is a widely used algorithm in volume visualization, but implementing this algorithm to render atmospheric volume data that cover a large area on virtual globes constitutes a challenging problem. Because atmospheric data are usually georeferenced to a spherical coordinate system described by longitude, latitude and altitude, adaptations to the conventional volumetric ray-casting method are needed to accommodate spherical volume texture sampling. In this paper, we present a volumetric ray-casting framework to visualize atmospheric data that cover a broad but thin geographic area (because of the thinness of Earth׳s atmosphere). Volume texture conforming to the spherical coordinate system of a virtual globe can be created directly from the spherical volume data to avoid oversampling, undersampling or a loss of accuracy due to reprojecting and resampling such data into a Cartesian coordinate system. Considering the insignificant physical thickness of the atmosphere of the Earth, the ray-casting method presented in this paper also allows for real-time vertical scaling (exaggeration of the altitudinal range) without the need to re-process the volume texture, enabling convenient visual observation of the altitudinal variations. The spherical volume ray-casting method is implemented in a deferred rendering framework to integrate the volume effects into a virtual globe composed of a variety of background geospatial data objects, such as terrain, imagery, vector shapes and 3D geometric models.",Volumetric ray-casting Virtual globe Atmospheric volume data Spherical volume texture,
468,Augmenting interventional ultrasound using statistical shape model for guiding percutaneous nephrolithotomy: Initial evaluation in pigs,Neurocomputing,2014," Successful percutaneous nephrolithotomy (PCNL) highly depends on an accurate needle puncture of the kidney. This puncture step is challenging and must be performed under intraoperative image guidance. This paper presents an image guidance method for PCNL by augmenting interventional ultrasound (US) using a 3D statistical kidney model. First, a 3D statistical kidney model is built aforehand from a collection of aligned training shapes. Intraoperatively, a patient-specific 3D kidney model is reconstructed in a fine-tune way using sparse manually picked points from calibrated US at maximum exhalation. The US images are then augmented with the kidney model and a real-time tracked virtual needle. Under the augmented-US guidance, percutaneous renal puncture can be performed. Experiments results based on Wuzhishan pig data have validated the accuracy and inter-operator repeatability of the presented kidney model reconstruction. The feasibility and efficiency of the proposed augmented US-guided puncture for PCNL have been demonstrated in pigs. With careful setup by trained surgeons, our proposed guidance has the potential to reduce the occurrence of complications such as PCS access failure and bleeding compared with a traditional US-guided method.",Ultrasound Statistical shape model Percutaneous nephrolithotomy Pig Kidney,
469,Boosting weighted ELM for imbalanced learning,Neurocomputing,2014," Extreme learning machine (ELM) for single-hidden-layer feedforward neural networks (SLFN) is a powerful machine learning technique, and has been attracting attentions for its fast learning speed and good generalization performance. Recently, a weighted ELM is proposed to deal with data with imbalanced class distribution. The key essence of weighted ELM is that each training sample is assigned with an extra weight. Although some empirical weighting schemes were provided, how to determine better sample weights remains an open problem. In this paper, we proposed a Boosting weighted ELM, which embedded weighted ELM seamlessly into a modified AdaBoost framework, to solve the above problem. Intuitively, the distribution weights in AdaBoost framework, which reflect importance of training samples, are input into weighted ELM as training sample weights. Furthermore, AdaBoost is modified in two aspects to be more effective for imbalanced learning: (i) the initial distribution weights are set to be asymmetric so that AdaBoost converges at a faster speed; (ii) the distribution weights are updated separately for different classes to avoid destroying the distribution weights asymmetry. Experimental results on 16 binary datasets and 5 multiclass datasets from KEEL repository show that the proposed method could achieve more balanced results than weighted ELM.",Extreme learning machine Weighted extreme learning machine Imbalanced datasets AdaBoost,
470,Visualizing dynamic geosciences phenomena using an octree-based view-dependent LOD strategy within virtual globes,Computers & Geosciences,2011,"Geoscientists build dynamic models to simulate various natural phenomena for a better understanding of our planet. Interactive visualizations of these geoscience models and their outputs through virtual globes on the Internet can help the public understand the dynamic phenomena related to the Earth more intuitively. However, challenges arise when the volume of four-dimensional data (4D), 3D in space plus time, is huge for rendering. Datasets loaded from geographically distributed data servers require synchronization between ingesting and rendering data. Also the visualization capability of display clients varies significantly in such an online visualization environment; some may not have high-end graphic cards. To enhance the efficiency of visualizing dynamic volumetric data in virtual globes, this paper proposes a systematic framework, in which an octree-based multiresolution data structure is implemented to organize time series 3D geospatial data to be used in virtual globe environments. This framework includes a view-dependent continuous level of detail (LOD) strategy formulated as a synchronized part of the virtual globe rendering process. Through the octree-based data retrieval process, the LOD strategy enables the rendering of the 4D simulation at a consistent and acceptable frame rate. To demonstrate the capabilities of this framework, data of a simulated dust storm event are rendered in World Wind, an open source virtual globe. The rendering performances with and without the octree-based LOD strategy are compared. The experimental results show that using the proposed data structure and processing strategy significantly enhances the visualization performance when rendering dynamic geospatial phenomena in virtual globes.",Geovisualization Virtual globe Octree Level of detail (LOD) World Wind,
471,Development and application of soft sensor model for heterogeneous information of aluminum reduction cells,Control Engineering Practice,2011,"The soft sensor model for heterogeneous information is presented because of the difficulty of online acquiring heterogeneous information of aluminum reduction cells. Firstly many redundancy samples are optimized by Fuzzy C-Means in order to acquire classified samples. Then dynamic process of heterogeneous information of aluminum reduction cells is modeled by multiple neural networks. Finally soft sensor model for heterogeneous information of aluminum reduction cells is developed. The model was used in 320 KA prebaked aluminum reduction cells in Guangxi Branch of China Aluminum Corporation. The results indicate that there are three types of instabilities for aluminum reduction cells: single anode irrationality, parameters irrationality of heat balance and outside operations. Corresponding measures to eliminate the three types of instabilities for aluminum reduction cells are the following: raising the anode, adjusting the parameters of heat balance and improving the operation of changing anode and taping metal.",Aluminum reduction cells Soft sensor Anode–cathode-distance (ACD) Model Fuzzy c-means (FCM),
472,Automatic Active Model Initialization via Poisson Inverse Gradient,"Image Processing, IEEE Transactions on",2008,"Active models have been widely used in image processing applications. A crucial stage that affects the ultimate active model performance is initialization. This paper proposes a novel automatic initialization approach for parametric active models in both 2-D and 3-D. The PIG initialization method exploits a novel technique that essentially estimates the external energy field from the external force field and determines the most likely initial segmentation. Examples and comparisons with two state-of-the-art automatic initialization methods are presented to illustrate the advantages of this innovation, including the ability to choose the number of active models deployed, rapid convergence, accommodation of broken edges, superior noise robustness, and segmentation accuracy.",gradient methods image segmentation stochastic processes PIG initialization method Poisson inverse gradient automatic active model initialization image processing applications image segmentation rapid convergence superior noise robustness Active contours Poisson inverse gradient Poisson's equation active models active surfaces deformable models deformable surfaces initialization snakes Algorithms Artificial Intelligence Computer Simulation Image Enhancement Image Interpretation Computer-Assisted Imaging Three-Dimensional Models Statistical Pattern Recognition Automated Poisson Distribution Reproducibility of Results Sensitivity and Specificity,
473,A Distributed Plugin Based Architecture for Medical Image Processing,Medical Imaging 2013: Advanced Pacs-Based Imaging Informatics and Therapeutic Applications,2013,"Large volumes of data are common in medical imaging and require special applications that enable fast and reliable processing. High performance computing enables such processing by running applications in parallel. This work presents a cluster architecture for the parallel processing of DICOM datasets, where the functionality is not specified by the application itself but rather by a set of plugins. Results show that processing times reduce with each added node while network latency is kept stable.",,
474,Accelerating 3D Medical Image Segmentation with High Performance Computing,,2008,Digital processing of medical images has helped physicians and patients during past years by allowing examination and diagnosis on a very precise level. Nowadays possibly the biggest deal of support it can offer for modern healthcare is the use of High Performance Computing architectures to treat the huge amounts of data that can be collected by modern acquisition devices. This paper presents a parallel processing implementation of an image segmentation algorithm that operates on a computer cluster equipped with 10 processing units. Thanks to well-organized distribution of the workload we manage to significantly shorten the execution time of the developed algorithm and reach a performance gain very close to linear.,,
475,Algorithm/Architecture Co-Exploration of Visual Computing on Emergent Platforms: Overview and Future Prospects,"Circuits and Systems for Video Technology, IEEE Transactions on",2009,"Concurrently exploring both algorithmic and architectural optimizations is a new design paradigm. This survey paper addresses the latest research and future perspectives on the simultaneous development of video coding, processing, and computing algorithms with emerging platforms that have multiple cores and reconfigurable architecture. As the algorithms in forthcoming visual systems become increasingly complex, many applications must have different profiles with different levels of performance. Hence, with expectations that the visual experience in the future will become continuously better, it is critical that advanced platforms provide higher performance, better flexibility, and lower power consumption. To achieve these goals, algorithm and architecture co-design is significant for characterizing the algorithmic complexity used to optimize targeted architecture. This paper shows that seamless weaving of the development of previously autonomous visual computing algorithms and multicore or reconfigurable architectures will unavoidably become the leading trend in the future of video technology.",optimisation reconfigurable architectures video coding algorithmic complexity multiple core optimization power consumption reconfigurable architecture video coding visual computing Algorithm/architecture co-exploration complexity analysis or characterization dataflow graphs multicore reconfigurability visual computing,
476,CUDA optimization strategies for compute- and memory-bound neuroimaging algorithms,Computer Methods and Programs in Biomedicine,2012,"As neuroimaging algorithms and technology continue to grow faster than CPU performance in complexity and image resolution, data-parallel computing methods will be increasingly important. The high performance, data-parallel architecture of modern graphical processing units (GPUs) can reduce computational times by orders of magnitude. However, its massively threaded architecture introduces challenges when GPU resources are exceeded. This paper presents optimization strategies for compute- and memory-bound algorithms for the CUDA architecture. For compute-bound algorithms, the registers are reduced through variable reuse via shared memory and the data throughput is increased through heavier thread workloads and maximizing the thread configuration for a single thread block per multiprocessor. For memory-bound algorithms, fitting the data into the fast but limited GPU resources is achieved through reorganizing the data into self-contained structures and employing a multi-pass approach. Memory latencies are reduced by selecting memory resources whose cache performance are optimized for the algorithm's access patterns. We demonstrate the strategies on two computationally expensive algorithms and achieve optimized GPU implementations that perform up to 6× faster than unoptimized ones. Compared to CPU implementations, we achieve peak GPU speedups of 129× for the 3D unbiased nonlinear image registration technique and 93× for the non-local means surface denoising algorithm.",Graphics Processing Unit (GPU) Performance Optimization Compute-bound Memory-bound CUDA Neuroimaging,
477,Parallel image processing applications on a network of workstations,Parallel Computing,1995,"Concurrent computing on networks of distributed computers has gained tremendous attention and popularity in recent years. In this paper, we use this computing environment for the development of efficient parallel image convolution applications for grey-level images and binary images. Significant speedup was achieved using different image sizes, kernel sizes, and number of workstations. We also present a performance prediction model that agrees well with our experimental measurements and allows the highest speedup to be predicted from the knowledge of the ratio of the computation time to the communication time. The main limiting factor in our programming environment is the bandwidth of the network. Thus, it seems with emerging high-speed networks such as ATM networks, parallel computing on networks of distributed computers can be a very attractive alternative to traditional parallel computing on SIMD and MIMD multiprocessors in executing computationally intensive applications in general and image processing applications in particular.",Image processing Convolution algorithm EXPRESS Network of workstations Distributed memory multiprocessor,
478,Cervical spine mobility analysis on radiographs: A fully automatic approach,Computerized Medical Imaging and Graphics,2012,"Conventional X-ray radiography remains nowadays the most common method to analyze spinal mobility in two dimensions. Therefore, the objective of this paper is to develop a framework dedicated to the fully automatic cervical spine mobility analysis on X-ray images. To this aim, we propose an approach based on three main steps: fully automatic vertebra detection, vertebra segmentation and angular measurement. The accuracy of the method was assessed for a total of 245 vertebræ. For the vertebra detection, we proposed an adapted version of two descriptors, namely Scale-invariant Feature Transform (SIFT) and Speeded-up Robust Features (SURF), coupled with a multi-class Support Vector Machine (SVM) classifier. Vertebræ are successfully detected in 89.8% of cases and it is demonstrated that SURF slightly outperforms SIFT. The Active Shape Model approach was considered as a segmentation procedure. We observed that a statistical shape model specific to the vertebral level improves the results. Angular errors of cervical spine mobility are presented. We showed that these errors remain within the inter-operator variability of the reference method.",Cervical spine motion Vertebra detection Vertebra segmentation X-ray images,
479,Denoising of PET images by combining wavelets and curvelets for improved preservation of resolution and quantitation,Medical Image Analysis,2013," Denoising of Positron Emission Tomography (PET) images is a challenging task due to the inherent low signal-to-noise ratio (SNR) of the acquired data. A pre-processing denoising step may facilitate and improve the results of further steps such as segmentation, quantification or textural features characterization. Different recent denoising techniques have been introduced and most state-of-the-art methods are based on filtering in the wavelet domain. However, the wavelet transform suffers from some limitations due to its non-optimal processing of edge discontinuities. More recently, a new multi scale geometric approach has been proposed, namely the curvelet transform. It extends the wavelet transform to account for directional properties in the image. In order to address the issue of resolution loss associated with standard denoising, we considered a strategy combining the complementary wavelet and curvelet transforms. We compared different figures of merit (e.g. SNR increase, noise decrease in homogeneous regions, resolution loss, and intensity bias) on simulated and clinical datasets with the proposed combined approach and the wavelet-only and curvelet-only filtering techniques. The three methods led to an increase of the SNR. Regarding the quantitative accuracy however, the wavelet and curvelet only denoising approaches led to larger biases in the intensity and the contrast than the proposed combined algorithm. This approach could become an alternative solution to filters currently used after image reconstruction in clinical systems such as the Gaussian filter.",Positron emission tomography Curvelet transform Wavelet transform Denoising Filtering,
480,Building a hybrid patient's model for augmented reality in surgery: A registration problem,Computers in Biology and Medicine,1995,"In the field of Augmented Reality in Surgery, building a hybrid patient's model, i.e. merging all the data and systems available for a given application, is a difficult but crucial technical problem. The purpose is to merge all the data that consitute the patient model with the reality of the surgery, i.e. the surgical tools and feedback devices. In this paper, we first develop this concept, we show that this construction comes to a problem of registration between various sensor data, and we detail a general framework of registration The state of the art in this domain is presented. Finally, we show results that we have obtained using a method which is based on the use of anatomical reference surfaces. We show that in many clinical cases, registration is only possible through the use of internal patient structures.",Registration Merging reality and models Fusion Integration Calibration Coordinate systems Localization,
481,Automated detection of intracranial aneurysms based on parent vessel 3D analysis,Medical Image Analysis,2010,"The detection of brain aneurysms plays a key role in reducing the incidence of intracranial subarachnoid hemorrhage (SAH) which carries a high rate of morbidity and mortality. The majority of non-traumatic SAH cases is caused by ruptured intracranial aneurysms and accurate detection can decrease a significant proportion of misdiagnosed cases. A scheme for automated detection of intracranial aneurysms is proposed in this study. Applied to the segmented cerebral vasculature, the method detects aneurysms as suspect regions on the vascular tree, and is designed to assist diagnosticians with their interpretations and thus reduce missed detections. In the current approach, the vessels are segmented and their medial axis is computed. Small regions along the vessels are inspected and the writhe number is introduced as a new surface descriptor to quantify how closely any given region approximates a tubular structure. Aneurysms are detected as non-tubular regions of the vascular tree. The geometric assumptions underlying the approach are investigated analytically and validated experimentally. The method is tested on 3D-rotational angiography (3D-RA) and computed tomography angiography (CTA). In our experiments, 100% sensitivity was achieved with average false positives rates of 0.66 per study on 3D-RA data and 5.36 false positive rates per study on CTA data.",Intracranial aneurysms Aneurysm detection Computed-aided diagnostic Surface descriptor Writhe number,
482,An optimised radial basis function algorithm for fast non-rigid registration of medical images,Computers in Biology and Medicine,2010,"The registration of multi-modal medical image data is important in the fields of image guided surgery and computer aided medical diagnosis. Registration accuracy is of utmost importance in both fields, however in the former, the speed of registration is equally important. In this paper, we present a point-based ‘fast’ non-rigid registration algorithm which exhibits significant speedups as compared to the non-optimised equivalent algorithm. Additionally, we make use of the parallel nature of the graphics processing unit (GPU) of the video adapter card of a standard PC to gain further speedups. The algorithm achieved sub-second performance when tested on the registration of MR with CT image data of size 256 3 .",Medical image registration Smooth interpolation Radial basis functions Real-time performance Computer assisted surgery,
483,Real-time nonlinear finite element computations on GPU – Application to neurosurgical simulation,Computer Methods in Applied Mechanics and Engineering,2010,"Application of biomechanical modeling techniques in the area of medical image analysis and surgical simulation implies two conflicting requirements: accurate results and high solution speeds. Accurate results can be obtained only by using appropriate models and solution algorithms. In our previous papers we have presented algorithms and solution methods for performing accurate nonlinear finite element analysis of brain shift (which includes mixed mesh, different non-linear material models, finite deformations and brain–skull contacts) in less than a minute on a personal computer for models having up to 50,000 degrees of freedom. In this paper we present an implementation of our algorithms on a graphics processing unit (GPU) using the new NVIDIA Compute Unified Device Architecture (CUDA) which leads to more than 20 times increase in the computation speed. This makes possible the use of meshes with more elements, which better represent the geometry, are easier to generate, and provide more accurate results.",Non-rigid image registration Biomechanical models Dynamic relaxation Graphics processing unit CUDA,
484,Numerical and experimental assessment of turbulent kinetic energy in an aortic coarctation,Journal of Biomechanics,2013," The turbulent blood flow through an aortic coarctation in a 63-year old female patient was studied experimentally using magnetic resonance imaging (MRI), and numerically using computational fluid dynamics (CFD), before and after catheter intervention. Turbulent kinetic energy (TKE) was computed in the numerical model using large eddy simulation and compared with direct in vivo MRI measurements. Despite the two totally different methods to obtain TKE values, both quantitative and qualitative results agreed very well. The results showed that even though both blood flow rate and Reynolds number increased after intervention, total turbulent kinetic energy levels decreased in the coarctation. Therefore, the use of the Reynolds number alone as a measure of turbulence in cardiovascular flows should be used with caution. Furthermore, the change in flow field and kinetic energy were assessed, and it was found that before intervention a jet formed in the throat of the coarctation, which impacted the arterial wall just downstream the constriction. After intervention the jet was significantly weaker and broke up almost immediately, presumably resulting in less stress on the wall. As there was a good agreement between measurements and numerical results (the increase and decrease of integrated TKE matched measurements almost perfectly while peak values differed by approximately 1 mJ), the CFD results confirmed the MRI measurements while at the same time providing high-resolution details about the flow. Thus, this preliminary study indicates that MR-based TKE measurements might be useful as a diagnostic tool when evaluating intervention outcome, while the detailed numerical results might be useful for further understanding of the flow for treatment planning.",Computational fluid dynamics Large eddy simulation Magnetic resonance imaging Turbulent flow,
485,Suite of finite element algorithms for accurate computation of soft tissue deformation for surgical simulation,Medical Image Analysis,2009,"Real time computation of soft tissue deformation is important for the use of augmented reality devices and for providing haptic feedback during operation or surgeon training. This requires algorithms that are fast, accurate and can handle material nonlinearities and large deformations. A set of such algorithms is presented in this paper, starting with the finite element formulation and the integration scheme used and addressing common problems such as hourglass control and locking. The computation examples presented prove that by using these algorithms, real time computations become possible without sacrificing the accuracy of the results. For a brain model having more than 7000 degrees of freedom, we computed the reaction forces due to indentation with frequency of around 1000 Hz using a standard dual core PC. Similarly, we conducted simulation of brain shift using a model with more than 50,000 degrees of freedom in less than one minute. The speed benefits of our models result from combining the Total Lagrangian formulation with explicit time integration and low order finite elements.",Real time computations Total Lagrangian formulation Explicit time integration Non-locking tetrahedron Hourglass control Contact algorithm,
486,Automated tracking in digitized videofluoroscopy sequences for spine kinematic analysis,Image and Vision Computing,2009,"Spine kinematic analysis provides useful information to aid understanding of the segmental motion of the vertebrae. Digitized videofluoroscopy (DVF) is the existing practical modality to image spine motion for kinematic data acquisition. However, obtaining kinematic parameters from DVF sequence requires manual landmarking which is a laborious process and can be subjective and error prone. This work develops an automated spine motion tracking algorithm for DVF sequences within a Bayesian framework. By utilizing the anatomical relationships between vertebrae, a dynamic Bayesian network with a particle filter at each node is constructed. The proposed algorithm overcomes the dimensionality problem in a regular particle filter and has more efficient and robust performance. It can provide results of about 1° and 2 pixels ( 0.2 mm ) variability in rotation and translation estimation, respectively, during repeated initialization analysis on sequences from simulation and in vivo healthy human subject studies.",Videofluoroscopy Spine kinematics Particle filter Dynamic Bayesian network Image processing,
487,Fast box-counting algorithm on GPU,Computer Methods and Programs in Biomedicine,2012,"The box-counting algorithm is one of the most widely used methods for calculating the fractal dimension (FD). The FD has many image analysis applications in the biomedical field, where it has been used extensively to characterize a wide range of medical signals. However, computing the FD for large images, especially in 3D, is a time consuming process. In this paper we present a fast parallel version of the box-counting algorithm, which has been coded in CUDA for execution on the Graphic Processing Unit (GPU). The optimized GPU implementation achieved an average speedup of 28 times (28×) compared to a mono-threaded CPU implementation, and an average speedup of 7 times (7×) compared to a multi-threaded CPU implementation. The performance of our improved box-counting algorithm has been tested with 3D models with different complexity, features and sizes. The validity and accuracy of the algorithm has been confirmed using models with well-known FD values. As a case study, a 3D FD analysis of several brain tissues has been performed using our GPU box-counting algorithm.",Box counting CUDA GPU Fractal dimension,
488,A Web platform for the interactive visualization and analysis of the 3D fractal dimension of MRI data,Journal of Biomedical Informatics,2014," This study presents a Web platform (http://3dfd.ujaen.es) for computing and analyzing the 3D fractal dimension (3DFD) from volumetric data in an efficient, visual and interactive way. The Web platform is specially designed for working with magnetic resonance images (MRIs) of the brain. The program estimates the 3DFD by calculating the 3D box-counting of the entire volume of the brain, and also of its 3D skeleton. All of this is done in a graphical, fast and optimized way by using novel technologies like CUDA and WebGL. The usefulness of the Web platform presented is demonstrated by its application in a case study where an analysis and characterization of groups of 3D MR images is performed for three neurodegenerative diseases: Multiple Sclerosis, Intrauterine Growth Restriction and Alzheimer’s disease. To the best of our knowledge, this is the first Web platform that allows the users to calculate, visualize, analyze and compare the 3DFD from MRI images in the cloud.",3D fractal dimension Box-counting Skeleton Magnetic-resonance imaging WebGL CUDA,
489,Polynomial interpolation and polynomial root finding on OTIS-mesh,Parallel Computing,2006,"OTIS-mesh is an efficient model of optoelctronic parallel computers. In this paper, we develop several parallel algorithms for two important problems of numerical analysis, i.e., polynomial interpolation and polynomial root finding on this network. The algorithms are based on two schemes of data mapping namely, row-column mapping and group mapping in which the input data elements are arranged to store into row/column and group wise respectively by suitably exploiting the links of the network. We use Lagrange method for polynomial interpolation and Durand-Kerner method [T.L. Freeman, Calculating polynomial zeros on local memory parallel computer, Parallel Comput. 12 (1989) 351-358.] for polynomial root finding. The optimality/near optimality of the algorithms is shown to achieve with the assumption that the data elements are initially stored following the above mentioned mapping. The scalability of the algorithms is also studied. 2006 Elsevier B.V. All rights reserved.",Interpolation Data reduction Lagrange multipliers Parallel algorithms Parallel processing systems Polynomials,
490,Polynomial interpolation and polynomial root finding on OTIS-mesh,Parallel Computing,2006,"OTIS-mesh is an efficient model of optoelctronic parallel computers. In this paper, we develop several parallel algorithms for two important problems of numerical analysis, i.e., polynomial interpolation and polynomial root finding on this network. The algorithms are based on two schemes of data mapping namely, row-column mapping and group mapping in which the input data elements are arranged to store into row/column and group wise respectively by suitably exploiting the links of the network. We use Lagrange method for polynomial interpolation and Durand-Kerner method [T.L. Freeman, Calculating polynomial zeros on local memory parallel computer, Parallel Comput. 12 (1989) 351-358.] for polynomial root finding. The optimality/near optimality of the algorithms is shown to achieve with the assumption that the data elements are initially stored following the above mentioned mapping. The scalability of the algorithms is also studied. [All rights reserved Elsevier].",interpolation multiprocessor interconnection networks optical interconnections parallel algorithms polynomial approximation,
491,Medical image fusion: A survey of the state of the art,Information Fusion,2014," Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years.",Image fusion Medical imaging Medical image analysis Diagnostics,
492,"Practicing vision: Integration, evaluation and applications",Pattern Recognition,1997,"Computer vision has emerged as a challenging and important area of research, both as an engineering and a scientific discipline. The growing importance of computer vision is evident from the fact that it was identified as one of the ″Grand Challenges″ and also from its prominent role in the National Information Infrastructure. While the design of a general purpose vision system continues to be elusive, machine vision systems are being used successfully in specific application domains. Building a practical vision system requires a careful selection of appropriate sensors, extraction and integration of information from available cues in the sensed data, and evaluation of system robustness and performance. We discuss and demonstrate advantages of (i) multi-sensor fusion, (ii) combination of features and classifiers, (iii) integration of visual modules, and (iv) admissibility and goal-directed evaluation of vision algorithms. The requirements of several prominent real world applications such as biometry, document image analysis, image and video database retrieval, and automatic object model construction offer exciting problems and new opportunities to design and evaluate vision algorithms.",Computer vision Machine vision System integration Evaluation Applications,
493,Enabling multi-user interaction in large high-resolution distributed environments,Future Generation Computer Systems,2011,"As the amount and the resolution of collected scientific data increase, scientists are realizing the potential benefits that large high-resolution displays can have in assimilating this incoming data. Often this data has to be processed on powerful remote computing and storage resources, converted to high-resolution digital media and yet visualized on a local tiled-display. This is the basic premise behind the OptIPuter model. While the streaming middleware to enable this kind of work exists and the optical networking infrastructure is becoming more widely available, enabling multi-user interaction in such environments is still a challenge. In this paper, we present an interaction system we developed to support collaborative work on large high-resolution displays using multiple interaction devices and scalable, distributed user interface widgets. This system allows multiple users to simultaneously interact with local or remote data, media and applications, through a variety of physical interaction devices on large high-resolution displays. Finally, we present our experiences with using the system over the past two years. Most importantly, having an actual working system based on the OptIPuter model allows us to focus our research efforts to better understand how to make such high-resolution environments more user-friendly and usable in true real-world collaborative scenarios as opposed to constrained laboratory settings.",Human factors Input devices and strategies Interactive systems Distributed systems Distributed graphics Collaborative computing,
494,A new graph representation for cable-membrane structures,Advances in Engineering Software,2002,"In this paper, a new graph representation is proposed which is applicable to cable-membrane structures modelled using both one- and two-dimensional elements. The proposed graph representation is an engineering design approach and not based on a mathematically derived representation. The proposed graphs are partitioned using state-of-the-art tools, including METIS [METIS, a software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices (1997); J Parallel Distribut Comput (1997)], and JOSTLE [Advances in computational mechanics with parallel and distributed processing (1997); Parallel dynamic graph-partitioning for unstructured meshes (1997); Int J High Perform Comput Appl 13 (1999) 334; Appl Math Model 25 (2000) 123]. The graph representation performs better than standard graph representations for those cases when the rules of geometric locality and uniform element distribution around nodes are violated. The relation of the proposed graph representation to the most advanced hyper-graph representation [IEEE Trans Parallel Distribut Syst 10 (1999) 673; Parallel Comput 26 (2000) 673] is also discussed. 2002 Published by Elsevier Science Ltd.",Graph theory Computational geometry Computer software Parallel processing systems Problem solving,
